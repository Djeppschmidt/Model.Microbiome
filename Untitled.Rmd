---
title: "M.M. Quick Tutorial"
author: "Dietrich Epp Schmidt"
date: "5/18/2020"
output: html_document
---

Table of contents:
Building a model
Emplementing a simulation
Outputs: examining taxon relationships to environment
i-PERMANOVA
ii-LII
iii-linear model
iv-index ratios & variance
v-statistical tests

Outputs: examining taxon relationships to each other

Introduction:

ModelMicrobiome is a novel platform for testing the performance and assumptions underlying ecological inference of microbiomes. Whereas previous benchmarking and modeling efforts have used case-control frameworks, and/or used randomized real data, none have tested the accuracy of ecological inference following a sequence count normalization procedure. This is to say previous efforts have focused on the ability to accurately detect a change in relative abudance (i.e. differential abundance analysis). But these tests do not reflect how well the ecological relationships are preserved through the normalization process (e.g. relationship between the taxa and their environment or among taxa). This software does not attempt to address bias that is introduced by the sequencing, DNA purification, or field sampling efforts. It only is meant as a tool to better understand the effect that any post-hoc sequence count normalization has on our ability to infer underlying ecological relationships. 

To further clarify the limits of this software, it is not intended to be applied to metagenomics analyses that derive from shotgun sequence data. There are database and sequencing biases that are particular to whole genome sequencing that must be addressed separately and that may confound the inferences made from this software. THIS SOFTWARE IS NOT DESIGNED TO ADDRESS SHOTGUN SEQUENCING OR ION TORRENT ETC. SEQUENCING. This software is designed specifically for amplicon (i.e. metabarcoding) studies of a single gene locus used to infer population structure. 

This software was written under R version 3.6.1. R version 4 has been published with major updates to some of the core mechanics. Model.MicrobiomeV2 will be updated for R 4.0.

With those disclaimers out of the way, let's dig in.

First, let's install Model.Microbiome:
```{r}
# update anything from my local computer (for me as I work, not you...) ####
#setwd("/Users/maullabmacbookpro/Documents/GitHub/Model.Microbiome")
#library(devtools)
#library(roxygen2)
#document()

#now install ####
devtools::install_github("djeppschmidt/Model.Microbiome")
```
Now load all the other packages we need (or download and load if you haven't already):
```{r}
# load required packages ####

library(Model.Microbiome)
library(reshape2)
library(ggplot2)
library(vegan)
library(dplyr)
library(plyr)
library(phyloseq)
library(viridis)
library(ranacapa)
library(edgeR)
library(limma)
library(GLDEX)
library(stats)
library(igraph)
library(car)
library(DESeq2)
```


The core functionality of Model.Microbiome is to create a model microbial community, then simulate sequencing by subsampling from it, then normalizing the raw sequence data. This is handled by the run.analysis2() function. I recommend running this function with SuppressWarnings() because many of the models throw warnings about too-perfect of a fit. Since the point of this software is to detect these functions, we don't care. We have ways of measuring these effects.

Building the model community works sequenially by:
1. creating an environment, with 5 environmental parameters
  a. environmetnal paramters 1-3 change in mean and variance according to the experimental design
  b. environmental parameters 4-5 are unrelated to the experimental design, but do affect taxon abundance
  c. the sampling environment consists of 30 samples, divided into 6 experimental treatments (or 5 reps per treatment)
  d. the environmental parameters have the same mean and variance for each group every time the software is run; however, the values of any given run are randomly drawn from this distribution. Therefore no two runs will have exactly the same environment, but they will represent exactly the same experiment.

2. Choosing which species exist in each sample.
  a. The user sets how many taxa will be chosen per sample
  b. the user sets how many taxa will exist only within certain experimental treatments
  c. the user sets how many taxa will exist only within certain samples
  d. the user sets how many taxa will be chosen to be globally distributed
  
NOTE: SEE SECTION DETAILS FOR CAVEATS ABOUT THIS SELECTION PROCESS!!

3. Community assembly.
  a. taxon abundances are calculated; each species has a specific relationship with the environmental variables.
  b. OTU table generated, and merged into a phyloseq object with metadata on the environment, etc.

4. Community is sampled
  a. this is a subsampling routine that creates the "sequence output" table

5. Sequence Normalization
  a. uses a function to normalize the data any arbitrary number of different methods

```{r}
knitr::include_graphics("~/Desktop/PhD/RarefactionProject/Process_Map.pdf")
```

So let's look at the function. It has several inputs necessary:

commonN: Number of taxa that have a global distribution
groupN: Number of taxa that are group-specific
singleN: Number of taxa that are sample-specific

CAVEAT: there are 700 species functions that the software selects from. For each of these values, it uses a random selection routine. It includes resampling among the groups. This means that:

THERE IS A CHANCE THAT A TAXON MAY END UP REPRESENTING SEVERAL GROUPS, OR SAMPLES.

D: mean sampling depth (i.e. simulating sequencing)
V: variance of sampling depth.

method: names of the functions to be used for count normalization (see below for details)

```{r}
method<-c("QSeq")

model<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))

```
This creates a single output object that contains the phyloseq objects created before, and a number of different statistics. We'll get to the indices later.In order to get it to work, we need at least one methods function.


Example method function:
```{r}

# these functions are already provided as a part of Model.Microbiome
make.scaled2<-function(ps, val, scale){
  scaled<-data.frame(mapply(`*`, data.frame(as.matrix(otu_table(transform_sample_counts(ps, function(x) x/sum(x))))), scale * val))# sample_data(ps)$val))
  names<-rownames(data.frame(as.matrix(otu_table(ps))))
  rownames(scaled)<-names
  scaled<-round(scaled)

  p2<-ps
  otu_table(p2)<- otu_table(scaled, taxa_are_rows=T)
  p2
}

QSeq<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=2*mean(sample_sums(ps)), scale)
    out
}

```

Notice that this is actually one function nested within another. The wrapper helps the function to play nice. It's easy to replicate wrappers to provide the underlying function with different inputs if we want to test the different potential conditions. For example, our QSeq function is meant to replicate the normalization technique called quantitative sequencing. This technique combines quantitative data from QPCR with sequence count distributions to normalize the sequence counts in each sample by the relative total sequence counts of that sample compared to other samples. This should more closely approximate the behavior of the sampled community than relative abundance. To do this though, we need to decide what the mean value for the sample-wise abundance normalization. We could make several versions of the QSeq function to test what is the optimal mean sample-wise total abundance. To do this we change the value parameter as follows:

```{r}
QSeq0.5<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=0.5*mean(sample_sums(ps)), scale)
    out
}

QSeq1<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=mean(sample_sums(ps)), scale)
    out
}

QSeq2<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=2*mean(sample_sums(ps)), scale)
    out
}

QSeq3<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=3*mean(sample_sums(ps)), scale)
    out
}

QSeq10<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=10*mean(sample_sums(ps)), scale)
    out
}

QSeq100<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=100*mean(sample_sums(ps)), scale)
    out
}
```

If we want to use all of them in our simulation study, then we simply do:
```{r}
method<-c("QSeq0.5", "QSeq1", "QSeq2", "QSeq3", "QSeq10", "QSeq100")
```

Then we implement it in the analysis pipeline:

```{r}
model<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))
```

We can call the phyloseq object from any one of the normalization (including the reference data and the raw sequence counts):
```{r}
model$model$comm
model$raw$comm
model$QSeq0.5$comm
model$QSeq1$comm
model$QSeq2$comm
model$QSeq3$comm
model$QSeq10$comm
```

Model.Microbiome also automatically calculates several metrics. For example, it calculates automatically an experiment-wide PERMANOVA for each method and each factor. So for example if we look at the reference, we have a seperate permanova for the experimental category:
```{r}
model$model$PERMANOVA$Category

```
And each of the environmental factors:
```{r}
model$model$PERMANOVA$F1
model$model$PERMANOVA$F2
model$model$PERMANOVA$F3
model$model$PERMANOVA$F4
model$model$PERMANOVA$F5
```

We can compare any of these across treatments:
```{r}
model$model$PERMANOVA$F1 # reference condition!
model$raw$PERMANOVA$F1 # not normalized
model$QSeq0.5$PERMANOVA$F1 # lowest mean value
model$QSeq1$PERMANOVA$F1  #...
model$QSeq2$PERMANOVA$F1 #...
model$QSeq3$PERMANOVA$F1 #...
model$QSeq10$PERMANOVA$F1
model$QSeq100$PERMANOVA$F1# highest mean value
```
You'll notice that the QSeq PERMANOVA tables are very similar to the reference PERMANOVA table, and that the un-normalized table is a bit different. This is great! 

Model.Microbiome uses a few metrics to understand bias that is introduced through the normalization process. All the metrics in Model.Microbiome consist of the ouput from an analysis conducted on a normalized dataset (treatment) divided by the same analysis conducted on the reference community. For example, if we are interested in the relationship between a taxon an the environment, we could construct a linear model where abundance of taxon x depends on environment A. The R-squared of this model would represent the amount of information captured by the model. If we applied this model to both the reference community, and the normalized community, then we have a measure of the "actual" relationship and the inferred relationship after our normalization, respectively. The ratio of these values gives us a comparable value where 1 represents a perfect match; greater than 1 indicates the normalization method leads to overestimating the explanatory power of the environmental factors on the abundance of the taxon; and a value of less than one indicates that the normalization underestimates the explanatory value of the environmental parameter. All of the indices are based on this interpretation principle. 

Here is a brief description of each index:

Loss of information index (LII) is based on a linear model approach, but instead of regressing the taxa against the environment and comparing the outputs, it regresses the treatment (normalized data) against the reference (pre-sampling). This provides a direct measure of information that is maintained about the abundance of the taxon across the dataset. The LII output has several components;

$Index : index value which is the sum from all taxa of (1 - the linear regression r-squared values) divided by the total number of taxa. This is a single value for the whole run. Lower scores equate to less overall information lost.

$R : a list of the R squared values, one per taxon in the run.

$diff : a list of (1 - the r-squared values) for the Index value, one for each taxon. This is useful to identify which taxa are deviating substantially, and help with troubleshooting the normalization method.

All of these can be accessed from the output object (see output object map). 

The second set of metrics consist of linear regressions of the taxa against environmental and experimental design predictors. This test allows us to diagnose the effect that our normalization protocol has on our ability to infer relationships between the taxa and the environment / experimental condition. Once again, the primary outputs of this process are then taken as a ratio to the reference for interpretation.

$lmRatiotab : list of taxa and their R-squared values between taxon and the environment, normalized by the reference

$lmRatiotab.model : list of taxa and their R-squared values between taxon and the experimental design, normalized by the reference


Object Map:
```{r}
knitr::include_graphics("~/Desktop/PhD/RarefactionProject/Output_Map.pdf")
```

The speciesMeta object also stores a few other taxon specific metrics that help us interpret the outcomes;

$prevalence is a taxon-wise measure of how many sites the taxon actually occures in.

$mean_abundance is a taxon-wise measure of the mean abundance of each taxon across all sites

$sd_abundance is a taxon-wise measure of the standard deviation in abundance of each taxon across all sites.

$M.Eval is the mean of the relative abundance of the taxon at each site divided by the sampling depth of that site. This is meant to represent an unweigthed estimation of the likelihood that a taxon will be sampled at any site, given that they exist in that site. Lower values mean the taxon is generally less abundant.

In addition to these other things, it has the taxon specific R-ratio that is the foundation of the LII for each normalization run. This allows us to understand better what is driving changes in the LII. These are each listed under the name of the method. For example, we have $QSeq, which represents the R-squared of the regression of the treatment abundance against the reference abundance for each taxon in the QSeq treatment. Let's look at some example plots that demonstrate the utility of these metadata:

First, let's examine the relationship between the expected likelihood of sampling a taxon and the amount of information that is lost:
```{r}

ggplot(model$model$SpeciesMeta, aes(x=log10(M.Eval),y=log(QSeq0.5), col=prevalence))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

```
The color represents the prevalence, we can see that in this case, there are many taxa that lose no information (LII is highest when there is the most information lost; negative slope indicates that less information is lost as the likelihood of sampling the taxon goes up). Most of these are taxa that only occur in one sample. Let's color the samples differently:

```{r}

ggplot(model$model$SpeciesMeta, aes(x=log10(M.Eval),y=log(QSeq0.5), col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

```
We can see that the taxa, though they exist only in one site, have a range of abundances.We can look at how this patterns shifts as a function of our normalization technique:

```{r}

ggplot(model$model$SpeciesMeta, aes(x=log10(M.Eval),y=log(QSeq0.5), col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

ggplot(model$model$SpeciesMeta, aes(x=log10(M.Eval),y=log(QSeq1), col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

ggplot(model$model$SpeciesMeta, aes(x=log10(M.Eval),y=log(QSeq10), col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

ggplot(model$model$SpeciesMeta, aes(x=log10(M.Eval),y=log(QSeq100), col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

```
We can see that in particular there is a shift in the amount of information that is lost in the middle M.Eval ranges; at high E.Val, there is relatively less information lost and so the values are more stable. At the low end, the lost information effects are dominated by whether or not the taxa happen to be detected; in the middle, taxa are regularly detected but their abundance is very dependent on sampling depth of the sample. Thus this is the area where we regain the most information.

```{r}
ggplot(model$model$SpeciesMeta, aes(x=prevalence,y=QSeq0.5, col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

ggplot(model$model$SpeciesMeta, aes(x=prevalence,y=QSeq1, col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()
ggplot(model$model$SpeciesMeta, aes(x=prevalence,y=QSeq10, col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()
ggplot(model$model$SpeciesMeta, aes(x=prevalence,y=QSeq100, col=log(mean_abundance)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

ggplot(model$model$SpeciesMeta, aes(x=log(mean_abundance),y=QSeq0.5, col=log(prevalence)))+
  geom_point()+ 
  scale_color_viridis_c(option="C")+
  theme_classic()

```

We can explore to see how well species abundance and prevalence explain the LII:

```{r}
summary(lm(model$model$SpeciesMeta$QSeq0.5~model$model$SpeciesMeta$prevalence*log(model$model$SpeciesMeta$mean_abundance)))

# look at prediction of prevalence
summary(lm(model$model$SpeciesMeta$QSeq0.5~model$model$SpeciesMeta$prevalence*log(model$model$SpeciesMeta$mean_abundance)))

summary(lm(model$model$SpeciesMeta$QSeq100~model$model$SpeciesMeta$prevalence*log(model$model$SpeciesMeta$mean_abundance)))


```

Pay close attention to the R-squared values of the model. We can see that the prevalence and mean abundance explain the highest amount of variance in the LII model. This doesn't necessarily mean that this model performs better; imagine if the relative patterns ofall taxa are perfectly preserved, then there would be no relationship between the mean abundance, prevalence and information that is lost through sampling. Conversely, if we have a really high predictive power between the abundance, prevalence, and lost information, it doesn't necessarily mean that the normalization is creating more bias in the dataset as there are likely real effects of relative abundance on the likelihood of detection in subsampling. This is to say that the statistics and the plots should be examined within the context of how the normalization procedure is known to operate in order to determine if it is creating bias. In this instance, QSeq0.5 decreases the R-squared of the model because some taxa are rounded down to 0, creating an anomaly that is removed at QSeq10 and QSeq100.


In nearly all our comparisons of a normalization technique, we are comparing the results of a statistical test given the normalization to the reference. we do this because it is possible that our normalization technique might overfit the data to an expected trend, thus information may be lost also by overfitting. We have looked at an explicit evaluation of lost information intrinsic to each taxon; we can also look at how this lost information degrades our abilty to detect true relationships between the taxa and the environmental gradients. To do this, we employ linear modeling. We have two linear model outputs for each taxon. In the first case, we regress the abundance of the taxon against all the environmental variables, excluding categorical experimental design from the test structure. In the second case, we include only the categorical variables and exclude the environmental variables. This allows us to resolve whether there has been overfitting of taxon abundance to our experimental design; and the effect this has had on our ability to detect the relationship to environmental gradients. Of course, our final interest is not on the outputs of the model per se, but the ratio of model outputs from normalized datasets to the reference. This gives ratio tells us clearly which taxa are accurately modeled. In aggregate, the variance of this ratio around 1 tells us which method is more consistently and accurately modeling the relationship of each taxon to the environmental gradient driving it's abundance. 
(R squared = 1 is ideal!)
```{r}
plot(log(model$model$SpeciesMeta$mean_abundance),model$QSeq0.5$lmRatiotab, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq0.5; linear model Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(log(model$model$SpeciesMeta$mean_abundance),model$QSeq100$lmRatiotab, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq100; linear model Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

```

```{r}
plot(log(model$model$SpeciesMeta$mean_abundance),model$QSeq0.5$lmRatiotab.model, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq0.5; linear model Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(log(model$model$SpeciesMeta$mean_abundance),model$QSeq100$lmRatiotab.model, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq100; linear model Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

```

We can clearly see from both linear model R-Ratio plots that there are a number of taxa for which we lost completely any relationship. These were likely taxa that were rounded out of the community in QSeq0.5 but not QSeq100. We can also see that overall the general pattern of relationships is maintained across the rest of the taxa. The rate of overestimation of the relationship between taxa and explanitory variables also remains the same

You may notice that certain taxa have an R squared of 0; some of them improve as we increase the overall scaling factor. This happens when taxa are very low abundance in a particular sample. If their count abundance is less than 0.5, they get rounded down to 0. So ideally we want to pick a scaling value that lets us keep all the information from sampling (i.e. such that in all samples, the minimum taxon has a count of at least 1). This is an important observation if we want to use this method on a real dataset because it gives us a criteria for choosing what our scaling factor will be.


We can see qualitatively that our higher QSeq methods increase the accuracy of our taxon-specific models by a small margin. We can evaluate this statistically:

```{r}
levenetest<-data.frame("ratio"=c(model$QSeq0.5$lmRatiotab, model$QSeq100$lmRatiotab), "method"=c(rep("raw", length(model$raw$lmRatiotab)), rep("QSeq", length(model$QSeq3$lmRatiotab))))

car::leveneTest(ratio ~ method, data = levenetest)
```
In this case it's not a huge improvement, but the method provides a solid evidence-based approach for optimizing normalization methods. 

It might be instructional at this point to look at the samle distribution without any normalization, to see how well the normalization improves our inference about the relationship of taxa to their environment:

```{r}
plot(model$model$SpeciesMeta$prevalence,model$raw$lmRatiotab, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="Not normalized; ratio-Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(model$model$SpeciesMeta$prevalence,model$QSeq100$lmRatiotab, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq100; ratio-Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(model$model$SpeciesMeta$prevalence,model$raw$lmRatiotab.model, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="Not normalized; ratio-Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(model$model$SpeciesMeta$prevalence,model$QSeq100$lmRatiotab.model, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq100; ratio-Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")
```
There is an important implication of the variance of the R-Ratio; that higher variance indicates less accurate estimation of the relationship between taxa and the environment across the whole community. If there were perfect accuracy, the value would be 1 for all taxa, and the variance would be zero.

Now let's look at how relationships among taxa are affected by normalization (or lack therof). We can access the R-Ratio for a correlation among taxa:
```{r}

model$raw$taxCor.Ratio

```
Negative values indicate a negative relationship between the relationship among taxa in the reference compared to the treatment. Ideally all relationships should be 1 if the information is perfectly preserved. We can subset the dataset by specific taxa, and use metadata about those taxa to better understand why the relationships change. If we look at taxon 153, we see that it exists in 22 sites. We can look at how the prevalence and abundance of other taxa relate to the predicted relationship between the taxa.

```{r}
table(rownames(model$model$SpeciesMeta)==model$QSeq0.5$taxCor.Ratio[model$QSeq0.5$taxCor.Ratio$Var2=="spp153",1])
# plot vs prevalence
plot(model$model$SpeciesMeta$prevalence,model$QSeq0.5$taxCor.Ratio[model$QSeq0.5$taxCor.Ratio$Var2=="spp153",3], xlab="Prevalence", ylab="taxcor.Ratio")

plot(model$model$SpeciesMeta$mean_abundance,model$QSeq0.5$taxCor.Ratio[model$QSeq0.5$taxCor.Ratio$Var2=="spp153",3], xlab="mean Abundance", ylab="taxcor.Ratio")

plot(model$model$SpeciesMeta$sd_abundance,model$QSeq0.5$taxCor.Ratio[model$QSeq0.5$taxCor.Ratio$Var2=="spp153",3], xlab="SD Abundance", ylab="taxcor.Ratio")
```
We can see in general, the largest errors come from where the prevalence and mean abundance are both low. However, in this example the one difference is a highly prevalent, but low abundance taxon. We can examine how this pattern holds up on a low prevalence taxon.
```{r}
# check taxa are in the right order
table(rownames(model$model$SpeciesMeta)==model$QSeq0.5$taxCor.Ratio[model$QSeq0.5$taxCor.Ratio$Var2=="spp10",1])
# plot vs prevalence
plot(model$model$SpeciesMeta$prevalence,model$QSeq0.5$taxCor.Ratio[model$QSeq0.5$taxCor.Ratio$Var2=="spp10",3], xlab="Prevalence", ylab="taxcor.Ratio")

plot(model$model$SpeciesMeta$mean_abundance,model$QSeq0.5$taxCor.Ratio[model$QSeq0.5$taxCor.Ratio$Var2=="spp10",3], xlab="mean Abundance", ylab="taxcor.Ratio")

plot(model$model$SpeciesMeta$sd_abundance,model$QSeq0.5$taxCor.Ratio[model$QSeq0.5$taxCor.Ratio$Var2=="spp10",3], xlab="SD Abundance", ylab="taxcor.Ratio")


```

```{r}

```

Model.Microbiome includes several R-ratio metrics for whole community patterns. These are based on the PERMANOVA r-squared value. Model.Microbiome conducts a PERMANOVA of each environmental factor and the categorical factor against a Bray-Curtis transformation of the count table. These are done separately because order of addition of the factors in the model influences the model outcomes; it is easiest to get comparable results run each factor in isolation. For example:

```{r}
model$raw$PERMANOVA$F1

```

The R squared part of the table is taken as a ratio of the reference:
```{r}
model$raw$PERMANOVA$F1Rratio
```

We can examine how well each of the environmental and categorical factors are maintained:
```{r}
model$raw$PERMANOVA$CategoryRratio
```

```{r}
model$raw$PERMANOVA$F2Rratio
```

```{r}
model$raw$PERMANOVA$F3Rratio
```

```{r}
model$raw$PERMANOVA$F4Rratio
```

```{r}
model$raw$PERMANOVA$F5Rratio
```
And of course, we can plot an ordination:

```{r}
bray_NMDS = ordinate(model$QSeq10$comm, "NMDS", "bray")
plot_ordination(model$QSeq10$comm, bray_pcoa, "samples", color="Factor")
```
We can compare the ordination

```{r}
bray_NMDS.ref = ordinate(model$model$comm, "NMDS", "bray")
plot_ordination(model$model$comm, bray_NMDS.ref, "samples", color="Factor")

bray_NMDS.raw = ordinate(model$raw$comm, "NMDS", "bray")
plot_ordination(model$raw$comm, bray_NMDS.raw, "samples", color="Factor")

bray_NMDS.Q = ordinate(model$QSeq10$comm, "NMDS", "bray")
plot_ordination(model$QSeq10$comm, bray_NMDS.Q, "samples", color="Factor")
```

# Conducting Experiments: Summarizing Outputs
This software also has a wrapper that allows us to iterate the function and produce many copies of the simulation. This has all the same inputs as before, plus one aditional input to tell the function how many times to iterate. In this case we'll do 10.
```{r}
model10<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=30, groupN=20, singleN=5, D=500, V=250, method))
```
model10 is now a list object with 10 items, each one as a replicated simulation. This is the basis for us to be able to extract performance metrics and do statistical analyses to compare different normalization methods. 

So now that we have the basic data structures and functionalities, let's look at some analyses that are possible when we can automate community generation.


We first extract the LII indicator with the function Summarize.LII(), then we melt the dataframe and do a statistical test on it:
```{r}
model10.LIIsummary<-Summarize.LII(model10, method)
model10.LIIsummary<-melt(model10.LIIsummary, value.name = "Value")

summary(aov(Value~Var2 +Error(Var1/Var2), data=model10.LIIsummary)) # blocks of method are nested within each run
TukeyHSD(aov(Value~Var2, data=model10.LIIsummary)) # ignoring the blocks bc TukeyHSD can't handle them. Still tells us which is most significant
plot(TukeyHSD(aov(Value~Var2, data=model10.LIIsummary))) # visual of confidence intervals

```
Clearly, the worst performing method is QSeq0.5. This is likely because it effectively leads to undersampling by rounding low count values to zero. Ideally we should choose a threshold value for this method that is sufficiently high enough that all sequences that pass QC in our pipeline are included in the sampling, and not rounded to 0 in any sample.

Let's look at how all these functionalities can be tied together into a comprehensive test of the strengths (or weaknesses) of a normalization procedure.


We can look at the difference in PERMANOVA outputs:
```{r}
model10.PCat<-Summarize.PERMANOVA.Rratio(model10, method, "CategoryRratio")

model10.PF1<-Summarize.PERMANOVA.Rratio(model10, method, "F1Rratio")
model10.PF2<-Summarize.PERMANOVA.Rratio(model10, method, "F2Rratio")
model10.PF3<-Summarize.PERMANOVA.Rratio(model10, method, "F3Rratio")
model10.PF4<-Summarize.PERMANOVA.Rratio(model10, method, "F4Rratio")
model10.PF5<-Summarize.PERMANOVA.Rratio(model10, method, "F5Rratio")

model10.PCat
model10.PF1
model10.PF2
model10.PF3
model10.PF4
model10.PF5

```

In general, there are many outputs that a researcher may want to generate and summarize that we have not anticipated. Here are some functions for extracting the R-ratio from the linear models to give a template on the extraction method that works well with the object. This should give a means for researchers to extract and summarize any metric in any way they want. We will use both of these functions later in the analysis.
```{r}
# get lmRatio ####
Summarize.lmRatiotab.Median<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method)) # make matrix
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-median(trt[[i]][[method[j]]]$lmRatiotab, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

Summarize.lmRatiotab.Var<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method)) # make matrix
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-var(trt[[i]][[method[j]]]$lmRatiotab, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

```
# Exp. 1: Normalization vs Structure
Let's say that we want to understand how the effect size of geographic structure within a community interacts with the performance of our normalization technique (or in the case of experimental design, what are the direct assemply effects of the treatment). Model.Microbiome gives us the power to vary the geographic structure/direct effects of the treatment on the community by determining the proportion of each sample that explicitly is determined by the grouping function:
```{r}

# biogeography begin ####
model10.A<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=53, groupN=1, singleN=1, D=500, V=250, method)) # essentially no filtration by group

model10.B<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=35, groupN=15, singleN=5, D=500, V=250, method)) # ~27% filtration by the group

model10.C<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=250, method)) # ~36% filtration by the group

model10.D<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=10, groupN=30, singleN=15, D=500, V=250, method)) # ~55% filtration by the group
```

First, let's look at how the structure of the data might interact with our normalization strategy to influence how well information is retained. For this we will extract the LII value. As a reminder, LII correlates each taxon in the normalized dataset against it's abundance in the reference dataset. The index itself is a sum of 1-R-squared; this means that low LII values mean that overall the R-squared of the taxa against their references were close to 1 (or little information was lost). Conversely, higher values indicate that a lot of information was lost.
```{r}

# get LII Biogeography ####
method2<- c("raw", method) # add raw data to the methods list to extract it too

# extract the LII from each level of sparcity ####
SummaryLII.model10.A<-as.data.frame(Summarize.LII(model10.A,method2))
SummaryLII.model10.B<-as.data.frame(Summarize.LII(model10.B, method2)) 
SummaryLII.model10.C<-as.data.frame(Summarize.LII(model10.C, method2))
SummaryLII.model10.D<-as.data.frame(Summarize.LII(model10.D, method2))

# prepare data for merging datasets
SummaryLII.model10.A$Level<-c(rep(1,10))
SummaryLII.model10.A<-melt(SummaryLII.model10.A, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.B$Level<-c(rep(2,10))
SummaryLII.model10.B<-melt(SummaryLII.model10.B, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C$Level<-c(rep(3,10))
SummaryLII.model10.C<-melt(SummaryLII.model10.C, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.D$Level<-c(rep(4,10))
SummaryLII.model10.D<-melt(SummaryLII.model10.D, id.vars = "Level", measure.vars = method2)

# merge datasets
model10Levels<-do.call("rbind", list(SummaryLII.model10.A,SummaryLII.model10.B,SummaryLII.model10.C,SummaryLII.model10.D))

# summarize for plotting
model10Levels.summary2<-data_summary2(model10Levels, varname="value", groupnames=c("Level", "variable"))
model10Levels.summary1<-data_summary(model10Levels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(model10Levels.summary2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("LII Value (Median +/- min/max)")+
  theme_classic()

ggplot(model10Levels.summary1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("LII Value (Mean +/- standard dev)")+
  theme_classic()
```
Here we can see clearly that the degree of structure in the dataset interacts with the normalization method to affect it's accuracy. In all cases the QSeq normaliation method out performs
And some statistics:

```{r}
summary(aov(model10Levels$value~model10Levels$Level*model10Levels$variable))

TukeyHSD(aov(model10Levels$value~model10Levels$Level*model10Levels$variable))
```
Conclusion: we can see clearly that the degree of structure in the dataset interacts with the normalization method to affect it's accuracy. In all cases the QSeq normaliation method out performs un-normalized data at low structure, but at high levels of structure only QSeq2 and QSeq3 parameters continue to consistently outperform the raw variables. Remeber that LII reports the degradation of R squared and so is a direct measure of lost information. Let's now look at how this results in a loss of environmental signal.

In this case interpretation is a little more challenging, because we are less interested in the mean value, and more interested in the variance of the values. This is because our metric is a ratio of the R-squared (variance explained) of each taxon by the environment to the same measure of the taxon in the reference dataset. This tells us the relative accuracy with which the normalized dataset is predicting the relationship of each taxon to the environment. Ideally the mean value should be 1 (a perfect fit). But it is possible for the mean to be 1, but the deviation to be substantial if it is balanced in the positive and negative direction. In otherwords, if the error in the positive and negative direction are equal in size, then even if there is large error in those directions, the mean value will still be 1 (a perfect fit). Thus we might have a perfect fit of the mean, yet have no taxa that is accurately modeled. To account for this, we put larger emphasis on the variance using a levene's test of equality of variance; but because variances may not be equally balanced in the positive and negative direction we are also still interested in changes in the average value. The two are necessary for accurate interpretation. We find especially for this analysis that outliers unduely affect the mean, so we prefer median for understanding the most common effect

```{r}

Summarize.lmRatiotab.Var(model10.A, method2)



SM.lmRatio.model10.A<-as.data.frame(Summarize.lmRatiotab.Median(model10.A,method2))
SV.lmRatio.model10.A<-as.data.frame(Summarize.lmRatiotab.Var(model10.A,method2))
SM.lmRatio.model10.B<-as.data.frame(Summarize.lmRatiotab.Median(model10.B,method2))
SV.lmRatio.model10.B<-as.data.frame(Summarize.lmRatiotab.Var(model10.B,method2))
SM.lmRatio.model10.C<-as.data.frame(Summarize.lmRatiotab.Median(model10.C,method2))
SV.lmRatio.model10.C<-as.data.frame(Summarize.lmRatiotab.Var(model10.C,method2))
SM.lmRatio.model10.D<-as.data.frame(Summarize.lmRatiotab.Median(model10.D,method2))
SV.lmRatio.model10.D<-as.data.frame(Summarize.lmRatiotab.Var(model10.D,method2))

# prepare data for merging datasets

SM.lmRatio.model10.A$Level<-c(rep(1,10))
SV.lmRatio.model10.A$Level<-c(rep(1,10))
SM.lmRatio.model10.B$Level<-c(rep(2,10))
SV.lmRatio.model10.B$Level<-c(rep(2,10))
SM.lmRatio.model10.C$Level<-c(rep(3,10))
SV.lmRatio.model10.C$Level<-c(rep(3,10))
SM.lmRatio.model10.D$Level<-c(rep(4,10))
SV.lmRatio.model10.D$Level<-c(rep(4,10))

SM.lmRatio.model10.A<-melt(SM.lmRatio.model10.A, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.A<-melt(SV.lmRatio.model10.A, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.B<-melt(SM.lmRatio.model10.B, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.B<-melt(SV.lmRatio.model10.B, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C<-melt(SM.lmRatio.model10.C, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C<-melt(SV.lmRatio.model10.C, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.D<-melt(SM.lmRatio.model10.D, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.D<-melt(SV.lmRatio.model10.D, id.vars = "Level", measure.vars = method2)

# merge datasets
SM.lmRatio.model10Levels<-do.call("rbind", list(SM.lmRatio.model10.A,SM.lmRatio.model10.B,SM.lmRatio.model10.C,SM.lmRatio.model10.D))
SV.lmRatio.model10Levels<-do.call("rbind", list(SV.lmRatio.model10.A,SV.lmRatio.model10.B,SV.lmRatio.model10.C,SV.lmRatio.model10.D))


# summarize for plotting
SM.lmRatio.model10.p1<-data_summary(SM.lmRatio.model10Levels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10.p1<-data_summary(SV.lmRatio.model10Levels, varname="value", groupnames=c("Level", "variable"))
# plot
SM.lmRatio.model10.p2<-data_summary2(SM.lmRatio.model10Levels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10.p2<-data_summary2(SV.lmRatio.model10Levels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(SM.lmRatio.model10.p1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Mean lmRatio (Mean +/- sd)")+
  theme_classic()

ggplot(SV.lmRatio.model10.p1, aes(x=Level, y=value, group = variable, color=variable))+scale_colour_viridis_d(direction=-1)+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Variance lmRatio (Mean +/- sd)")+
  theme_classic()


ggplot(SM.lmRatio.model10.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Median lmRatio (Median +/- min/max)")+
  theme_classic()

ggplot(SV.lmRatio.model10.p, aes(x=Level, y=log(value), group = variable, color=variable))+scale_colour_viridis_d(direction=-1)+
  geom_errorbar(aes(ymin=log(low), ymax=log(high)), width=.1, position=position_dodge(.3)) +
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("log Variance lmRatio (Median +/- min/max)")+
  theme_classic()


```
... and run statistics to see the effect:
```{r}
summary(aov(SM.lmRatio.model10Levels$value~SM.lmRatio.model10Levels$Level*SM.lmRatio.model10Levels$variable))

TukeyHSD(aov(SM.lmRatio.model10Levels$value~SM.lmRatio.model10Levels$Level*SM.lmRatio.model10Levels$variable))

summary(aov(SV.lmRatio.model10Levels$value~SV.lmRatio.model10Levels$Level*SV.lmRatio.model10Levels$variable))

TukeyHSD(aov(SV.lmRatio.model10Levels$value~SV.lmRatio.model10Levels$Level*SV.lmRatio.model10Levels$variable))


```
Conclusion: there is an interaction between the normalization method and the internal structure of the community on our ability to accurately describe the relationship between taxa and the environmental gradients. According to this analysis we should prefer either QSeq2 or QSeq3 [check this later!!! (add in QSeq10)]

What about our ability to detect the experimental design? Following the same methodology:

```{r}  
Summarize.lmRatiotabModel.Median<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method)) # make matrix
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-median(trt[[i]][[method[j]]]$lmRatiotab.model, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

Summarize.lmRatiotabModel.Var<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method)) # make matrix
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-var(trt[[i]][[method[j]]]$lmRatiotab.model, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}


SM.lmRatioM.model10.A<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.A,method2))
SV.lmRatioM.model10.A<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.A,method2))
SM.lmRatioM.model10.B<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.B,method2))
SV.lmRatioM.model10.B<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.B,method2))
SM.lmRatioM.model10.C<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C,method2))
SV.lmRatioM.model10.C<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C,method2))
SM.lmRatioM.model10.D<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.D,method2))
SV.lmRatioM.model10.D<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.D,method2))

# prepare data for merging datasets

SM.lmRatioM.model10.A$Level<-c(rep(1,10))
SV.lmRatioM.model10.A$Level<-c(rep(1,10))
SM.lmRatioM.model10.B$Level<-c(rep(2,10))
SV.lmRatioM.model10.B$Level<-c(rep(2,10))
SM.lmRatioM.model10.C$Level<-c(rep(3,10))
SV.lmRatioM.model10.C$Level<-c(rep(3,10))
SM.lmRatioM.model10.D$Level<-c(rep(4,10))
SV.lmRatioM.model10.D$Level<-c(rep(4,10))

SM.lmRatioM.model10.A<-melt(SM.lmRatioM.model10.A, id.vars = "Level", measure.vars = method2)
SV.lmRatioM.model10.A<-melt(SV.lmRatioM.model10.A, id.vars = "Level", measure.vars = method2)
SM.lmRatioM.model10.B<-melt(SM.lmRatioM.model10.B, id.vars = "Level", measure.vars = method2)
SV.lmRatioM.model10.B<-melt(SV.lmRatioM.model10.B, id.vars = "Level", measure.vars = method2)
SM.lmRatioM.model10.C<-melt(SM.lmRatioM.model10.C, id.vars = "Level", measure.vars = method2)
SV.lmRatioM.model10.C<-melt(SV.lmRatioM.model10.C, id.vars = "Level", measure.vars = method2)
SM.lmRatioM.model10.D<-melt(SM.lmRatioM.model10.D, id.vars = "Level", measure.vars = method)
SV.lmRatioM.model10.D<-melt(SV.lmRatioM.model10.D, id.vars = "Level", measure.vars = method2)

# merge datasets
SM.lmRatioM.model10Levels<-do.call("rbind", list(SM.lmRatioM.model10.A,SM.lmRatioM.model10.B,SM.lmRatioM.model10.C,SM.lmRatioM.model10.D))
SV.lmRatioM.model10Levels<-do.call("rbind", list(SV.lmRatioM.model10.A,SV.lmRatioM.model10.B,SV.lmRatioM.model10.C,SV.lmRatioM.model10.D))


# summarize for plotting

SM.lmRatioM.model10.p1<-data_summary(SM.lmRatioM.model10Levels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatioM.model10.p1<-data_summary(SV.lmRatioM.model10Levels, varname="value", groupnames=c("Level", "variable"))
SM.lmRatioM.model10.p2<-data_summary2(SM.lmRatioM.model10Levels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatioM.model10.p2<-data_summary2(SV.lmRatioM.model10Levels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(SM.lmRatioM.model10.p1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d()+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Median lmRatio.model (Mean +/- sd)")+
  theme_classic()

ggplot(SV.lmRatioM.model10.p1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Variance lmRatio.model (Mean +/- sd)")+
  
  theme_classic()
ggplot(SM.lmRatioM.model10.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d()+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Median lmRatio.model (Median +/- min/max)")+
  theme_classic()

ggplot(SV.lmRatioM.model10.p, aes(x=Level, y=log(value), group = variable, color=variable))+
  geom_errorbar(aes(ymin=log(low), ymax=log(high)), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Variance lmRatio.model (Median +/- min/max)")+
  
  theme_classic()

```

And again we do the stats:

```{r}
summary(aov(SM.lmRatioM.model10Levels$value~SM.lmRatioM.model10Levels$Level*SM.lmRatioM.model10Levels$variable))

TukeyHSD(aov(SM.lmRatioM.model10Levels$value~SM.lmRatioM.model10Levels$variable)) # something wrong here?

summary(aov(SV.lmRatioM.model10Levels$value~SV.lmRatioM.model10Levels$Level*SV.lmRatioM.model10Levels$variable))

TukeyHSD(aov(SV.lmRatioM.model10Levels$value~SV.lmRatioM.model10Levels$Level*SV.lmRatioM.model10Levels$variable))

```
An emerging pattern that we see here is that raw data without any processing performs much worst when there is less structure and environmental variables can act more as a filter. Also, in our normalization method QSeq, higher sampling variables that don't omit low abundance taxa perform better. Higher abundance will continue to perform even better.

Let's check the accuracy of the model:
```{r}
# outputs an object with both variance and median
TaxRatio.model10.A<-getTaxCor.Tab(model10.A,method2)
TaxRatio.model10.B<-getTaxCor.Tab(model10.B,method2)
TaxRatio.model10.C<-getTaxCor.Tab(model10.C,method2)
TaxRatio.model10.D<-getTaxCor.Tab(model10.D,method2)

#separate median and variance tables, and melt
V.TaxRatio.model10.A<-melt(TaxRatio.model10.A$V.tax)
M.TaxRatio.model10.A<-melt(TaxRatio.model10.A$Median.tax)
V.TaxRatio.model10.B<-melt(TaxRatio.model10.B$V.tax)
M.TaxRatio.model10.B<-melt(TaxRatio.model10.B$Median.tax)
V.TaxRatio.model10.C<-melt(TaxRatio.model10.C$V.tax)
M.TaxRatio.model10.C<-melt(TaxRatio.model10.C$Median.tax)
V.TaxRatio.model10.D<-melt(TaxRatio.model10.D$V.tax)
M.TaxRatio.model10.D<-melt(TaxRatio.model10.D$Median.tax)

# prepare data for merging datasets
V.TaxRatio.model10.A<-as.data.frame(V.TaxRatio.model10.A)
M.TaxRatio.model10.A<-as.data.frame(M.TaxRatio.model10.A)
V.TaxRatio.model10.B<-as.data.frame(V.TaxRatio.model10.B)
M.TaxRatio.model10.B<-as.data.frame(M.TaxRatio.model10.B)
V.TaxRatio.model10.C<-as.data.frame(V.TaxRatio.model10.C)
M.TaxRatio.model10.C<-as.data.frame(M.TaxRatio.model10.C)
V.TaxRatio.model10.D<-as.data.frame(V.TaxRatio.model10.D)
M.TaxRatio.model10.D<-as.data.frame(M.TaxRatio.model10.D)

V.TaxRatio.model10.A$Level<-c(rep(1,nrow(V.TaxRatio.model10.A)))
V.TaxRatio.model10.B$Level<-c(rep(2,nrow(V.TaxRatio.model10.B)))
V.TaxRatio.model10.C$Level<-c(rep(3,nrow(V.TaxRatio.model10.C)))
V.TaxRatio.model10.D$Level<-c(rep(4,nrow(V.TaxRatio.model10.D)))
M.TaxRatio.model10.A$Level<-c(rep(1,nrow(M.TaxRatio.model10.A)))
M.TaxRatio.model10.B$Level<-c(rep(2,nrow(M.TaxRatio.model10.B)))
M.TaxRatio.model10.C$Level<-c(rep(3,nrow(M.TaxRatio.model10.C)))
M.TaxRatio.model10.D$Level<-c(rep(4,nrow(M.TaxRatio.model10.D)))



# merge datasets
V.TaxRatio.model10Levels<-do.call("rbind", list(V.TaxRatio.model10.A,V.TaxRatio.model10.B,V.TaxRatio.model10.C,V.TaxRatio.model10.D))
M.TaxRatio.model10Levels<-do.call("rbind", list(M.TaxRatio.model10.A,M.TaxRatio.model10.B,M.TaxRatio.model10.C,M.TaxRatio.model10.D))


# summarize for plotting
V.TaxRatio.model10.p1<-data_summary(V.TaxRatio.model10Levels, varname="value", groupnames=c("Level", "Var2"))
M.TaxRatio.model10.p1<-data_summary(M.TaxRatio.model10Levels, varname="value", groupnames=c("Level", "Var2"))
V.TaxRatio.model10.p2<-data_summary2(V.TaxRatio.model10Levels, varname="value", groupnames=c("Level", "Var2"))
M.TaxRatio.model10.p2<-data_summary2(M.TaxRatio.model10Levels, varname="value", groupnames=c("Level", "Var2"))
# plot
ggplot(V.TaxRatio.model10.p1, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Variance taxRatio (Mean +/- sd)")+
  theme_classic()

ggplot(M.TaxRatio.model10.p1, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Mean taxRatio (Mean +/- sd)")+
  theme_classic()

ggplot(V.TaxRatio.model10.p2, aes(x=Level, y=log10(value), group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=log10(low), ymax=log10(high)), width=.1, position=position_dodge(.3)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Variance taxRatio (Median +/- min/max)")+
  theme_classic()

ggplot(M.TaxRatio.model10.p2, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("log Median taxRatio (Median +/- min/max)")+
  theme_classic()

```
In terms of the median, it appears that at all levels of structure, QSeq10 methods are nominally the same.QSeq0.5 is generally not particularly great; and raw count data performs the worst at very unstructured data. In terms of the variance, it looks like all the QSeq outperform raw, but aren't very differentiated among themselves. But let's check the stats:

```{r}
summary(aov(M.TaxRatio.model10Levels$value~M.TaxRatio.model10Levels$Level*M.TaxRatio.model10Levels$Var2))

TukeyHSD(aov(M.TaxRatio.model10Levels$value~M.TaxRatio.model10Levels$Var2))

summary(aov(V.TaxRatio.model10Levels$value~V.TaxRatio.model10Levels$Level*V.TaxRatio.model10Levels$Var2))

TukeyHSD(aov(V.TaxRatio.model10Levels$value~V.TaxRatio.model10Levels$Level*V.TaxRatio.model10Levels$Var2))

#leveneTest(value~as.factor(Level), data=V.TaxRatio.model10.p)
```
The TukeyHSD confirms that differences in median value are primarily driven by QSeq0.5. There was no statistical difference between normalization methods in terms of the variance value.

Now let's look at a common community level statistic.
```{r}
# get permanova geography ####


model10A.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.A,method2, "CategoryRratio"))
model10A.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.A, method2, "F1Rratio"))
model10A.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.A, method2, "F2Rratio"))
model10A.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.A, method2, "F3Rratio"))
model10A.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.A, method2, "F4Rratio"))
model10A.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.A,method2, "F5Rratio"))


model10B.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.B, method2, "CategoryRratio"))
model10B.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.B, method2, "F1Rratio"))
model10B.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.B, method2, "F2Rratio"))
model10B.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.B, method2, "F3Rratio"))
model10B.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.B, method2, "F4Rratio"))
model10B.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.B, method2, "F5Rratio"))


model10C.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C, method2, "CategoryRratio"))
model10C.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C, method2, "F1Rratio"))
model10C.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C, method2, "F2Rratio"))
model10C.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C, method2, "F3Rratio"))
model10C.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C, method2, "F4Rratio"))
model10C.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C, method2, "F5Rratio"))


model10D.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.D, method2, "CategoryRratio"))
model10D.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.D, method2, "F1Rratio"))
model10D.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.D, method2, "F2Rratio"))
model10D.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.D, method2, "F3Rratio"))
model10D.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.D, method2, "F4Rratio"))
model10D.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.D, method2, "F5Rratio"))

model10A.Permanova.model$Level<-c(rep(1,10))
model10A.Permanova.env1$Level<-c(rep(1,10))
model10A.Permanova.env2$Level<-c(rep(1,10))
model10A.Permanova.env3$Level<-c(rep(1,10))
model10A.Permanova.env4$Level<-c(rep(1,10))
model10A.Permanova.env5$Level<-c(rep(1,10))

model10B.Permanova.model$Level<-c(rep(2,10))
model10B.Permanova.env1$Level<-c(rep(2,10))
model10B.Permanova.env2$Level<-c(rep(2,10))
model10B.Permanova.env3$Level<-c(rep(2,10))
model10B.Permanova.env4$Level<-c(rep(2,10))
model10B.Permanova.env5$Level<-c(rep(2,10))

model10C.Permanova.model$Level<-c(rep(3,10))
model10C.Permanova.env1$Level<-c(rep(3,10))
model10C.Permanova.env2$Level<-c(rep(3,10))
model10C.Permanova.env3$Level<-c(rep(3,10))
model10C.Permanova.env4$Level<-c(rep(3,10))
model10C.Permanova.env5$Level<-c(rep(3,10))

model10D.Permanova.model$Level<-c(rep(4,10))
model10D.Permanova.env1$Level<-c(rep(4,10))
model10D.Permanova.env2$Level<-c(rep(4,10))
model10D.Permanova.env3$Level<-c(rep(4,10))
model10D.Permanova.env4$Level<-c(rep(4,10))
model10D.Permanova.env5$Level<-c(rep(4,10))

permanova.model10Levels.model<-do.call("rbind", list(model10A.Permanova.model,model10B.Permanova.model,model10C.Permanova.model,model10D.Permanova.model))
permanova.model10Levels.env1<-do.call("rbind", list(model10A.Permanova.env1,model10B.Permanova.env1,model10C.Permanova.env1,model10D.Permanova.env1))
permanova.model10Levels.env2<-do.call("rbind", list(model10A.Permanova.env2,model10B.Permanova.env2,model10C.Permanova.env2,model10D.Permanova.env2))
permanova.model10Levels.env3<-do.call("rbind", list(model10A.Permanova.env3,model10B.Permanova.env3,model10C.Permanova.env3,model10D.Permanova.env3))
permanova.model10Levels.env4<-do.call("rbind", list(model10A.Permanova.env4,model10B.Permanova.env4,model10C.Permanova.env4,model10D.Permanova.env4))
permanova.model10Levels.env5<-do.call("rbind", list(model10A.Permanova.env5,model10B.Permanova.env5,model10C.Permanova.env5,model10D.Permanova.env5))

permanova.model10Levels.model<-melt(permanova.model10Levels.model, id.vars = "Level", measure.vars = method2)
permanova.model10Levels.env1<-melt(permanova.model10Levels.env1, id.vars = "Level", measure.vars = method2)
permanova.model10Levels.env2<-melt(permanova.model10Levels.env2, id.vars = "Level", measure.vars = method2)
permanova.model10Levels.env3<-melt(permanova.model10Levels.env3, id.vars = "Level", measure.vars = method2)
permanova.model10Levels.env4<-melt(permanova.model10Levels.env4, id.vars = "Level", measure.vars = method2)
permanova.model10Levels.env5<-melt(permanova.model10Levels.env5, id.vars = "Level", measure.vars = method2)

permanova.model10Levels.modelP<-data_summary2(permanova.model10Levels.model, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env1P<-data_summary2(permanova.model10Levels.env1, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env2P<-data_summary2(permanova.model10Levels.env2, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env3P<-data_summary2(permanova.model10Levels.env3, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env4P<-data_summary2(permanova.model10Levels.env4, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env5P<-data_summary2(permanova.model10Levels.env5, varname="value", groupnames=c("Level", "variable"))


ggplot(permanova.model10Levels.modelP, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova modelRatio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10Levels.env1P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env1Ratio (Median +/- min/max)")+
  theme_classic()


ggplot(permanova.model10Levels.env2P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env2Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10Levels.env3P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env3Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10Levels.env4P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env4Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10Levels.env5P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env5Ratio (Median +/- min/max)")+
  theme_classic()

```

```{r}
permanova.model10Levels.modelP1<-data_summary(permanova.model10Levels.model, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env1P1<-data_summary(permanova.model10Levels.env1, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env2P1<-data_summary(permanova.model10Levels.env2, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env3P1<-data_summary(permanova.model10Levels.env3, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env4P1<-data_summary(permanova.model10Levels.env4, varname="value", groupnames=c("Level", "variable"))
permanova.model10Levels.env5P1<-data_summary(permanova.model10Levels.env5, varname="value", groupnames=c("Level", "variable"))


ggplot(permanova.model10Levels.modelP1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova modelRatio (Mean +/- sd)")+
  theme_classic()

ggplot(permanova.model10Levels.env1P1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env1Ratio (Mean +/- sd)")+
  theme_classic()


ggplot(permanova.model10Levels.env2P1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env2Ratio (Mean +/- sd)")+
  theme_classic()

ggplot(permanova.model10Levels.env3P1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env3Ratio (Mean +/- sd)")+
  theme_classic()

ggplot(permanova.model10Levels.env4P1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env4Ratio (Mean +/- sd)")+
  theme_classic()

ggplot(permanova.model10Levels.env5P1, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.3)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Permanova env5Ratio (Mean +/- sd)")+
  theme_classic()
```
Statistics:
```{r}

summary(aov(permanova.model10Levels.model$value~permanova.model10Levels.model$Level*permanova.model10Levels.model$variable))
summary(aov(permanova.model10Levels.env1$value~permanova.model10Levels.env1$Level*permanova.model10Levels.env1$variable))
summary(aov(permanova.model10Levels.env2$value~permanova.model10Levels.env2$Level*permanova.model10Levels.env2$variable))
summary(aov(permanova.model10Levels.env3$value~permanova.model10Levels.env3$Level*permanova.model10Levels.env3$variable))
summary(aov(permanova.model10Levels.env4$value~permanova.model10Levels.env4$Level*permanova.model10Levels.env4$variable))
summary(aov(permanova.model10Levels.env5$value~permanova.model10Levels.env5$Level*permanova.model10Levels.env5$variable))

leveneTest(permanova.model10Levels.model$value~permanova.model10Levels.model$variable)
leveneTest(permanova.model10Levels.env1$value~permanova.model10Levels.env1$variable)
leveneTest(permanova.model10Levels.env2$value~permanova.model10Levels.env2$variable)
leveneTest(permanova.model10Levels.env3$value~permanova.model10Levels.env3$variable)
leveneTest(permanova.model10Levels.env4$value~permanova.model10Levels.env4$variable)
leveneTest(permanova.model10Levels.env5$value~permanova.model10Levels.env5$variable)
```
# Expt 2: Normalization vs Sequencing Depth
Let's say we want to know the effect of undersampling interacts with our normalization protocol to affect our ability to detect relationships between taxa and the environment. We can vary the sequencing depth:
```{r}

# make sequencing depth ####
#method<-c("QSeq0.5", "QSeq1", "QSeq2", "QSeq3", "QSeq10")
model10.C1<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=100, V=50, method)

model10.C2<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=200, V=100, method)

model10.C3<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=250, method)

model10.C4<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=1000, V=500, method)

model10.C5<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=2000, V=1000, method)


```

```{r}
SummaryLII.model10.C1<-as.data.frame(Summarize.LII(model10.C1,method2))
SummaryLII.model10.C2<-as.data.frame(Summarize.LII(model10.C2, method2)) 
SummaryLII.model10.C3<-as.data.frame(Summarize.LII(model10.C3, method2))
SummaryLII.model10.C4<-as.data.frame(Summarize.LII(model10.C4, method2))
SummaryLII.model10.C5<-as.data.frame(Summarize.LII(model10.C5, method2))
# prepare data for merging datasets
SummaryLII.model10.C1$Level<-c(rep(100,10))
SummaryLII.model10.C1<-melt(SummaryLII.model10.C1, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C2$Level<-c(rep(200,10))
SummaryLII.model10.C2<-melt(SummaryLII.model10.C2, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C3$Level<-c(rep(500,10))
SummaryLII.model10.C3<-melt(SummaryLII.model10.C3, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C4$Level<-c(rep(1000,10))
SummaryLII.model10.C4<-melt(SummaryLII.model10.C4, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C5$Level<-c(rep(2000,10))
SummaryLII.model10.C5<-melt(SummaryLII.model10.C5, id.vars = "Level", measure.vars = method2)
# merge datasets
model10.SeqLevels<-do.call("rbind", list(SummaryLII.model10.C1,SummaryLII.model10.C2,SummaryLII.model10.C3,SummaryLII.model10.C4,SummaryLII.model10.C5))

# summarize for plotting
model10.SeqLevels.summary<-data_summary(model10.SeqLevels, varname="value", groupnames=c("Level", "variable"))
model10.SeqLevels.summary2<-data_summary2(model10.SeqLevels, varname="value", groupnames=c("Level", "variable"))

# plot
ggplot(model10.SeqLevels.summary, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(30)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Sequencing Depth")+
  ylab("LII Value (Mean +/- sd)")+
  theme_classic()

ggplot(model10.SeqLevels.summary2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Sequencing Depth")+
  ylab("LII Value (Median +/- min/max)")+
  theme_classic()
```
Just like in the example about sparsity, we can look at the effect on sequencing depth on infering the relationship of species abundance to environmental gradients:
```{r}

# sequencing depth env. lm ratio ####
SM.lmRatio.model10.C1<-as.data.frame(Summarize.lmRatiotab.Median(model10.C1,method2))
SV.lmRatio.model10.C1<-as.data.frame(Summarize.lmRatiotab.Var(model10.C1,method2))
SM.lmRatio.model10.C2<-as.data.frame(Summarize.lmRatiotab.Median(model10.C2,method2))
SV.lmRatio.model10.C2<-as.data.frame(Summarize.lmRatiotab.Var(model10.C2,method2))
SM.lmRatio.model10.C3<-as.data.frame(Summarize.lmRatiotab.Median(model10.C3,method2))
SV.lmRatio.model10.C3<-as.data.frame(Summarize.lmRatiotab.Var(model10.C3,method2))
SM.lmRatio.model10.C4<-as.data.frame(Summarize.lmRatiotab.Median(model10.C4,method2))
SV.lmRatio.model10.C4<-as.data.frame(Summarize.lmRatiotab.Var(model10.C4,method2))
SM.lmRatio.model10.C5<-as.data.frame(Summarize.lmRatiotab.Median(model10.C5,method2))
SV.lmRatio.model10.C5<-as.data.frame(Summarize.lmRatiotab.Var(model10.C5,method2))
# prepare data for merging datasets

SM.lmRatio.model10.C1$Level<-c(rep(100,10))
SV.lmRatio.model10.C1$Level<-c(rep(100,10))
SM.lmRatio.model10.C2$Level<-c(rep(200,10))
SV.lmRatio.model10.C2$Level<-c(rep(200,10))
SM.lmRatio.model10.C3$Level<-c(rep(500,10))
SV.lmRatio.model10.C3$Level<-c(rep(500,10))
SM.lmRatio.model10.C4$Level<-c(rep(1000,10))
SV.lmRatio.model10.C4$Level<-c(rep(1000,10))
SM.lmRatio.model10.C5$Level<-c(rep(2000,10))
SV.lmRatio.model10.C5$Level<-c(rep(2000,10))

SM.lmRatio.model10.C1<-melt(SM.lmRatio.model10.C1, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C1<-melt(SV.lmRatio.model10.C1, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C2<-melt(SM.lmRatio.model10.C2, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C2<-melt(SV.lmRatio.model10.C2, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C3<-melt(SM.lmRatio.model10.C3, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C3<-melt(SV.lmRatio.model10.C3, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C4<-melt(SM.lmRatio.model10.C4, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C4<-melt(SV.lmRatio.model10.C4, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C5<-melt(SM.lmRatio.model10.C5, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C5<-melt(SV.lmRatio.model10.C5, id.vars = "Level", measure.vars = method2)

# merge datasets
SM.lmRatio.model10.CLevels<-do.call("rbind", list(SM.lmRatio.model10.C1,SM.lmRatio.model10.C2,SM.lmRatio.model10.C3,SM.lmRatio.model10.C4, SM.lmRatio.model10.C5))
SV.lmRatio.model10.CLevels<-do.call("rbind", list(SV.lmRatio.model10.C1,SV.lmRatio.model10.C2,SV.lmRatio.model10.C3,SV.lmRatio.model10.C4, SV.lmRatio.model10.C5))


# summarize for plotting
SM.lmRatio.model10C.p<-data_summary(SM.lmRatio.model10.CLevels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10C.p<-data_summary(SV.lmRatio.model10.CLevels, varname="value", groupnames=c("Level", "variable"))
# plot

SM.lmRatio.model10C.p2<-data_summary2(SM.lmRatio.model10.CLevels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10C.p2<-data_summary2(SV.lmRatio.model10.CLevels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(SM.lmRatio.model10C.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(30)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Median lmRatio (Mean +/- sd)")+
  theme_classic()

ggplot(SV.lmRatio.model10C.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(30)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Variance lmRatio (Mean +/- sd)")+
  theme_classic()

ggplot(SM.lmRatio.model10C.p2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Median lmRatio (Median +/- min/max)")+
  theme_classic()

ggplot(SV.lmRatio.model10C.p2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Variance lmRatio (Median +/- min/max)")+
  theme_classic()
```
... and run statistics to see the effect:
```{r}
summary(aov(SM.lmRatio.model10.CLevels$value~SM.lmRatio.model10.CLevels$Level*SM.lmRatio.model10.CLevels$variable))

TukeyHSD(aov(SM.lmRatio.model10.CLevels$value~SM.lmRatio.model10.CLevels$Level*SM.lmRatio.model10.CLevels$variable))

summary(aov(SV.lmRatio.model10.CLevels$value~SV.lmRatio.model10.CLevels$Level*SV.lmRatio.model10.CLevels$variable))

TukeyHSD(aov(SV.lmRatio.model10.CLevels$value~SV.lmRatio.model10.CLevels$Level*SV.lmRatio.model10.CLevels$variable))

leveneTest(value~as.factor(Level)*variable, data=SV.lmRatio.model10.CLevels)
```

And same steps but for the categorical variables:
```{r}
# lm ratio model sequencing depth ####
SM.lmRatioM.model10.C1<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C1,method2))
SV.lmRatioM.model10.C1<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C1,method2))
SM.lmRatioM.model10.C2<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C2,method2))
SV.lmRatioM.model10.C2<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C2,method2))
SM.lmRatioM.model10.C3<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C3,method2))
SV.lmRatioM.model10.C3<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C3,method2))
SM.lmRatioM.model10.C4<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C4,method2))
SV.lmRatioM.model10.C4<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C4,method2))
SM.lmRatioM.model10.C5<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C5,method2))
SV.lmRatioM.model10.C5<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C5,method2))
# prepare data for merging datasets

SM.lmRatioM.model10.C1$Level<-c(rep(100,10))
SV.lmRatioM.model10.C1$Level<-c(rep(100,10))
SM.lmRatioM.model10.C2$Level<-c(rep(200,10))
SV.lmRatioM.model10.C2$Level<-c(rep(200,10))
SM.lmRatioM.model10.C3$Level<-c(rep(500,10))
SV.lmRatioM.model10.C3$Level<-c(rep(500,10))
SM.lmRatioM.model10.C4$Level<-c(rep(1000,10))
SV.lmRatioM.model10.C4$Level<-c(rep(1000,10))
SM.lmRatioM.model10.C5$Level<-c(rep(2000,10))
SV.lmRatioM.model10.C5$Level<-c(rep(2000,10))

SM.lmRatioM.model10.C1<-melt(SM.lmRatioM.model10.C1, id.vars = "Level", measure.vars = method2)
SV.lmRatioM.model10.C1<-melt(SV.lmRatioM.model10.C1, id.vars = "Level", measure.vars = method2)
SM.lmRatioM.model10.C2<-melt(SM.lmRatioM.model10.C2, id.vars = "Level", measure.vars = method2)
SV.lmRatioM.model10.C2<-melt(SV.lmRatioM.model10.C2, id.vars = "Level", measure.vars = method2)
SM.lmRatioM.model10.C3<-melt(SM.lmRatioM.model10.C3, id.vars = "Level", measure.vars = method2)
SV.lmRatioM.model10.C3<-melt(SV.lmRatioM.model10.C3, id.vars = "Level", measure.vars = method2)
SM.lmRatioM.model10.C4<-melt(SM.lmRatioM.model10.C4, id.vars = "Level", measure.vars = method2)
SV.lmRatioM.model10.C4<-melt(SV.lmRatioM.model10.C4, id.vars = "Level", measure.vars = method2)
SM.lmRatioM.model10.C5<-melt(SM.lmRatioM.model10.C5, id.vars = "Level", measure.vars = method2)
SV.lmRatioM.model10.C5<-melt(SV.lmRatioM.model10.C5, id.vars = "Level", measure.vars = method2)

# merge datasets
SM.lmRatioM.model10CLevels<-do.call("rbind", list(SM.lmRatioM.model10.C1,SM.lmRatioM.model10.C2,SM.lmRatioM.model10.C3,SM.lmRatioM.model10.C4, SM.lmRatioM.model10.C5))
SV.lmRatioM.model10CLevels<-do.call("rbind", list(SV.lmRatioM.model10.C1,SV.lmRatioM.model10.C2,SV.lmRatioM.model10.C3,SV.lmRatioM.model10.C4, SV.lmRatioM.model10.C5))


# summarize for plotting
SM.lmRatioM.model10C.p<-data_summary(SM.lmRatioM.model10CLevels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatioM.model10C.p<-data_summary(SV.lmRatioM.model10CLevels, varname="value", groupnames=c("Level", "variable"))
SM.lmRatioM.model10C.p2<-data_summary2(SM.lmRatioM.model10CLevels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatioM.model10C.p2<-data_summary2(SV.lmRatioM.model10CLevels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(SM.lmRatioM.model10C.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(30)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Sequencing Depth")+
  ylab("Median lmRatio.model (Mean +/- sd)")+
  theme_classic()

ggplot(SV.lmRatioM.model10C.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(30)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Sequencing Depth")+
  ylab("log Variance lmRatio.model (Mean +/- sd)")+
  theme_classic()

ggplot(SM.lmRatioM.model10C.p2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Sequencing Depth")+
  ylab("Median lmRatio.model (Median +/- min/max)")+
  theme_classic()

ggplot(SV.lmRatioM.model10C.p2, aes(x=Level, y=log(value), group = variable, color=variable))+
  geom_errorbar(aes(ymin=log(low), ymax=log(high)), width=.1, position=position_dodge(30)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Sequencing Depth")+
  ylab("log Variance lmRatio.model (Median +/- min/max)")+
  theme_classic()

```

And again we do the stats:

```{r}
summary(aov(SM.lmRatioM.model10CLevels$value~SM.lmRatioM.model10CLevels$Level*SM.lmRatioM.model10CLevels$variable))

TukeyHSD(aov(SM.lmRatioM.model10CLevels$value~SM.lmRatioM.model10CLevels$Level*SM.lmRatioM.model10CLevels$variable))

summary(aov(SV.lmRatioM.model10CLevels$value~SV.lmRatioM.model10CLevels$Level*SV.lmRatioM.model10CLevels$variable))

TukeyHSD(aov(SV.lmRatioM.model10CLevels$value~SV.lmRatioM.model10CLevels$Level*SV.lmRatioM.model10CLevels$variable))

```

Sequencing Depth:
```{r}
TaxRatio.model10.C1<-getTaxCor.Tab(model10.C1,method2)
TaxRatio.model10.C2<-getTaxCor.Tab(model10.C2,method2)
TaxRatio.model10.C3<-getTaxCor.Tab(model10.C3,method2)
TaxRatio.model10.C4<-getTaxCor.Tab(model10.C4,method2)
TaxRatio.model10.C5<-getTaxCor.Tab(model10.C5,method2)

V.TaxRatio.model10.C1<-melt(TaxRatio.model10.C1$V.tax)
M.TaxRatio.model10.C1<-melt(TaxRatio.model10.C1$Median.tax)
V.TaxRatio.model10.C2<-melt(TaxRatio.model10.C2$V.tax)
M.TaxRatio.model10.C2<-melt(TaxRatio.model10.C2$Median.tax)
V.TaxRatio.model10.C3<-melt(TaxRatio.model10.C3$V.tax)
M.TaxRatio.model10.C3<-melt(TaxRatio.model10.C3$Median.tax)
V.TaxRatio.model10.C4<-melt(TaxRatio.model10.C4$V.tax)
M.TaxRatio.model10.C4<-melt(TaxRatio.model10.C4$Median.tax)
V.TaxRatio.model10.C5<-melt(TaxRatio.model10.C5$V.tax)
M.TaxRatio.model10.C5<-melt(TaxRatio.model10.C5$Median.tax)

V.TaxRatio.model10.C1<-as.data.frame(V.TaxRatio.model10.C1)
M.TaxRatio.model10.C1<-as.data.frame(M.TaxRatio.model10.C1)
V.TaxRatio.model10.C2<-as.data.frame(V.TaxRatio.model10.C2)
M.TaxRatio.model10.C2<-as.data.frame(M.TaxRatio.model10.C2)
V.TaxRatio.model10.C3<-as.data.frame(V.TaxRatio.model10.C3)
M.TaxRatio.model10.C3<-as.data.frame(M.TaxRatio.model10.C3)
V.TaxRatio.model10.C4<-as.data.frame(V.TaxRatio.model10.C4)
M.TaxRatio.model10.C4<-as.data.frame(M.TaxRatio.model10.C4)
V.TaxRatio.model10.C5<-as.data.frame(V.TaxRatio.model10.C5)
M.TaxRatio.model10.C5<-as.data.frame(M.TaxRatio.model10.C5)

# prepare data for merging datasets

V.TaxRatio.model10.C1$Level<-c(rep(100,nrow(V.TaxRatio.model10.C1)))
V.TaxRatio.model10.C2$Level<-c(rep(200,nrow(V.TaxRatio.model10.C2)))
V.TaxRatio.model10.C3$Level<-c(rep(500,nrow(V.TaxRatio.model10.C3)))
V.TaxRatio.model10.C4$Level<-c(rep(1000,nrow(V.TaxRatio.model10.C4)))
V.TaxRatio.model10.C5$Level<-c(rep(2000,nrow(V.TaxRatio.model10.C5)))
M.TaxRatio.model10.C1$Level<-c(rep(100,nrow(M.TaxRatio.model10.C1)))
M.TaxRatio.model10.C2$Level<-c(rep(200,nrow(M.TaxRatio.model10.C2)))
M.TaxRatio.model10.C3$Level<-c(rep(500,nrow(M.TaxRatio.model10.C3)))
M.TaxRatio.model10.C4$Level<-c(rep(1000,nrow(M.TaxRatio.model10.C4)))
M.TaxRatio.model10.C5$Level<-c(rep(2000,nrow(M.TaxRatio.model10.C5)))


# merge datasets
V.TaxRatio.model10Levels<-do.call("rbind", list(V.TaxRatio.model10.C1,V.TaxRatio.model10.C2,V.TaxRatio.model10.C3,V.TaxRatio.model10.C4,V.TaxRatio.model10.C5))
M.TaxRatio.model10Levels<-do.call("rbind", list(M.TaxRatio.model10.C1,M.TaxRatio.model10.C2,M.TaxRatio.model10.C3,M.TaxRatio.model10.C4, M.TaxRatio.model10.C5))


# summarize for plotting
V.TaxRatio.model10.Seq.p<-data_summary(V.TaxRatio.model10Levels, varname="value", groupnames=c("Level", "Var2"))
M.TaxRatio.model10.Seq.p<-data_summary(M.TaxRatio.model10Levels, varname="value", groupnames=c("Level", "Var2"))
V.TaxRatio.model10.Seq.p2<-data_summary2(V.TaxRatio.model10Levels, varname="value", groupnames=c("Level", "Var2"))
M.TaxRatio.model10.Seq.p2<-data_summary2(M.TaxRatio.model10Levels, varname="value", groupnames=c("Level", "Var2"))
# plot
ggplot(V.TaxRatio.model10.Seq.p, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(100)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(100))+
  geom_point(position=position_dodge(100))+
  xlab("Sequencing Depth")+
  ylab("Variance taxRatio (Mean +/- sd)")+
  theme_classic()

ggplot(M.TaxRatio.model10.Seq.p, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(100)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(100))+
  geom_point(position=position_dodge(100))+
  xlab("Sequencing Depth")+
  ylab("Median taxRatio (Mean +/- sd)")+
  theme_classic()
ggplot(V.TaxRatio.model10.Seq.p2, aes(x=Level, y=log10(value), group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=log10(low), ymax=log10(high)), width=.1, position=position_dodge(100)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(100))+
  geom_point(position=position_dodge(100))+
  xlab("Sequencing Depth")+
  ylab("log10 Variance taxRatio (Median +/- min/max)")+
  theme_classic()

ggplot(M.TaxRatio.model10.Seq.p2, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(100)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(100))+
  geom_point(position=position_dodge(100))+
  xlab("Sequencing Depth")+
  ylab("Median taxRatio (Median +/- min/max)")+
  theme_classic()

```

```{r}
summary(aov(M.TaxRatio.model10Levels$value~M.TaxRatio.model10Levels$Level*M.TaxRatio.model10Levels$Var2))

TukeyHSD(aov(M.TaxRatio.model10.Seq.p$value~M.TaxRatio.model10.Seq.p$Level*M.TaxRatio.model10.Seq.p$Var2))

summary(aov(V.TaxRatio.model10Levels$value~V.TaxRatio.model10Levels$Level*V.TaxRatio.model10Levels$Var2))

TukeyHSD(aov(V.TaxRatio.model10Levels$value~V.TaxRatio.model10Levels$Level*V.TaxRatio.model10Levels$Var2))
```

```{r}
model10C1.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1,method2, "CategoryRratio"))
model10C1.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1, method2, "F1Rratio"))
model10C1.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1, method2, "F2Rratio"))
model10C1.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1, method2, "F3Rratio"))
model10C1.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1, method2, "F4Rratio"))
model10C1.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1,method2, "F5Rratio"))


model10C2.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C2, method2, "CategoryRratio"))
model10C2.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C2, method2, "F1Rratio"))
model10C2.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C2, method2, "F2Rratio"))
model10C2.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C2, method2, "F3Rratio"))
model10C2.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C2, method2, "F4Rratio"))
model10C2.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C2, method2, "F5Rratio"))


model10C3.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C3, method2, "CategoryRratio"))
model10C3.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C3, method2, "F1Rratio"))
model10C3.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C3, method2, "F2Rratio"))
model10C3.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C3, method2, "F3Rratio"))
model10C3.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C3, method2, "F4Rratio"))
model10C3.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C3, method2, "F5Rratio"))


model10C4.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C4, method2, "CategoryRratio"))
model10C4.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C4, method2, "F1Rratio"))
model10C4.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C4, method2, "F2Rratio"))
model10C4.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C4, method2, "F3Rratio"))
model10C4.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C4, method2, "F4Rratio"))
model10C4.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C4, method2, "F5Rratio"))

model10C5.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C5, method2, "CategoryRratio"))
model10C5.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C5, method2, "F1Rratio"))
model10C5.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C5, method2, "F2Rratio"))
model10C5.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C5, method2, "F3Rratio"))
model10C5.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C5, method2, "F4Rratio"))
model10C5.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C5, method2, "F5Rratio"))

model10C1.Permanova.model$Level<-c(rep(100,10))
model10C1.Permanova.env1$Level<-c(rep(100,10))
model10C1.Permanova.env2$Level<-c(rep(100,10))
model10C1.Permanova.env3$Level<-c(rep(100,10))
model10C1.Permanova.env4$Level<-c(rep(100,10))
model10C1.Permanova.env5$Level<-c(rep(100,10))

model10C2.Permanova.model$Level<-c(rep(200,10))
model10C2.Permanova.env1$Level<-c(rep(200,10))
model10C2.Permanova.env2$Level<-c(rep(200,10))
model10C2.Permanova.env3$Level<-c(rep(200,10))
model10C2.Permanova.env4$Level<-c(rep(200,10))
model10C2.Permanova.env5$Level<-c(rep(200,10))

model10C3.Permanova.model$Level<-c(rep(500,10))
model10C3.Permanova.env1$Level<-c(rep(500,10))
model10C3.Permanova.env2$Level<-c(rep(500,10))
model10C3.Permanova.env3$Level<-c(rep(500,10))
model10C3.Permanova.env4$Level<-c(rep(500,10))
model10C3.Permanova.env5$Level<-c(rep(500,10))

model10C4.Permanova.model$Level<-c(rep(1000,10))
model10C4.Permanova.env1$Level<-c(rep(1000,10))
model10C4.Permanova.env2$Level<-c(rep(1000,10))
model10C4.Permanova.env3$Level<-c(rep(1000,10))
model10C4.Permanova.env4$Level<-c(rep(1000,10))
model10C4.Permanova.env5$Level<-c(rep(1000,10))

model10C5.Permanova.model$Level<-c(rep(2000,10))
model10C5.Permanova.env1$Level<-c(rep(2000,10))
model10C5.Permanova.env2$Level<-c(rep(2000,10))
model10C5.Permanova.env3$Level<-c(rep(2000,10))
model10C5.Permanova.env4$Level<-c(rep(2000,10))
model10C5.Permanova.env5$Level<-c(rep(2000,10))

permanova.model10CLevels.model<-do.call("rbind", list(model10C1.Permanova.model,model10C2.Permanova.model,model10C3.Permanova.model,model10C4.Permanova.model,model10C5.Permanova.model))
permanova.model10CLevels.env1<-do.call("rbind", list(model10C1.Permanova.env1,model10C2.Permanova.env1,model10C3.Permanova.env1,model10C4.Permanova.env1,model10C5.Permanova.env1))
permanova.model10CLevels.env2<-do.call("rbind", list(model10C1.Permanova.env2,model10C2.Permanova.env2,model10C3.Permanova.env2,model10C4.Permanova.env2,model10C5.Permanova.env2))
permanova.model10CLevels.env3<-do.call("rbind", list(model10C1.Permanova.env3,model10C2.Permanova.env3,model10C3.Permanova.env3,model10C4.Permanova.env3,model10C5.Permanova.env3))
permanova.model10CLevels.env4<-do.call("rbind", list(model10C1.Permanova.env4,model10C2.Permanova.env4,model10C3.Permanova.env4,model10C4.Permanova.env4,model10C5.Permanova.env4))
permanova.model10CLevels.env5<-do.call("rbind", list(model10C1.Permanova.env5,model10C2.Permanova.env5,model10C3.Permanova.env5,model10C4.Permanova.env5,model10C5.Permanova.env5))

permanova.model10CLevels.model<-melt(permanova.model10CLevels.model, id.vars = "Level", measure.vars = method2)
permanova.model10CLevels.env1<-melt(permanova.model10CLevels.env1, id.vars = "Level", measure.vars = method2)
permanova.model10CLevels.env2<-melt(permanova.model10CLevels.env2, id.vars = "Level", measure.vars = method2)
permanova.model10CLevels.env3<-melt(permanova.model10CLevels.env3, id.vars = "Level", measure.vars = method2)
permanova.model10CLevels.env4<-melt(permanova.model10CLevels.env4, id.vars = "Level", measure.vars = method2)
permanova.model10CLevels.env5<-melt(permanova.model10CLevels.env5, id.vars = "Level", measure.vars = method2)

permanova.model10CLevels.modelP<-data_summary2(permanova.model10CLevels.model, varname="value", groupnames=c("Level", "variable"))
permanova.model10CLevels.env1P<-data_summary2(permanova.model10CLevels.env1, varname="value", groupnames=c("Level", "variable"))
permanova.model10CLevels.env2P<-data_summary2(permanova.model10CLevels.env2, varname="value", groupnames=c("Level", "variable"))
permanova.model10CLevels.env3P<-data_summary2(permanova.model10CLevels.env3, varname="value", groupnames=c("Level", "variable"))
permanova.model10CLevels.env4P<-data_summary2(permanova.model10CLevels.env4, varname="value", groupnames=c("Level", "variable"))
permanova.model10CLevels.env5P<-data_summary2(permanova.model10CLevels.env5, varname="value", groupnames=c("Level", "variable"))


ggplot(permanova.model10CLevels.modelP, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Permanova modelRatio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10CLevels.env1P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Permanova env1Ratio (Median +/- min/max)")+
  theme_classic()


ggplot(permanova.model10CLevels.env2P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Permanova env2Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10CLevels.env3P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Permanova env3Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10CLevels.env4P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Permanova env4Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10CLevels.env5P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(30)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(30))+
  geom_point(position=position_dodge(30))+
  xlab("Degree of Structure")+
  ylab("Permanova env5Ratio (Median +/- min/max)")+
  theme_classic()
```

Stats:
```{r}
summary(aov(permanova.model10CLevels.model$value~permanova.model10CLevels.model$Level*permanova.model10CLevels.model$variable))

TukeyHSD(aov(permanova.model10CLevels.model$value~permanova.model10CLevels.model$Level*permanova.model10CLevels.model$variable))

summary(aov(permanova.model10CLevels.env1$value~permanova.model10CLevels.env1$Level*permanova.model10CLevels.env1$variable))

TukeyHSD(aov(permanova.model10CLevels.env1$value~permanova.model10CLevels.env1$Level*permanova.model10CLevels.env1$variable))

summary(aov(permanova.model10CLevels.env2$value~permanova.model10CLevels.env2$Level*permanova.model10CLevels.env2$variable))

TukeyHSD(aov(permanova.model10CLevels.env2$value~permanova.model10CLevels.env2$Level*permanova.model10CLevels.env2$variable))

summary(aov(permanova.model10CLevels.env3$value~permanova.model10CLevels.env3$Level*permanova.model10CLevels.env3$variable))

TukeyHSD(aov(permanova.model10CLevels.env3$value~permanova.model10CLevels.env3$Level*permanova.model10CLevels.env3$variable))

summary(aov(permanova.model10CLevels.env4$value~permanova.model10CLevels.env4$Level*permanova.model10CLevels.env4$variable))

TukeyHSD(aov(permanova.model10CLevels.env4$value~permanova.model10CLevels.env4$Level*permanova.model10CLevels.env4$variable))

summary(aov(permanova.model10CLevels.env5$value~permanova.model10CLevels.env5$Level*permanova.model10CLevels.env5$variable))

TukeyHSD(aov(permanova.model10CLevels.env5$value~permanova.model10CLevels.env5$Level*permanova.model10CLevels.env5$variable))



leveneTest(permanova.model10CLevels.model$value~permanova.model10CLevels.model$variable)
leveneTest(permanova.model10CLevels.env1$value~permanova.model10CLevels.env1$variable)
leveneTest(permanova.model10CLevels.env2$value~permanova.model10CLevels.env2$variable)
leveneTest(permanova.model10CLevels.env3$value~permanova.model10CLevels.env3$variable)
leveneTest(permanova.model10CLevels.env4$value~permanova.model10CLevels.env4$variable)
leveneTest(permanova.model10CLevels.env5$value~permanova.model10CLevels.env5$variable)





```

Thus far we have looked at the effect of normalization on detecting important relationships between taxa and the modeled environment. Now let's look at how relationships among taxa are affected by normalization (or lack therof). We will be conducting a network analysis for this section. In the following section, each taxon is represented by a vertice, and relationships between taxa are represented as edges. To construct a network, we typically correlate taxa, then choose a threshold for netowrk construction. Taxa that have a high enough correlation coefficient are connected with an edge, and those falling below the threshold are not connected. We are generally interested in 3 network metrics:

In this analysis, a network is constructed using two methods: one has a preset correlation coefficient that determines which taxa are included (is called "static"); the other includes a dynamic threshold to reach a certain present number of edges (called "dynamic"). A common statistic that is used in network theory, but that also is interpretable in ecology is modularity. This is a measure of internal structure of the community. We can use the modules to interpret functional groups, identify covarying taxa, and potentially hypothesize functional relationships among organisms. However, measures of modularity are very sensitive to the number of edges and the number of vertices in a network. In our experiment, all of the networks have the same number of vertices because they come from the same community. But normalization may affect the strength of the relatioships among taxa, so it might affect the number of edges. The dynamic method is designed to ensure that the number of edges is the same across all networks constructed. To do this, the threshold for inclusion is varied. So for interpretation, we need to look at both the modularity, and the threshold level. Even if the modularity is the same, if the threshold from one normalization is much lower than another, it means that this method reduces the strength of the relatioships. The static method includes all relationships that meet a certain threshold. This can give a clear indication also of how well relationships are preserved through the normalization process.

Let's look at an example of this in practice. We'll take the same experiments we've just generated to understand how count normalization methods interact with various community features and affect our ability to accurately describe the relationship between taxa and their environment; we'll use the network analyses to understand how these same community and sampling features affect our ability to infer species-species interactions.

Community structure:

Typically the first thing that we do for a network analysis is to correlate taxa to one another. Model.Microbiome does this automatically, using a pearson correlation. All we need to do is access this information using the getTaxCor.Tab function. This function outputs an object that contains a table of variances and median correlation ratios. These are calculated in a similar fashion as the correlation ratios of taxa to the environment;

let a be the correlation coefficient between two taxa in the reference.
let b be the correlation coefficient between two taxa after normalization treatment.

The correlation ratio is defined as b/a

Thus we are extracting the variance of all ratios for each normalization method; and the median ratio of all normalization methods

So with that, let's look at how community structure interacts with our normalization procedure to affect the accuracy of taxon-taxon correlation measures.


```{r}
method3<-c("raw", method) # make a list for outputs

```

A quick note for interpretation- we are looking for the variance to be as low as possible; but we are looking for the median value to be as close to 1 as possible. With that:




# Expt 3: Normalization vs Sequencing Variance

```{r}
# sequencing variance ####
model10.C1V1<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=50, method)

model10.C1V2<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=100, method)

model10.C1V3<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=250, method)

model10.C1V4<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=500, method)

model10.C1V5<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=1000, method)


```


```{r}
# sequencing variants LII ####

SummaryLII.model10.C1V1<-as.data.frame(Summarize.LII(model10.C1V1, method2)) # actually C3 ...
SummaryLII.model10.C1V2<-as.data.frame(Summarize.LII(model10.C1V2, method2)) 
SummaryLII.model10.C1V3<-as.data.frame(Summarize.LII(model10.C1V3, method2))
SummaryLII.model10.C1V4<-as.data.frame(Summarize.LII(model10.C1V4, method2))
SummaryLII.model10.C1V5<-as.data.frame(Summarize.LII(model10.C1V5, method2))
# prepare data for merging datasets
SummaryLII.model10.C1V1$Level<-c(rep(0.1,10))
SummaryLII.model10.C1V1<-melt(SummaryLII.model10.C1V1, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C1V2$Level<-c(rep(0.2,10))
SummaryLII.model10.C1V2<-melt(SummaryLII.model10.C1V2, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C1V3$Level<-c(rep(0.5,10))
SummaryLII.model10.C1V3<-melt(SummaryLII.model10.C1V3, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C1V4$Level<-c(rep(1,10))
SummaryLII.model10.C1V4<-melt(SummaryLII.model10.C1V4, id.vars = "Level", measure.vars = method2)
SummaryLII.model10.C1V5$Level<-c(rep(2,10))
SummaryLII.model10.C1V5<-melt(SummaryLII.model10.C1V5, id.vars = "Level", measure.vars = method2)
# merge datasets
model10.SeqVarLevels<-do.call("rbind", list(SummaryLII.model10.C1V1,SummaryLII.model10.C1V2,SummaryLII.model10.C1V3,SummaryLII.model10.C1V4,SummaryLII.model10.C1V5))

# summarize for plotting
model10.SeqVarLevels.summary<-data_summary(model10.SeqVarLevels, varname="value", groupnames=c("Level", "variable"))
model10.SeqVarLevels.summary2<-data_summary2(model10.SeqVarLevels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(model10.SeqVarLevels.summary, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.1)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.1))+
  geom_point(position=position_dodge(.1))+
  xlab("Sequencing Variance")+
  ylab("LII Value (Mean +/- sd)")+
  theme_classic()

ggplot(model10.SeqVarLevels.summary2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.1)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.1))+
  geom_point(position=position_dodge(.1))+
  xlab("Sequencing Variance")+
  ylab("LII Value (Median +/- min/max)")+
  theme_classic()
```
```{r}

summary(aov(SummaryLII.model10.C1V1$value~SummaryLII.model10.C1V1$Level*SummaryLII.model10.C1V1$variable))
TukeyHSD(aov(SummaryLII.model10.C1V1$value~SummaryLII.model10.C1V1$Level*SummaryLII.model10.C1V1$variable))

```

```{r}
# sequencing variants lm ####

SM.lmRatio.model10.C1V1<-as.data.frame(Summarize.lmRatiotab.Median(model10.C1V1,method2))
SV.lmRatio.model10.C1V1<-as.data.frame(Summarize.lmRatiotab.Var(model10.C1V1,method2))
SM.lmRatio.model10.C1V2<-as.data.frame(Summarize.lmRatiotab.Median(model10.C1V2,method2))
SV.lmRatio.model10.C1V2<-as.data.frame(Summarize.lmRatiotab.Var(model10.C1V2,method2))
SM.lmRatio.model10.C1V3<-as.data.frame(Summarize.lmRatiotab.Median(model10.C1V3,method2))
SV.lmRatio.model10.C1V3<-as.data.frame(Summarize.lmRatiotab.Var(model10.C1V3,method2))
SM.lmRatio.model10.C1V4<-as.data.frame(Summarize.lmRatiotab.Median(model10.C1V4,method2))
SV.lmRatio.model10.C1V4<-as.data.frame(Summarize.lmRatiotab.Var(model10.C1V4,method2))
SM.lmRatio.model10.C1V5<-as.data.frame(Summarize.lmRatiotab.Median(model10.C1V5,method2))
SV.lmRatio.model10.C1V5<-as.data.frame(Summarize.lmRatiotab.Var(model10.C1V5,method2))
# prepare data for merging datasets

SM.lmRatio.model10.C1V1$Level<-c(rep(0.1,10))
SV.lmRatio.model10.C1V1$Level<-c(rep(0.1,10))
SM.lmRatio.model10.C1V2$Level<-c(rep(0.2,10))
SV.lmRatio.model10.C1V2$Level<-c(rep(0.2,10))
SM.lmRatio.model10.C1V3$Level<-c(rep(0.5,10))
SV.lmRatio.model10.C1V3$Level<-c(rep(0.5,10))
SM.lmRatio.model10.C1V4$Level<-c(rep(1,10))
SV.lmRatio.model10.C1V4$Level<-c(rep(1,10))
SM.lmRatio.model10.C1V5$Level<-c(rep(2,10))
SV.lmRatio.model10.C1V5$Level<-c(rep(2,10))

SM.lmRatio.model10.C1V1<-melt(SM.lmRatio.model10.C1V1, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C1V1<-melt(SV.lmRatio.model10.C1V1, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C1V2<-melt(SM.lmRatio.model10.C1V2, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C1V2<-melt(SV.lmRatio.model10.C1V2, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C1V3<-melt(SM.lmRatio.model10.C1V3, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C1V3<-melt(SV.lmRatio.model10.C1V3, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C1V4<-melt(SM.lmRatio.model10.C1V4, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C1V4<-melt(SV.lmRatio.model10.C1V4, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.C1V5<-melt(SM.lmRatio.model10.C1V5, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.C1V5<-melt(SV.lmRatio.model10.C1V5, id.vars = "Level", measure.vars = method2)

# merge datasets
SM.lmRatio.model10.CVLevels<-do.call("rbind", list(SM.lmRatio.model10.C1V1,SM.lmRatio.model10.C1V2,SM.lmRatio.model10.C1V3,SM.lmRatio.model10.C1V4, SM.lmRatio.model10.C1V5))
SV.lmRatio.model10.CVLevels<-do.call("rbind", list(SV.lmRatio.model10.C1V1,SV.lmRatio.model10.C1V2,SV.lmRatio.model10.C1V3,SV.lmRatio.model10.C1V4, SV.lmRatio.model10.C1V5))


# summarize for plotting
SM.lmRatio.model10CV.p<-data_summary(SM.lmRatio.model10.CVLevels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10CV.p<-data_summary(SV.lmRatio.model10.CVLevels, varname="value", groupnames=c("Level", "variable"))
SM.lmRatio.model10CV.p2<-data_summary2(SM.lmRatio.model10.CVLevels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10CV.p2<-data_summary2(SV.lmRatio.model10.CVLevels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(SM.lmRatio.model10CV.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.01)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.01))+
  geom_point(position=position_dodge(.01))+
  xlab("Sequencing Variance")+
  ylab("Median lmRatio (Mean +/- sd)")+
  theme_classic()

ggplot(SV.lmRatio.model10CV.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.01)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.01))+
  geom_point(position=position_dodge(.01))+
  xlab("Sequencing Variance")+
  ylab("Variance lmRatio (Mean +/- sd)")+
  theme_classic()
ggplot(SM.lmRatio.model10CV.p2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.01)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.01))+
  geom_point(position=position_dodge(.01))+
  xlab("Sequencing Variance")+
  ylab("Median lmRatio (Median +/- min/max)")+
  theme_classic()

ggplot(SV.lmRatio.model10CV.p2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.01)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.01))+
  geom_point(position=position_dodge(.01))+
  xlab("Sequencing Variance")+
  ylab("Variance lmRatio (Median +/- min/max)")+
  theme_classic()
```
```{r}

summary(aov(SM.lmRatio.model10.C1V1$value~SM.lmRatio.model10.C1V1$Level*SM.lmRatio.model10.C1V1$variable))
TukeyHSD(aov(SM.lmRatio.model10.C1V1$value~SM.lmRatio.model10.C1V1$Level*SM.lmRatio.model10.C1V1$variable))

summary(aov(SV.lmRatio.model10.C1V1$value~SV.lmRatio.model10.C1V1$Level*SV.lmRatio.model10.C1V1$variable))
TukeyHSD(aov(SV.lmRatio.model10.C1V1$value~SV.lmRatio.model10.C1V1$Level*SV.lmRatio.model10.C1V1$variable))

```

```{r}
#sequence variants lm model ####
SM.lmRatio.model10.MC1V1<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C1V1,method2))
SV.lmRatio.model10.MC1V1<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C1V1,method2))
SM.lmRatio.model10.MC1V2<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C1V2,method2))
SV.lmRatio.model10.MC1V2<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C1V2,method2))
SM.lmRatio.model10.MC1V3<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C1V3,method2))
SV.lmRatio.model10.MC1V3<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C1V3,method2))
SM.lmRatio.model10.MC1V4<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C1V4,method2))
SV.lmRatio.model10.MC1V4<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C1V4,method2))
SM.lmRatio.model10.MC1V5<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C1V5,method2))
SV.lmRatio.model10.MC1V5<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C1V5,method2))
# prepare data for merging datasets

SM.lmRatio.model10.MC1V1$Level<-c(rep(0.1,10))
SV.lmRatio.model10.MC1V1$Level<-c(rep(0.1,10))
SM.lmRatio.model10.MC1V2$Level<-c(rep(0.2,10))
SV.lmRatio.model10.MC1V2$Level<-c(rep(0.2,10))
SM.lmRatio.model10.MC1V3$Level<-c(rep(0.5,10))
SV.lmRatio.model10.MC1V3$Level<-c(rep(0.5,10))
SM.lmRatio.model10.MC1V4$Level<-c(rep(1,10))
SV.lmRatio.model10.MC1V4$Level<-c(rep(1,10))
SM.lmRatio.model10.MC1V5$Level<-c(rep(2,10))
SV.lmRatio.model10.MC1V5$Level<-c(rep(2,10))

SM.lmRatio.model10.MC1V1<-melt(SM.lmRatio.model10.MC1V1, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.MC1V1<-melt(SV.lmRatio.model10.MC1V1, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.MC1V2<-melt(SM.lmRatio.model10.MC1V2, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.MC1V2<-melt(SV.lmRatio.model10.MC1V2, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.MC1V3<-melt(SM.lmRatio.model10.MC1V3, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.MC1V3<-melt(SV.lmRatio.model10.MC1V3, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.MC1V4<-melt(SM.lmRatio.model10.MC1V4, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.MC1V4<-melt(SV.lmRatio.model10.MC1V4, id.vars = "Level", measure.vars = method2)
SM.lmRatio.model10.MC1V5<-melt(SM.lmRatio.model10.MC1V5, id.vars = "Level", measure.vars = method2)
SV.lmRatio.model10.MC1V5<-melt(SV.lmRatio.model10.MC1V5, id.vars = "Level", measure.vars = method2)

# merge datasets
SM.lmRatio.model10.MCVLevels<-do.call("rbind", list(SM.lmRatio.model10.MC1V1,SM.lmRatio.model10.MC1V2,SM.lmRatio.model10.MC1V3,SM.lmRatio.model10.MC1V4, SM.lmRatio.model10.MC1V5))
SV.lmRatio.model10.MCVLevels<-do.call("rbind", list(SV.lmRatio.model10.MC1V1,SV.lmRatio.model10.MC1V2,SV.lmRatio.model10.MC1V3,SV.lmRatio.model10.MC1V4, SV.lmRatio.model10.MC1V5))


# summarize for plotting
SM.lmRatio.model10MCV.p<-data_summary(SM.lmRatio.model10.MCVLevels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10MCV.p<-data_summary(SV.lmRatio.model10.MCVLevels, varname="value", groupnames=c("Level", "variable"))

SM.lmRatio.model10MCV.p2<-data_summary2(SM.lmRatio.model10.MCVLevels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10MCV.p2<-data_summary2(SV.lmRatio.model10.MCVLevels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(SM.lmRatio.model10MCV.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.01)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.01))+
  geom_point(position=position_dodge(.01))+
  xlab("Sequencing Variance")+
  ylab("Median lmRatio (Mean +/- sd)")+
  theme_classic()

ggplot(SV.lmRatio.model10MCV.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(.01)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.01))+
  geom_point(position=position_dodge(.01))+
  xlab("Sequencing Variance")+
  ylab("Variance lmRatio (Mean +/- sd)")+
  theme_classic()

ggplot(SM.lmRatio.model10MCV.p2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.01)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.01))+
  geom_point(position=position_dodge(.01))+
  xlab("Sequencing Variance")+
  ylab("Median lmRatio (Median +/- min/max)")+
  theme_classic()

ggplot(SV.lmRatio.model10MCV.p2, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.01)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(.01))+
  geom_point(position=position_dodge(.01))+
  xlab("Sequencing Variance")+
  ylab("Variance lmRatio (Median +/- min/max)")+
  theme_classic()
```
```{r}

summary(aov(SM.lmRatio.model10.MC1V1$value~SM.lmRatio.model10.MC1V1$Level*SM.lmRatio.model10.MC1V1$variable))
TukeyHSD(aov(SM.lmRatio.model10.MC1V1$value~SM.lmRatio.model10.C1V1$Level*SM.lmRatio.model10.MC1V1$variable))

summary(aov(SV.lmRatio.model10.MC1V1$value~SV.lmRatio.model10.MC1V1$Level*SV.lmRatio.model10.MC1V1$variable))
TukeyHSD(aov(SV.lmRatio.model10.MC1V1$value~SV.lmRatio.model10.MC1V1$Level*SV.lmRatio.model10.MC1V1$variable))

```


```{r}
# sequencing variants taxcor ####

TaxRatio.model10.C1V1<-getTaxCor.Tab(model10.C1V1,method2)
TaxRatio.model10.C1V2<-getTaxCor.Tab(model10.C1V2,method2)
TaxRatio.model10.C1V3<-getTaxCor.Tab(model10.C1V3,method2)
TaxRatio.model10.C1V4<-getTaxCor.Tab(model10.C1V4,method2)
TaxRatio.model10.C1V5<-getTaxCor.Tab(model10.C1V5,method2)

V.TaxRatio.model10.C1V1<-melt(TaxRatio.model10.C1V1$V.tax)
M.TaxRatio.model10.C1V1<-melt(TaxRatio.model10.C1V1$Median.tax)
V.TaxRatio.model10.C1V2<-melt(TaxRatio.model10.C1V2$V.tax)
M.TaxRatio.model10.C1V2<-melt(TaxRatio.model10.C1V2$Median.tax)
V.TaxRatio.model10.C1V3<-melt(TaxRatio.model10.C1V3$V.tax)
M.TaxRatio.model10.C1V3<-melt(TaxRatio.model10.C1V3$Median.tax)
V.TaxRatio.model10.C1V4<-melt(TaxRatio.model10.C1V4$V.tax)
M.TaxRatio.model10.C1V4<-melt(TaxRatio.model10.C1V4$Median.tax)
V.TaxRatio.model10.C1V5<-melt(TaxRatio.model10.C1V5$V.tax)
M.TaxRatio.model10.C1V5<-melt(TaxRatio.model10.C1V5$Median.tax)

V.TaxRatio.model10.C1V1<-as.data.frame(V.TaxRatio.model10.C1V1)
M.TaxRatio.model10.C1V1<-as.data.frame(M.TaxRatio.model10.C1V1)
V.TaxRatio.model10.C1V2<-as.data.frame(V.TaxRatio.model10.C1V2)
M.TaxRatio.model10.C1V2<-as.data.frame(M.TaxRatio.model10.C1V2)
V.TaxRatio.model10.C1V3<-as.data.frame(V.TaxRatio.model10.C1V3)
M.TaxRatio.model10.C1V3<-as.data.frame(M.TaxRatio.model10.C1V3)
V.TaxRatio.model10.C1V4<-as.data.frame(V.TaxRatio.model10.C1V4)
M.TaxRatio.model10.C1V4<-as.data.frame(M.TaxRatio.model10.C1V4)
V.TaxRatio.model10.C1V5<-as.data.frame(V.TaxRatio.model10.C1V5)
M.TaxRatio.model10.C1V5<-as.data.frame(M.TaxRatio.model10.C1V5)

# prepare data for merging datasets

V.TaxRatio.model10.C1V1$Level<-c(rep(0.1,nrow(V.TaxRatio.model10.C1V1)))
V.TaxRatio.model10.C1V2$Level<-c(rep(0.2,nrow(V.TaxRatio.model10.C1V2)))
V.TaxRatio.model10.C1V3$Level<-c(rep(0.5,nrow(V.TaxRatio.model10.C1V3)))
V.TaxRatio.model10.C1V4$Level<-c(rep(1,nrow(V.TaxRatio.model10.C1V4)))
V.TaxRatio.model10.C1V5$Level<-c(rep(2,nrow(V.TaxRatio.model10.C1V5)))
M.TaxRatio.model10.C1V1$Level<-c(rep(0.1,nrow(M.TaxRatio.model10.C1V1)))
M.TaxRatio.model10.C1V2$Level<-c(rep(0.2,nrow(M.TaxRatio.model10.C1V2)))
M.TaxRatio.model10.C1V3$Level<-c(rep(0.5,nrow(M.TaxRatio.model10.C1V3)))
M.TaxRatio.model10.C1V4$Level<-c(rep(1,nrow(M.TaxRatio.model10.C1V4)))
M.TaxRatio.model10.C1V5$Level<-c(rep(2,nrow(M.TaxRatio.model10.C1V5)))


# merge datasets
V.TaxRatio.model10VLevels<-do.call("rbind", list(V.TaxRatio.model10.C1V1,V.TaxRatio.model10.C1V2,V.TaxRatio.model10.C1V3,V.TaxRatio.model10.C1V4,V.TaxRatio.model10.C1V5))
M.TaxRatio.model10VLevels<-do.call("rbind", list(M.TaxRatio.model10.C1V1,M.TaxRatio.model10.C1V2,M.TaxRatio.model10.C1V3,M.TaxRatio.model10.C1V4, M.TaxRatio.model10.C1V5))


# summarize for plotting
V.TaxRatio.model10.SeqV.p<-data_summary(V.TaxRatio.model10VLevels, varname="value", groupnames=c("Level", "Var2"))
M.TaxRatio.model10.SeqV.p<-data_summary(M.TaxRatio.model10VLevels, varname="value", groupnames=c("Level", "Var2"))
# plot

V.TaxRatio.model10.SeqV.p2<-data_summary2(V.TaxRatio.model10VLevels, varname="value", groupnames=c("Level", "Var2"))
M.TaxRatio.model10.SeqV.p2<-data_summary2(M.TaxRatio.model10VLevels, varname="value", groupnames=c("Level", "Var2"))
# plot
ggplot(V.TaxRatio.model10.SeqV.p, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(0.1)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Sequencing Depth")+
  ylab("log10 Variance taxRatio (Mean +/- sd)")+
  theme_classic()

ggplot(M.TaxRatio.model10.SeqV.p, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.1, position=position_dodge(0.1)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Sequencing Depth")+
  ylab("Median taxRatio (Mean +/- sd)")+
  theme_classic()

ggplot(V.TaxRatio.model10.SeqV.p2, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(0.1)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Sequencing Depth")+
  ylab("log10 Variance taxRatio (Median +/- min/max)")+
  theme_classic()

ggplot(M.TaxRatio.model10.SeqV.p2, aes(x=Level, y=value, group = Var2, color=Var2))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(0.1)) +
  scale_color_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Sequencing Depth")+
  ylab("Median taxRatio (Median +/- min/max)")+
  theme_classic()

```


```{r}

summary(aov(V.TaxRatio.model10VLevels$value~V.TaxRatio.model10VLevels$Level*V.TaxRatio.model10VLevels$Var2))
TukeyHSD(aov(V.TaxRatio.model10VLevels$value~V.TaxRatio.model10VLevels$Level*V.TaxRatio.model10VLevels$Var2))

summary(aov(M.TaxRatio.model10VLevels$value~M.TaxRatio.model10VLevels$Level*M.TaxRatio.model10VLevels$Var2))
TukeyHSD(aov(M.TaxRatio.model10VLevels$value~M.TaxRatio.model10VLevels$Var2))

```

```{r}
# sequencing variants PERMANOVA ####

model10C1V1.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V1,method2, "CategoryRratio"))
model10C1V1.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V1, method2, "F1Rratio"))
model10C1V1.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V1, method2, "F2Rratio"))
model10C1V1.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V1, method2, "F3Rratio"))
model10C1V1.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V1, method2, "F4Rratio"))
model10C1V1.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V1,method2, "F5Rratio"))


model10C1V2.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V2, method2, "CategoryRratio"))
model10C1V2.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V2, method2, "F1Rratio"))
model10C1V2.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V2, method2, "F2Rratio"))
model10C1V2.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V2, method2, "F3Rratio"))
model10C1V2.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V2, method2, "F4Rratio"))
model10C1V2.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V2, method2, "F5Rratio"))


model10C1V3.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V3, method2, "CategoryRratio"))
model10C1V3.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V3, method2, "F1Rratio"))
model10C1V3.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V3, method2, "F2Rratio"))
model10C1V3.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V3, method2, "F3Rratio"))
model10C1V3.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V3, method2, "F4Rratio"))
model10C1V3.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V3, method2, "F5Rratio"))


model10C1V4.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V4, method2, "CategoryRratio"))
model10C1V4.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V4, method2, "F1Rratio"))
model10C1V4.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V4, method2, "F2Rratio"))
model10C1V4.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V4, method2, "F3Rratio"))
model10C1V4.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V4, method2, "F4Rratio"))
model10C1V4.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V4, method2, "F5Rratio"))

model10C1V5.Permanova.model<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V5, method2, "CategoryRratio"))
model10C1V5.Permanova.env1<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V5, method2, "F1Rratio"))
model10C1V5.Permanova.env2<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V5, method2, "F2Rratio"))
model10C1V5.Permanova.env3<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V5, method2, "F3Rratio"))
model10C1V5.Permanova.env4<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V5, method2, "F4Rratio"))
model10C1V5.Permanova.env5<-as.data.frame(Summarize.PERMANOVA.Rratio(model10.C1V5, method2, "F5Rratio"))

model10C1V1.Permanova.model$Level<-c(rep(0.1,10))
model10C1V1.Permanova.env1$Level<-c(rep(.100,10))
model10C1V1.Permanova.env2$Level<-c(rep(.100,10))
model10C1V1.Permanova.env3$Level<-c(rep(.100,10))
model10C1V1.Permanova.env4$Level<-c(rep(.100,10))
model10C1V1.Permanova.env5$Level<-c(rep(.100,10))

model10C1V2.Permanova.model$Level<-c(rep(.200,10))
model10C1V2.Permanova.env1$Level<-c(rep(.200,10))
model10C1V2.Permanova.env2$Level<-c(rep(.200,10))
model10C1V2.Permanova.env3$Level<-c(rep(.200,10))
model10C1V2.Permanova.env4$Level<-c(rep(.200,10))
model10C1V2.Permanova.env5$Level<-c(rep(.200,10))

model10C1V3.Permanova.model$Level<-c(rep(.500,10))
model10C1V3.Permanova.env1$Level<-c(rep(.500,10))
model10C1V3.Permanova.env2$Level<-c(rep(.500,10))
model10C1V3.Permanova.env3$Level<-c(rep(.500,10))
model10C1V3.Permanova.env4$Level<-c(rep(.500,10))
model10C1V3.Permanova.env5$Level<-c(rep(.500,10))

model10C1V4.Permanova.model$Level<-c(rep(1.000,10))
model10C1V4.Permanova.env1$Level<-c(rep(1.000,10))
model10C1V4.Permanova.env2$Level<-c(rep(1.000,10))
model10C1V4.Permanova.env3$Level<-c(rep(1.000,10))
model10C1V4.Permanova.env4$Level<-c(rep(1.000,10))
model10C1V4.Permanova.env5$Level<-c(rep(1.000,10))

model10C1V5.Permanova.model$Level<-c(rep(2.000,10))
model10C1V5.Permanova.env1$Level<-c(rep(2.000,10))
model10C1V5.Permanova.env2$Level<-c(rep(2.000,10))
model10C1V5.Permanova.env3$Level<-c(rep(2.000,10))
model10C1V5.Permanova.env4$Level<-c(rep(2.000,10))
model10C1V5.Permanova.env5$Level<-c(rep(2.000,10))

permanova.model10CVLevels.model<-do.call("rbind", list(model10C1V1.Permanova.model,model10C1V2.Permanova.model,model10C1V3.Permanova.model,model10C1V4.Permanova.model,model10C1V5.Permanova.model))
permanova.model10CVLevels.env1<-do.call("rbind", list(model10C1V1.Permanova.env1,model10C1V2.Permanova.env1,model10C1V3.Permanova.env1,model10C1V4.Permanova.env1,model10C1V5.Permanova.env1))
permanova.model10CVLevels.env2<-do.call("rbind", list(model10C1V1.Permanova.env2,model10C1V2.Permanova.env2,model10C1V3.Permanova.env2,model10C1V4.Permanova.env2,model10C1V5.Permanova.env2))
permanova.model10CVLevels.env3<-do.call("rbind", list(model10C1V1.Permanova.env3,model10C1V2.Permanova.env3,model10C1V3.Permanova.env3,model10C1V4.Permanova.env3,model10C1V5.Permanova.env3))
permanova.model10CVLevels.env4<-do.call("rbind", list(model10C1V1.Permanova.env4,model10C1V2.Permanova.env4,model10C1V3.Permanova.env4,model10C1V4.Permanova.env4,model10C1V5.Permanova.env4))
permanova.model10CVLevels.env5<-do.call("rbind", list(model10C1V1.Permanova.env5,model10C1V2.Permanova.env5,model10C1V3.Permanova.env5,model10C1V4.Permanova.env5,model10C1V5.Permanova.env5))

permanova.model10CVLevels.model<-melt(permanova.model10CVLevels.model, id.vars = "Level", measure.vars = method2)
permanova.model10CVLevels.env1<-melt(permanova.model10CVLevels.env1, id.vars = "Level", measure.vars = method2)
permanova.model10CVLevels.env2<-melt(permanova.model10CVLevels.env2, id.vars = "Level", measure.vars = method2)
permanova.model10CVLevels.env3<-melt(permanova.model10CVLevels.env3, id.vars = "Level", measure.vars = method2)
permanova.model10CVLevels.env4<-melt(permanova.model10CVLevels.env4, id.vars = "Level", measure.vars = method2)
permanova.model10CVLevels.env5<-melt(permanova.model10CVLevels.env5, id.vars = "Level", measure.vars = method2)

permanova.model10CVLevels.modelP<-data_summary2(permanova.model10CVLevels.model, varname="value", groupnames=c("Level", "variable"))
permanova.model10CVLevels.env1P<-data_summary2(permanova.model10CVLevels.env1, varname="value", groupnames=c("Level", "variable"))
permanova.model10CVLevels.env2P<-data_summary2(permanova.model10CVLevels.env2, varname="value", groupnames=c("Level", "variable"))
permanova.model10CVLevels.env3P<-data_summary2(permanova.model10CVLevels.env3, varname="value", groupnames=c("Level", "variable"))
permanova.model10CVLevels.env4P<-data_summary2(permanova.model10CVLevels.env4, varname="value", groupnames=c("Level", "variable"))
permanova.model10CVLevels.env5P<-data_summary2(permanova.model10CVLevels.env5, varname="value", groupnames=c("Level", "variable"))


ggplot(permanova.model10CVLevels.modelP, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.01)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.01))+
  geom_point(position=position_dodge(0.01))+
  xlab("Degree of Structure")+
  ylab("Permanova modelRatio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10CVLevels.env1P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(0.1)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Degree of Structure")+
  ylab("Permanova env1Ratio (Median +/- min/max)")+
  theme_classic()


ggplot(permanova.model10CVLevels.env2P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(0.1)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Degree of Structure")+
  ylab("Permanova env2Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10CVLevels.env3P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(0.1)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Degree of Structure")+
  ylab("Permanova env3Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10CVLevels.env4P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(0.1)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Degree of Structure")+
  ylab("Permanova env4Ratio (Median +/- min/max)")+
  theme_classic()

ggplot(permanova.model10CVLevels.env5P, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(0.1)) +
  scale_colour_viridis_d(direction=-1)+
  geom_line(position=position_dodge(0.1))+
  geom_point(position=position_dodge(0.1))+
  xlab("Degree of Structure")+
  ylab("Permanova env5Ratio (Median +/- min/max)")+
  theme_classic()

```


```{r}
summary(aov(permanova.model10CVLevels.model$value~permanova.model10CVLevels.model$Level*permanova.model10CVLevels.model$variable))

TukeyHSD(aov(permanova.model10CVLevels.model$value~permanova.model10CVLevels.model$Level*permanova.model10CVLevels.model$variable))

summary(aov(permanova.model10CVLevels.env1$value~permanova.model10CVLevels.env1$Level*permanova.model10CVLevels.env1$variable))

TukeyHSD(aov(permanova.model10CVLevels.env1$value~permanova.model10CVLevels.env1$Level*permanova.model10CVLevels.env1$variable))

summary(aov(permanova.model10CVLevels.env2$value~permanova.model10CVLevels.env2$Level*permanova.model10CVLevels.env2$variable))

TukeyHSD(aov(permanova.model10CVLevels.env2$value~permanova.model10CVLevels.env2$Level*permanova.model10CVLevels.env2$variable))

summary(aov(permanova.model10CVLevels.env3$value~permanova.model10CVLevels.env3$Level*permanova.model10CVLevels.env3$variable))

TukeyHSD(aov(permanova.model10CVLevels.env3$value~permanova.model10CVLevels.env3$Level*permanova.model10CVLevels.env3$variable))

summary(aov(permanova.model10CVLevels.env4$value~permanova.model10CVLevels.env4$Level*permanova.model10CVLevels.env4$variable))

TukeyHSD(aov(permanova.model10CVLevels.env4$value~permanova.model10CVLevels.env4$Level*permanova.model10CVLevels.env4$variable))

summary(aov(permanova.model10CVLevels.env5$value~permanova.model10CVLevels.env5$Level*permanova.model10CVLevels.env5$variable))

TukeyHSD(aov(permanova.model10CVLevels.env5$value~permanova.model10CVLevels.env5$Level*permanova.model10CVLevels.env5$variable))



leveneTest(permanova.model10CVLevels.model$value~permanova.model10CVLevels.model$variable)
leveneTest(permanova.model10CVLevels.env1$value~permanova.model10CVLevels.env1$variable)
leveneTest(permanova.model10CVLevels.env2$value~permanova.model10CVLevels.env2$variable)
leveneTest(permanova.model10CVLevels.env3$value~permanova.model10CVLevels.env3$variable)
leveneTest(permanova.model10CVLevels.env4$value~permanova.model10CVLevels.env4$variable)
leveneTest(permanova.model10CVLevels.env5$value~permanova.model10CVLevels.env5$variable)
```


````{r}

TaxRatio.model10.B$V.tax
TaxRatio.model10.B$Median.tax
model10.B$rep6$model$SpeciesMeta

model10.B$rep6$QSeq10$taxCor.Ratio[order(model10.B$rep6$QSeq10$taxCor.Ratio$value),]

spp130, spp11, spp324


model10.B$rep6$QSeq0.5$taxCor.Ratio[order(model10.B$rep6$QSeq10$taxCor.Ratio$value),]
ggplot(model10.B$rep6$model$SpeciesMeta, aes(x=M.Eval,y=log(QSeq10),col=prevalence))+
  geom_point()+
  scale_color_viridis_c(option="C")+
  theme_classic()

ggplot(model10.B$rep6$model$SpeciesMeta, aes(x=prevalence, y=M.Eval,col=prevalence))+
  geom_point()+
  scale_color_viridis_c(option="C")+
  theme_classic()

ggplot(model10.B$rep1$model$SpeciesMeta, aes(x=prevalence, y=M.Eval,col=prevalence))+
  geom_point()+
  scale_color_viridis_c(option="C")+
  theme_classic()

ggplot(C.D500$rep1$model$SpeciesMeta, aes(x=log10(M.Eval),y=log(deseqLII),col=deseqDMI))+geom_point()+ scale_color_viridis_c(option="C")

```


```{r}
getNetStats<-function(x, method){
  print(length(method))
  D.module.table<-matrix(nrow=length(x), ncol=length(method))
  D.Threshold.table<-matrix(nrow=length(x), ncol=length(method))
  S.module.table<-matrix(nrow=length(x), ncol=length(method))
  S.Threshold.table<-matrix(nrow=length(x), ncol=length(method))
  for(i in 1:length(x)){
    for(j in 1:length(method)){
      print(x[[i]][[method[j]]]$networkStat$stats[1,3])
      D.module.table[i,j]<-x[[i]][[method[j]]]$networkStat$stats[1,3]
      D.Threshold.table[i,j]<-x[[i]][[method[j]]]$networkStat$stats[1,4]
      S.module.table[i,j]<-x[[i]][[method[j]]]$networkStat$stats[2,3]
      S.Threshold.table[i,j]<-x[[i]][[method[j]]]$networkStat$stats[2,4]
      }
  }
  
  rownames(D.module.table)<-paste("rep", 1:length(x))
  rownames(D.Threshold.table)<-paste("rep", 1:length(x))
  rownames(S.module.table)<-paste("rep", 1:length(x))
  rownames(S.Threshold.table)<-paste("rep", 1:length(x))
  colnames(D.module.table)<-method
  colnames(D.Threshold.table)<-method
  colnames(S.module.table)<-method
  colnames(S.Threshold.table)<-method
  out<-NULL
  out$D.module.table<-D.module.table
  out$D.Threshold.table<-D.Threshold.table
  out$S.module.table<-S.module.table
  out$S.Threshold.table<-S.Threshold.table
  out
}

method3<-c("model", "raw", "QSeq0.5", "QSeq10")

plotstats.A<-getNetStats(model10.A, method2)

plotstats$D.module.table
plotstats$D.Threshold.table
plotstats$S.module.table
plotstats$S.Threshold.table

plotstats.B<-getNetStats(model10.B, method2)

plotstats.C<-getNetStats(model10.C, method2)

plotstats.D<-getNetStats(model10.D, method2)

plotstats.D$D.module.table
plotstats.D$D.Threshold.table
plotstats.D$S.module.table
plotstats.D$S.Threshold.table


test.dist<-phyloseq::distance(model10.A$rep1$model$comm, method="sor", type="taxa")

test.dist2<-phyloseq::distance(model10.A$rep1$raw$comm, method="sor", type="taxa")
test.dist3<-phyloseq::distance(model10.A$rep1$QSeq10$comm, method="sor", type="taxa")

test.dist2/test.dist
test.dist3/test.dist

identical(test.dist2/test.dist, test.dist3/test.dist)
View(as.matrix(test.dist))

?cor
cor.tst<-cor(t(as.data.frame(as.matrix(otu_table(model10.A$rep1$model$comm)))))
cor.tst2<-cor(t(as.data.frame(as.matrix(otu_table(model10.A$rep1$raw$comm)))))

var(cor.tst2/cor.tst)

identical(rownames(cor.tst2), rownames(cor.tst))
identical(colnames(cor.tst2), colnames(cor.tst))

t.melt<-melt(cor.tst)
```
Here we can see clearly that the degree of structure in the dataset interacts with the normalization method to affect it's accuracy. In all cases the QSeq normaliation method out performs
And some statistics:

```{r}

```




[FOR PAPER 2]/ development
```{r}
# new functions for more flexible use ####
#raw<-function(ps){ps}


RA<-function(ps){
  s<-transform_sample_counts(ps, function(x) x / sum(x))
  s
  }
eRare<-function(ps, level=NULL){
  if(is.null(level)){
  out<-make.rarefy2(ps, min(sample_sums(ps)))}
  else{
    out<-make.rarefy2(ps, level)}
  
  out
  }
pRare<-function(ps, level=NULL){
  if(is.null(level)){
  out<-make.rarefy2(ps, min(sample_sums(ps))*sample_sums(ps)/mean(sample_sums(ps)))} else{out<-make.rarefy2(ps, level)}
  
  out
}

QSeq<-function(ps){
  scale<-sample_data(ps)$DensityF
  out<-make.scaled2(ps, val=2*mean(sample_sums(ps)), scale)
  out
  }
deseqVST<-function(ps){
  out<-make.deseqVST(ps, "Factor", l=1)
  out
}
deseqVST.log<-function(ps){
  out<-make.deseqVST(ps, "Factor", l=1)
  out
}
limmaVST<-function(ps){
  out<-make.limmaVST(ps, "Factor")
  out
}
limmaVST.log<-function(ps){
  out<-make.limmaVST(ps, "Factor")
  out
}
lm.model<-function(ps){
  anova.otu<-t(as.data.frame(as.matrix(otu_table(ps))))
  anova.env<-data.frame(as.matrix(sample_data(ps)))
  anova.env$F1<-as.numeric(as.character(anova.env$F1))
  anova.env$F2<-as.numeric(as.character(anova.env$F2))
  anova.env$F3<-as.numeric(as.character(anova.env$F3))
  anova.env$F4<-as.numeric(as.character(anova.env$F4))
  anova.env$F5<-as.numeric(as.character(anova.env$F5))

  testlm<-adply(anova.otu, 2, function(x) {

    l1=summary(lm(x~anova.env$F1+anova.env$F2+anova.env$F3+anova.env$F4+anova.env$F5))
    return(l1$r.squared)
    })
  testlm[is.na(testlm)]<-0
  testlm2<-testlm$V1
  names(testlm2)<-testlm$X1
  #row.names(testlm)<-testlm$X1
  #testlm<-testlm[,-1]
  #testlm<-testlm[order(row.names(testlm)),]
  testlm2
  }

lm.model2<-function(ps){
  anova.otu<-t(as.data.frame(as.matrix(otu_table(ps))))
  anova.env<-data.frame(as.matrix(sample_data(ps)))
  

  testlm<-adply(anova.otu, 2, function(x) {

    l1=summary(lm(x~anova.env$Factor2))
    return(l1$r.squared)
    })
  testlm[is.na(testlm)]<-0
  testlm2<-testlm$V1
  names(testlm2)<-testlm$X1
  #row.names(testlm)<-testlm$X1
  #testlm<-testlm[,-1]
  #testlm<-testlm[order(row.names(testlm)),]
  testlm2
  }
  #' workhorse function for benchmark.MM version 2 with flexible inputs
  #' @param commonN number of common species
  #' @param groupN number of unique taxa to groups
  #' @param singleN number of unique taxa to samples
  #' @param D average sampling depth
  #' @param V variation in sampling depth
  #' @param method new normalization function to implement
  #' @keywords benchmark
  #' @export
  #' @examples
  #' run.analysis()

  
  
  # test funcitons ####

  method<-c("RA", "eRare", "pRare", "QSeq", "deseqVST", "limmaVST")
  
 #test.analysis<-run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method)
test.analysisB<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))
 test.analysis2<-run.analysis(commonN=30, groupN=20, singleN=5, D=500, V=250)
```



```{r}




run.analysis2<-function(commonN, groupN, singleN, D, V, method){
      AllSpp<-c(paste0("spp", c(1:700), sep="")) # make a quick list of all species functions
      AllSpp<-lapply(AllSpp, get) # connect function to name
      AllSpp<-unlist(AllSpp)  # format to be read by downstream functions
      names(AllSpp)<-c(paste0("spp", c(1:700)))

      # Define list of 5 species w/ global distribution
      global.spp<-names(sample(AllSpp, commonN, replace=F))

  # define list of species w/ regional distribution
      group.spp<-NULL
      group.spp$group1<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group2<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group3<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group4<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group5<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group6<-names(sample(AllSpp, groupN, replace=F))

  # define list of species found at each site
      rando.spp<-NULL
      rando.spp$Site1<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site2<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site3<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site4<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site5<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site6<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site7<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site8<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site9<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site10<-unique(c(names(sample(AllSpp,singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site11<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site12<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site13<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site14<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site15<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site16<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site17<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site18<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site19<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site20<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site21<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site22<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site23<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site24<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site25<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site26<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
      rando.spp$Site27<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
      rando.spp$Site28<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
      rando.spp$Site29<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
      rando.spp$Site30<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))

  # make list of unique species arrays

      library(reshape2)
      f1c1<-c(5,5,5,5,5,5) # number of selections
      f1c2<-c(1,3,10,30,60,15) # mean value of selections
      f1c3<-c(0.5,1,4,10,20,5) # SD of selections
      F1.frame<-mapply(rnorm, f1c1,f1c2,f1c3) # pick Factor 1 value for each site
      F1<-reshape2::melt(F1.frame)

  #F2
      f2c1<-c(5,5,5,5,5,5) # number of selections
      f2c2<-c(34,30,10,55,35,60) # mean value of selections
      f2c3<-c(10,10,3,10,1,20) # SD of selections
      F2.frame<-mapply(rnorm, f2c1,f2c2,f2c3) # pick Factor 2 value for each site
      F2<-reshape2::melt(F2.frame)

  #F3
      f3c1<-c(5,5,5,5,5,5) # number of selections
      f3c2<-c(1,3,10,15,3,15) # mean value of selections
      f3c3<-c(0.5,1,3,3,1,5) # SD of selections
      F3.frame<-mapply(rnorm, f3c1,f3c2,f3c3) # pick Factor 3 value for each site
      F3<-reshape2::melt(F3.frame)

  #F4
      f4c1<-c(5,5,5,5,5,5) # number of selections
      f4c2<-c(5,5,5,5,5,5) # mean value of selections
      f4c3<-c(1,1,1,1,1,1) # SD of selections
      F4.frame<-mapply(rnorm, f4c1,f4c2,f4c3) # pick Factor 4 value for each site
      F4<-reshape2::melt(F4.frame)

  #F5
      f5c1<-c(5,5,5,5,5,5) # number of selections
      f5c2<-c(50,50,50,50,50,50) # mean value of selections
      f5c3<-c(20,20,20,20,20,20) # SD of selections
      F5.frame<-mapply(rnorm, f5c1,f5c2,f5c3) # pick Factor 5 value for each site
      F5<-reshape2::melt(F5.frame)
      Factors<-data.frame(F1$value,F2$value,F3$value,F4$value,F5$value) # combine factors into data table
      Sites<-c(paste0("Site", 1:30))
      rownames(Factors)<-Sites
      colnames(Factors)<-c("F1","F2","F3","F4","F5")

      output<-list("model"=NULL, "spplist"=NULL, "raw"=NULL)

      output$spplist<-rando.spp

      output$model$comm<-make.refcomm(rando.spp, Factors) # output a phyloseq object... will make a list of phyloseq objects
      output$model$comm<-filter_taxa(output$model$comm, function(x) sum(x)>0, TRUE)
      output$model$EV<-transform_sample_counts(output$model$comm, function(x) x / sum(x) )
      output$metrics<-NULL
      output$metrics$stats<-NULL
      output$metrics$Richness<-NULL
      Rich<-estimate_richness(output$model$comm, measures="Observed")
      output$metrics$Richness<-Rich
      output$metrics$skewness<-median(apply(X = otu_table(output$model$comm), MARGIN=2,FUN = function(x){skewness(x)}))
     
      sample<-set.seqDepth(D,V)
      output$raw$comm<-model.rarefy(output$model$comm, sample, D, V)

      print("spp selection complete")
      sample_data(output$model$comm)$Density<-sample_sums(output$model$comm)# add sample sums
      sample_data(output$model$comm)$DensityF<-sample_sums(output$model$comm)/mean(sample_sums(output$model$comm))
      sample_data(output$model$comm)$Factor<-as.factor(c(rep("one",5),rep("two",5),rep("three",5),rep("four",5),rep("five",5),rep("six",5)))
      sample_data(output$model$comm)$Factor2<-as.factor(c(rep(1,5),rep(2,5),rep(3,5),rep(4,5),rep(5,5),rep(6,5)))


      print("start subsampling")

      print("subsampling finished")
      # remove taxa that have zero abundance in "raw" sequencing run
      tax.filt<-filter_taxa(output$raw$comm, function(x)sum(x)>0)
      output$metrics$tax.lost<-tax.filt
        output$raw$comm<-filter_taxa(output$raw$comm, function(x)sum(x)>0, TRUE)
        # remove taxa that are not kept from sequencing so that they don't penalize downstream methods
        output$model$comm<-prune_taxa(tax.filt, output$model$comm)
        output$model$EV<-prune_taxa(tax.filt, output$model$EV)


      # for each species: measure prevalence
          prevalence=apply(X = otu_table(output$model$comm), MARGIN=1,FUN = function(x){sum(x > 0)})
      # for each species: measure relative abundance (proportion of total counts?
          p.abund<-transform_sample_counts(output$model$comm, function(x) x/sum(x) )

          mean_abundance<-apply(X = otu_table(p.abund), MARGIN=1,FUN = function(x){mean(x)})

          sd_abundance<-apply(X = otu_table(p.abund), MARGIN=1,FUN = function(x){sd(x)})
      # create a tax table for whole dataset ...
          tab<-data.frame(prevalence, mean_abundance, sd_abundance)
          tab$names<-rownames(tab)
          output$model$SpeciesMeta<-tab

          output$model$R<-lm.test(output$model$comm)
         

      # make expected value
          s<-sample_sums(output$raw$comm)
          s2<-as.data.frame(as.matrix(otu_table(output$model$EV)))
          s2<-for (i in 1:ncol(s2)) {s2[,i]<-s2[,i]*s[i]}
          otu_table(output$model$EV)<-otu_table(output$model$EV, taxa_are_rows=TRUE)
          M.Eval<-apply(X = otu_table(output$model$EV), MARGIN=1,FUN = function(x){mean(x[x>0])})


      #output$model$SpeciesMeta$USI<-output$model$SpeciesMeta$
      # create a tax table for whole dataset ...
          tab<-data.frame(prevalence, mean_abundance, sd_abundance, M.Eval)
          tab$names<-rownames(tab)
          output$model$SpeciesMeta<-tab
          output$model$R<-lm.test(output$model$comm)
          sample_data(output$raw$comm)<-sample_data(output$model$comm)
          output$model$PERMANOVA<-make.PERMANOVA(output$model$comm)
          output$model$rarecurve<-ggrare(output$model$comm, 50, color="Factor")
  print("metadata complete")
  
  # implement each normalization function
      for (i in 1:length(method)){
        a<-get(method[i])
        #name(a)<-i
        b<-output$raw$comm
        c<-a(b)
        output[[method[i]]]$comm<-c
        output[[method[i]]]$PERMANOVA<-make.PERMANOVA(c)
        output[[method[i]]]$PERMANOVA$Rratio<-output[[method[i]]]$PERMANOVA$aov.tab$R2/output$model$PERMANOVA$aov.tab$R2
        output[[method[i]]]$LII<-LII(output$model$comm, output[[method[i]]]$comm)
        #output[[method[i]]]$Dtab.plot<-plot(output$model$SpeciesMeta$prevalence,output[[method[i]]]$Dtab, col=output$model$SpeciesMeta$M.Eval, main="Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")
        #output[[method[i]]]$Dtab.model.plot<-plot(output$model$SpeciesMeta$prevalence,output[[method[i]]]$Dtab.model, col=output$model$SpeciesMeta$M.Eval, main="Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")
        
        print(paste(method[i], " complete"))
      }
# prepare raw metadata for lm analysis and model for lm analysis
      output$raw$PERMANOVA<-make.PERMANOVA(output$raw$comm)
      print("raw permanova complete")
      output$raw$LII<-LII(output$model$comm, output$raw$comm)
      print("raw LII complete")
      output$raw$lmtab<-lm.model(output$raw$comm)
      output$raw$lmtab.model<-lm.model2(output$raw$comm)
      print("lmtab raw complete")
      output$model$lmtab<-lm.model(output$model$comm)
      output$model$lmtab.model<-lm.model2(output$model$comm)# linear model of env. parameters as explanatory variables for abundance of each taxon
      print("lmtab model complete")
      output$raw$lmRatiotab<-output$raw$lmtab/output$model$lmtab #ratio of lm of env from normalized data to reference
      output$raw$lmRatiotab.model<-output$raw$lmtab.model/output$model$lmtab.model #ratio of lm of env from normalized data to reference using only categorical model variables (no explicit env. model)
      print("dtab complete")

  # do 1- : calculates the information lost per taxon
    for (i in 1:length(method)){
    # make the species-wise LII value:
      output$model$SpeciesMeta[[method[i]]]<-1-output[[method[i]]]$LII$R
      # make the lm output for each normalization: (this is r-squared, could be r value ...)
      output[[method[i]]]$lmtab<-lm.model(output[[method[i]]]$comm)# linear model of env. parameters as explanatory variables for abundance of each taxon
      output[[method[i]]]$lmtab.model<-lm.model2(output[[method[i]]]$comm)
      # conditional statement to make sure that the taxon names match, then:
      # difference in rsquared values (or r values?) from reference:
      if(names(output$model$lmtab)==names(output[[method[i]]]$lmtab)){
      output[[method[i]]]$lmRatiotab<-output[[method[i]]]$lmtab/output$model$lmtab
      output[[method[i]]]$lmRatiotab.model<-output[[method[i]]]$lmtab.model/output$model$lmtab.model} #ratio of lm of env from normalized data to reference
      else{print("Error in Dtab: names do not match")}
    }
    
    output
  }
```

test space:
```{r}
method<-c("RA", "QSeq")
tget<-get(method[1])

trt$rep1<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))
trt$rep2<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))

Summarize.Ptable.Mean<-function(trt, method){
  model<-matrix(NA, nrow = length(trt), ncol = length(method))
  env1<-matrix(NA, nrow = length(trt), ncol = length(method))
  env2<-matrix(NA, nrow = length(trt), ncol = length(method))
  env3<-matrix(NA, nrow = length(trt), ncol = length(method))
  env4<-matrix(NA, nrow = length(trt), ncol = length(method))
  env5<-matrix(NA, nrow = length(trt), ncol = length(method))
  resids<-matrix(NA, nrow = length(trt), ncol = length(method))
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      model[i,j]<-trt[[i]][[method[j]]]$PERMANOVA$aov.tab$R2[1]/trt[[i]]$model$PERMANOVA$aov.tab$R2[1]
      env1[i,j]<-trt[[i]][[method[j]]]$PERMANOVA$aov.tab$R2[2]/trt[[i]]$model$PERMANOVA$aov.tab$R2[2]
      env2[i,j]<-trt[[i]][[method[j]]]$PERMANOVA$aov.tab$R2[3]/trt[[i]]$model$PERMANOVA$aov.tab$R2[3]
      env3[i,j]<-trt[[i]][[method[j]]]$PERMANOVA$aov.tab$R2[4]/trt[[i]]$model$PERMANOVA$aov.tab$R2[4]
      env4[i,j]<-trt[[i]][[method[j]]]$PERMANOVA$aov.tab$R2[5]/trt[[i]]$model$PERMANOVA$aov.tab$R2[5]
      env5[i,j]<-trt[[i]][[method[j]]]$PERMANOVA$aov.tab$R2[6]/trt[[i]]$model$PERMANOVA$aov.tab$R2[6]
      resids[i,j]<-trt[[i]][[method[j]]]$PERMANOVA$aov.tab$R2[7]/trt[[i]]$model$PERMANOVA$aov.tab$R2[7]
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(model)<-names(trt)
  colnames(model)<-method
  rownames(env1)<-names(trt)
  colnames(env1)<-method
  rownames(env2)<-names(trt)
  colnames(env2)<-method
  rownames(env3)<-names(trt)
  colnames(env3)<-method
  rownames(env4)<-names(trt)
  colnames(env4)<-method
  rownames(env5)<-names(trt)
  colnames(env5)<-method
  rownames(resids)<-names(trt)
  colnames(resids)<-method
  Ftab<-NULL
  Ftab$model<-model
  Ftab$env1<-env1
  Ftab$env2<-env2
  Ftab$env3<-env3
  Ftab$env4<-env4
  Ftab$env5<-env5
  Ftab$resids<-resids
  Ftab
}

Summarize.PERMANOVA.Rratio<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method))
  print("reference")
  for(i in 1:length(trt)){
    for(j in 1:length(method)){

     print(trt[[i]][[method[j]]]$PERMANOVA$Rratio)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
}



Summarize.LII<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method))
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-sum(trt[[i]][[method[j]]]$LII$Index, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

Plot.dtab<-function(trt, method){
  for(i in 1:length(trt)){
    for(j in 1:length(method)){plot(trt[[i]]$model$SpeciesMeta$prevalence,trt[[i]][[method[j]]]$Dtab, col=trt[[i]]$model$SpeciesMeta$M.Eval, main="Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")
  }
}}

Summarize.PERMANOVA.Rratio(trt, method)

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$QSeq$Dtab, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="QSeq; Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$QSeq$Dtab.model, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="QSeq; Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")


plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$deseqVST$Dtab, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="deseq; Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$deseqVST$Dtab.model, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="deseq; Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$QSeq$lmtab, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="QSeq; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$QSeq$lmtab.model, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="QSeq; Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$deseqVST$lmtab, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="deseq; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$deseqVST$lmtab.model, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="deseq; Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

#collect variance data for stats!!
var(trt$rep1$QSeq$Dtab)
median(trt$rep1$QSeq$Dtab)
var(trt$rep1$QSeq$Dtab.model)
median(trt$rep1$QSeq$Dtab.model)
var(trt$rep1$deseqVST$Dtab)
median(trt$rep1$deseqVST$Dtab)
var(trt$rep1$deseqVST$Dtab.model)
median(trt$rep1$deseqVST$Dtab.model)
# make output with difference with env. and without env. variables in lm for lm.model()
# 
```
```{r}
plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$QSeq$Dtab, col=gray(trt$rep2$model$SpeciesMeta$M.Eval), 
  main="QSeq; Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$QSeq$Dtab.model, col=gray(trt$rep2$model$SpeciesMeta$M.Eval), 
  main="QSeq; Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")


plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$deseqVST$Dtab, col=gray(trt$rep2$model$SpeciesMeta$M.Eval), 
  main="deseq; Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$deseqVST$Dtab.model, col=gray(trt$rep2$model$SpeciesMeta$M.Eval), 
  main="deseq; Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$QSeq$lmtab, col=gray(2*trt$rep2$model$SpeciesMeta$M.Eval), 
  main="QSeq; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$QSeq$lmtab.model, col=gray(2*trt$rep2$model$SpeciesMeta$M.Eval), 
  main="QSeq; Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$deseqVST$lmtab, col=gray(2*trt$rep2$model$SpeciesMeta$M.Eval), 
  main="deseq; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$deseqVST$lmtab.model, col=gray(2*trt$rep2$model$SpeciesMeta$M.Eval), 
  main="deseq; Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

#collect variance data for stats!!
var(trt$rep2$QSeq$Dtab)
median(trt$rep2$QSeq$Dtab)
var(trt$rep2$QSeq$Dtab.model)
median(trt$rep2$QSeq$Dtab.model)
var(trt$rep1$deseqVST$Dtab)
median(trt$rep2$deseqVST$Dtab)
var(trt$rep2$deseqVST$Dtab.model)
median(trt$rep2$deseqVST$Dtab.model)

ConnStat<-function(ps, num=200){
  require(phyloseq)
  require(igraph)
  o<-otu_table(ps)
  c<-cor(o)
  i=1
  c[c<1]<-0
  n<-graph_from_incidence_matrix(c)
  while(ecount(n)<num){
    t<-otu_table(ps)
    t<-cor(t)
    t[t<i]<-0
    t[t>i]<-1
    n<-graph_from_incidence_matrix(t)
    i=i-0.001
  }
   cfg<-cluster_fast_greedy(as.undirected(n))
  plot(cfg, as.undirected(n), layout=layout_nicely(n), vertex.label=NA, main="Dynamic", vertex.size=10)
  table<-matrix(nrow=2,ncol=4)
  colnames(table)<-c("Mean_Closeness", "Mean_Degree", "Modularity", "Threshold")
  rownames(table)<-c("Dynamic", "Static")
  table[1,1]<-mean(closeness(n))
  table[1,2]<-mean(degree(n))
  table[1,3]<-modularity(n, membership(cfg))
  table[1,4]<-i

  t<-otu_table(ps)
  t<-cor(t)
  t[t<0.8]<-0
  t[t>0.8]<-1
  n<-graph_from_incidence_matrix(t)
  cfg<-cluster_fast_greedy(as.undirected(n))
  table[2,1]<-mean(closeness(n))
  table[2,2]<-mean(degree(n))
  table[2,3]<-modularity(n, membership(cfg))
  table[2,4]<-0.8
  plot(cfg, as.undirected(n), layout=layout_nicely(n), vertex.label=NA, main="Static", vertex.size=10)
  table
}


ConnStat(trt$rep1$model$comm, num=50)
ConnStat(trt$rep1$model$comm, num=100)
ConnStat(trt$rep1$model$comm, num=150)
ConnStat(trt$rep1$model$comm, num=200)
ConnStat(trt$rep1$model$comm, num=250)
ConnStat(trt$rep1$model$comm, num=350)
ConnStat(trt$rep1$model$comm, num=450)


Mean<-c(1,3,10,30,60,15,34,30,10,55,35,60,1,3,10,15,3,15,5,5,5,5,5,5,50,50,50,50,50,50)
Var<-c(0.5,1,4,10,20,5,10,10,3,10,1,20,0.5,1,3,3,1,5,1,1,1,1,1,1,20,20,20,20,20,20)
ymin<-Mean-Var
ymax<-Mean+Var
Factor.Label<-c(rep("Factor 1", 6), rep("Factor 2", 6), rep("Factor 3", 6), rep("Factor 4", 6), rep("Factor 5", 6))
Group.label<-c(rep(c("Group 1", "Group 2", "Group 3", "Group 4", "Group 5", "Group 6"), 5))

df.envplot<-data.frame(Group.label, Factor.Label, Mean, Var, ymin, ymax)

ggplot(df.envplot, aes(x=Group.label, y=Mean,group=Factor.Label, colour=Factor.Label))+ geom_point() + geom_line()+geom_errorbar(aes(ymin=ymin, ymax=ymax), linetype=2, alpha=0.9, position=position_dodge(0.1))+theme_classic()



```
