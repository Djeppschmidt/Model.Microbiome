---
title: "M.M. Quick Tutorial"
author: "Dietrich Epp Schmidt"
date: "5/18/2020"
output: html_document
---

Table of contents:
Building a model
Emplementing a simulation
Outputs: examining taxon relationships to environment
i-PERMANOVA
ii-LII
iii-linear model
iv-index ratios & variance
v-statistical tests

Outputs: examining taxon relationships to each other

Introduction:

ModelMicrobiome is a novel platform for testing the performance and assumptions underlying ecological inference of microbiomes. Whereas previous benchmarking and modeling efforts have used case-control frameworks, and/or used randomized real data, none have tested the accuracy of ecological inference following a sequence count normalization procedure. This is to say previous efforts have focused on the ability to accurately detect a change in relative abudance (i.e. differential abundance analysis). But these tests do not reflect how well the ecological relationships are preserved through the normalization process (e.g. relationship between the taxa and their environment or among taxa). This software does not attempt to address bias that is introduced by the sequencing, DNA purification, or field sampling efforts. It only is meant as a tool to better understand the effect that any post-hoc sequence count normalization has on our ability to infer underlying ecological relationships. 

To further clarify the limits of this software, it is not intended to be applied to metagenomics analyses that derive from shotgun sequence data. There are database and sequencing biases that are particular to whole genome sequencing that must be addressed separately and that may confound the inferences made from this software. THIS SOFTWARE IS NOT DESIGNED TO ADDRESS SHOTGUN SEQUENCING OR ION TORRENT ETC. SEQUENCING. This software is designed specifically for amplicon (i.e. metabarcoding) studies of a single gene locus used to infer population structure. 

This software was written under R version 3.6.1. R version 4 has been published with major updates to some of the core mechanics. Model.MicrobiomeV2 will be updated for R 4.0.

With those disclaimers out of the way, let's dig in.

First, let's install Model.Microbiome:
```{r}
devtools::install_github("djeppschmidt/Model.Microbiome")
```
Now load all the other packages we need (or download and load if you haven't already):
```{r}
# load required packages ####

library(Model.Microbiome)
library(reshape2)
library(ggplot2)
library(vegan)
library(dplyr)
library(plyr)
library(phyloseq)
library(viridis)
library(ranacapa)
library(edgeR)
library(limma)
library(GLDEX)
library(stats)
library(igraph)
library(car)
```


The core functionality of Model.Microbiome is to create a model microbial community, then simulate sequencing by subsampling from it, then normalizing the raw sequence data. This is handled by the run.analysis2() function. I recommend running this function with SuppressWarnings() because many of the models throw warnings about too-perfect of a fit. Since the point of this software is to detect these functions, we don't care. We have ways of measuring these effects.

Building the model community works sequenially by:
1. creating an environment, with 5 environmental parameters
  a. environmetnal paramters 1-3 change in mean and variance according to the experimental design
  b. environmental parameters 4-5 are unrelated to the experimental design, but do affect taxon abundance
  c. the sampling environment consists of 30 samples, divided into 6 experimental treatments (or 5 reps per treatment)
  d. the environmental parameters have the same mean and variance for each group every time the software is run; however, the values of any given run are randomly drawn from this distribution. Therefore no two runs will have exactly the same environment, but they will represent exactly the same experiment.

2. Choosing which species exist in each sample.
  a. The user sets how many taxa will be chosen per sample
  b. the user sets how many taxa will exist only within certain experimental treatments
  c. the user sets how many taxa will exist only within certain samples
  d. the user sets how many taxa will be chosen to be globally distributed
  
NOTE: SEE SECTION DETAILS FOR CAVEATS ABOUT THIS SELECTION PROCESS!!

3. Community assembly.
  a. taxon abundances are calculated; each species has a specific relationship with the environmental variables.
  b. OTU table generated, and merged into a phyloseq object with metadata on the environment, etc.

4. Community is sampled
  a. this is a subsampling routine that creates the "sequence output" table

5. Sequence Normalization
  a. uses a function to normalize the data any arbitrary number of different methods

```{r}
knitr::include_graphics("~/Desktop/PhD/RarefactionProject/Process_Map.pdf")
```

So let's look at the function. It has several inputs necessary:

commonN: Number of taxa that have a global distribution
groupN: Number of taxa that are group-specific
singleN: Number of taxa that are sample-specific

CAVEAT: there are 700 species functions that the software selects from. For each of these values, it uses a random selection routine. It includes resampling among the groups. This means that THERE IS A CHANCE THAT A TAXON MAY END UP REPRESENTING SEVERAL GROUPS, OR SAMPLES.

D: mean sampling depth (e.g. for sequencing)
V: variance of sampling depth.

method: names of the functions to be used for count normalization (see below for details)

```{r}
method<-c("QSeq")

model<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))
```
This creates a single output object that contains the phyloseq objects created before, and a number of different statistics. We'll get to the idices later.In order to get it to work, we need at least one methods function.


Example method function:
```{r}

# these functions are already provided as a part of Model.Microbiome
make.scaled2<-function(ps, val, scale){
  scaled<-data.frame(mapply(`*`, data.frame(as.matrix(otu_table(transform_sample_counts(ps, function(x) x/sum(x))))), scale * val))# sample_data(ps)$val))
  names<-rownames(data.frame(as.matrix(otu_table(ps))))
  rownames(scaled)<-names
  scaled<-round(scaled)

  p2<-ps
  otu_table(p2)<- otu_table(scaled, taxa_are_rows=T)
  p2
}

QSeq<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=2*mean(sample_sums(ps)), scale)
    out
}

```

Notice that this is actually one function nested within another. The wrapper helps the function to play nice. It's easy to replicate wrappers to provide the underlying function with different inputs if we want to test the different potential conditions. For example, our QSeq function is meant to replicate the normalization technique called quantitative sequencing. This technique combines quantitative data from QPCR with sequence count distributions to normalize the sequence counts in each sample by the relative total sequence counts of that sample compared to other samples. This should more closely approximate the behavior of the sampled community than relative abundance. To do this though, we need to decide what the mean value for the sample-wise abundance normalization. We could make several versions of the QSeq function to test what is the optimal mean sample-wise total abundance. To do this we change the value parameter as follows:

```{r}
QSeq0.5<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=0.5*mean(sample_sums(ps)), scale)
    out
}

QSeq1<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=mean(sample_sums(ps)), scale)
    out
}

QSeq2<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=2*mean(sample_sums(ps)), scale)
    out
}

QSeq3<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=3*mean(sample_sums(ps)), scale)
    out
}

QSeq4<-function(ps){
    scale<-sample_data(ps)$DensityF
    out<-make.scaled2(ps, val=10*mean(sample_sums(ps)), scale)
    out
}
```

If we want to use all of them in our simulation study, then we simply do:
```{r}
method<-c("QSeq0.5", "QSeq1", "QSeq2", "QSeq3", "QSeq4")
```

Then we implement it in the analysis pipeline:

```{r}
model<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))
```

We can call the phyloseq object from any one of the normalization (including the reference data and the raw sequence counts):
```{r}
model$model$comm
model$raw$comm
model$QSeq0.5$comm
model$QSeq1$comm
model$QSeq2$comm
model$QSeq3$comm
```

Model.Microbiome also automatically calculates several metrics. For example, it calculates automatically an experiment-wide PERMANOVA for each method:
```{r}
model$model$PERMANOVA # reference condition!
model$raw$PERMANOVA # not normalized
model$QSeq0.5$PERMANOVA # lowest mean value
model$QSeq1$PERMANOVA  #...
model$QSeq2$PERMANOVA #...
model$QSeq3$PERMANOVA # highest mean value
```
You'll notice that the QSeq PERMANOVA tables are very similar to the reference PERMANOVA table, and that the un-normalized table is a bit different. This is great! 

Model.Microbiome uses a few statistics to understand bias that is introduced through the normalization process. All the metrics in Model.Microbiome consist of the ouput from an analysis conducted on a normalized dataset divided by the same analysis conducted on the reference community. For example, if we are interested in the relationship between a taxon an the environment, we could construct a linear model where abundance of taxon x depends on enviornment A. The R-squared of this model would represent the amount of information captured by the model. If we applied this model to both the reference community, and the normalized community, then we have a measure of the "actual" relationship and the infered relationship after our normalization. The ratio of these values gives us a comparable and interpretable value where 1 represents a perfect match; greater than 1 means the normalization method leads to overestimating the explanatory power of the environmental factors on the abundance of the taxon; and a value of less than one is interpreted to mean that the normalization underestimates the explanatory value of the environmental parameter. All of the indices are based on this interpretation principle. Here is a brief description of each index:


Loss of information index (LII) is based on a linear model approach, but instead of regressing the taxa against the environment and comparing the outputs, it regresses the treatment (normalized data) against the reference (pre-sampling). This provides a direct measure of information that is maintained about the abundance of the taxon across the dataset. The LII output has several components;

$Index : index value which is the sum of (1 - the r-squared values) across all taxa. This is a single value for the whole run. Lower scores equate to less overall information lost.

$R : a list of the R squared values, one per taxon in the run.

$diff : a list of (1 - the r-squared values) for the Index value, one for each taxon. This is useful to identify which taxa are deviating substantially, and help with troubleshooting the normalization method.

All of these can be accessed from the normalization object (see output object map). 



The second set of metrics consist of linear regressions of the taxa against environmental and experimental design predictors. This test allows us to diagnose the effect that our normalization protocol has on our ability to infer relationships between the taxa and the environment / experimental condition. Once again, the primary outputs of this process are then taken as a ratio to the reference for interpretation.

$lmRatiotab : list of taxa and their R-squared values between taxon and the environment, normalized by the reference

$lmRatiotab.model : list of taxa and their R-squared values between taxon and the experimental design, normalized by the reference

These datasets are useful

Object Map:
```{r}
knitr::include_graphics("~/Desktop/PhD/RarefactionProject/Output_Map.pdf")
```

Here are a few plots of reference vs. normalized abundance for a few randomly selected taxa:

```{r}

```


We might look for information that could explain why certain taxa lose more information than others. For example, we can look at the relationship between sparcity and lost information in our normalization function: 

(R squared = 1 is ideal!)
```{r}
plot(model$model$SpeciesMeta$prevalence,model$QSeq0.5$LII$R, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq0.5; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(model$model$SpeciesMeta$prevalence,model$QSeq1$LII$R, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq1; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(model$model$SpeciesMeta$prevalence,model$QSeq2$LII$R, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq2; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(model$model$SpeciesMeta$prevalence,model$QSeq3$LII$R, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq3; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
```

It's interesting that the prevalence of the taxon does not seem to determine how well information about it's abundance is retained! But, looks can be decieving; we can do a statistical test:

```{r}

```


You may notice that certain taxa have an R squared of 0; some of them improve as we increase the overall scaling factor. This happens when taxa are very low abundance in a particular sample. If their count abundance is less than 0.5, they get rounded down to 0. So ideally we want to pick a scaling value that lets us keep all the information from sampling (i.e. such that in all samples, the minimum taxon has a count of at least 1). This is an important observation if we want to use this method on a real dataset because it gives us a criteria for choosing what our scaling factor will be.

In nearly all our comparisons of a normalization technique, we are comparing the results of a statistical test given the normalization to the reference. we do this because it is possible that our normalization technique might overfit the data to an expected trend, thus information may be lost also by overfitting. We have looked at an explicit evaluation of lost information intrinic to each taxon; we can also look at how this lost information degrades our abilty to detect true relationships between the taxa and the environmental gradients. To do this, we employ linear modeling. We have two linear model outputs for each taxon. In the first case, we regress the abundance of the taxon against all the environmental variables, excluding categorical experimental design from the test structure. In the second case, we include only the categorical variables and exclude the environmental variables. This allows us to resolve whether there has been overfitting of taxon abundance to our experimental design; and the effect this has had on our ability to detect the relationship to environmental gradients. Of course, our final interest is not on the outputs of the model per se, but the ratio of model outputs from normalized datasets to the reference. This gives ratio tells us clearly which taxa are accurately modeled. In aggregate, the variance of this ratio around 1 tells us which method is more consistently and accurately modeling the relationship of each taxon to the environmental gradient driving it's abundance. 

Let's look at a couple examples. This linear model ratio is calculated automatically in each run:
```{r}
plot(model$model$SpeciesMeta$prevalence,model$raw$lmRatiotab, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="Raw; ratio-Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(model$model$SpeciesMeta$prevalence,model$QSeq3$lmRatiotab, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq3; ratio-Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
```
It's intruiging that actually highly prevalent taxa seem to lose the relationships to the environmental gradients the most consistently; and that low prevalence taxa seem most likely to have overestimated variance explained by the environment. We can see qualitatively that our QSeq methods increase the accuracy of our taxon-specific models by a small margin. We can evaluate this statistically:

```{r}
levenetest<-data.frame("ratio"=c(model$raw$lmRatiotab, model$QSeq3$lmRatiotab), "method"=c(rep("raw", length(model$raw$lmRatiotab)), rep("QSeq", length(model$QSeq3$lmRatiotab))))

car::leveneTest(ratio ~ method, data = levenetest)
```
In this case it's not a huge improvement, but the method provides a solid evidence-based approach for optimizing normalization methods. 

So far we've looked at how normalization might affect the ability to accurately detect environmental gradient effects on taxon abundance. Now let's use the same approach to see how the normalization affects our ability to detect experimental conditions. Model.Microbiome also calculates the variance explained by the experimental condition on each taxon:

```{r}
plot(model$model$SpeciesMeta$prevalence,model$raw$lmRatiotab.model, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="Raw; ratio-Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(model$model$SpeciesMeta$prevalence,model$QSeq3$lmRatiotab.model, col=gray(model$model$SpeciesMeta$M.Eval), 
  main="QSeq3; ratio-Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
```

```{r}
levenetest2<-data.frame("ratio"=c(model$raw$lmRatiotab.model, model$QSeq3$lmRatiotab.model), "method"=c(rep("raw", length(model$raw$lmRatiotab.model)), rep("QSeq", length(model$QSeq3$lmRatiotab.model))))

car::leveneTest(ratio ~ method, data = levenetest2)
```
We can see that in this case, there is a weaker effect of normalization on the dispersion of the ratio from 1. It is interesting to note though that non-normalized data again seems much more likely to over-estimate the significance of the experimental design on the taxa than the normalized data.


Thus far we have looked at the effect of normalization on detecting important relationships between taxa and the modeled environment. Now let's look at how relationships among taxa are affected by normalization (or lack therof). We will be conducting a network analysis for this section.
```{r}

```


```{r}

```

```{r}

```

We take a similar approach for PERMANOVA metrics: we use a ratio of the permanova F value, or R-squared (both give relevant information), of the normalized dataset to the reference. We interpret therefore a value greater than one meaning the data is over-fit; and a value of less than one to mean that information has been lost. Model.Mocrobiome has a function that extracts the relevant PERMANOVA data and displays in a table:

```{r}
Summarize.PERMANOVA.Rratio(model, method) # needs work !!
```
We can see that ...



```{r}

```


```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```







This software also has a wrapper that allows us to iterate the function and produce many copies of the simulation. This has all the same inputs as before, plus one aditional input to tell the function how many times to iterate. In this case we'll do 10
```{r}
model10<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=30, groupN=20, singleN=5, D=500, V=250, method))
```
model10 is now a list object with 10 items, each one as a replicated simulation. This is the basis for us to be able to extract performance metrics and do statistical analyses to compare different normalization methods. 

So now that we have the basic data structures and functionalities, let's look at some analyses that are possible when we can automate community generation.


We first extract the LII indicator with the function Summarize.LII(), then we melt the dataframe and do a statistical test on it:
```{r}
model10.LIIsummary<-Summarize.LII(model10, method)
model10.LIIsummary<-melt(model10.LIIsummary, value.name = "Value")

summary(aov(Value~Var2 +Error(Var1/Var2), data=model10.LIIsummary)) # blocks of method are nested within each run
TukeyHSD(aov(Value~Var2, data=model10.LIIsummary)) # ignoring the blocks bc TukeyHSD can't handle them. Still tells us which is most significant
plot(TukeyHSD(aov(Value~Var2, data=model10.LIIsummary))) # visual of confidence intervals

```
Clearly, the worst performing method is QSeq0.5. This is likely because it effectively leads to undersampling by rounding low count values to zero. Ideally we should choose a threshold value for this method that is sufficiently high enough that all sequences that pass QC in our pipeline are included in the sampling, and not rounded to 0 in any sample.

But wait, there's more. Let's say that we want to understand how the effect size of geographic structure within a community interacts with the performance of our normalization technique (or in the case of experimental design, what are the direct assemply effects of the treatment). Model.Microbiome gives us the power to vary the geographic structure/direct effects of the treatment on the community by determining the proportion of each sample that explicitly is determined by the grouping function:
```{r}
model10.A<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=53, groupN=1, singleN=1, D=500, V=250, method)) # essentially no filtration by group

model10.B<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=35, groupN=15, singleN=5, D=500, V=250, method)) # ~27% filtration by the group

model10.C<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=250, method)) # ~36% filtration by the group

model10.D<-suppressWarnings(BENCHMARK.MM(reps=10, commonN=10, groupN=30, singleN=15, D=500, V=250, method)) # ~55% filtration by the group
```

First, let's look at how the structure of the data might interact with our normalization strategy to influence how well information is retained. For this we will extract the LII value. As a reminder, LII correlates each taxon in the normalized dataset against it's abundance in the reference dataset. The index itself is a sum of 1-R-squared; this means that low LII values mean that overall the R-squared of the taxa against their references were close to 1 (or little information was lost). Conversely, higher values indicate that a lot of information was lost.
```{r}
method<- c("raw", method) # add raw data to the methods list to extract it too

# extract the LII from each level of sparcity
SummaryLII.model10.A<-as.data.frame(Summarize.LII(model10.A,method))
SummaryLII.model10.B<-as.data.frame(Summarize.LII(model10.B, method)) 
SummaryLII.model10.C<-as.data.frame(Summarize.LII(model10.C, method))
SummaryLII.model10.D<-as.data.frame(Summarize.LII(model10.D, method))

# prepare data for merging datasets
SummaryLII.model10.A$Level<-c(rep(1,10))
SummaryLII.model10.A<-melt(SummaryLII.model10.A, id.vars = "Level", measure.vars = method)
SummaryLII.model10.B$Level<-c(rep(2,10))
SummaryLII.model10.B<-melt(SummaryLII.model10.B, id.vars = "Level", measure.vars = method)
SummaryLII.model10.C$Level<-c(rep(3,10))
SummaryLII.model10.C<-melt(SummaryLII.model10.C, id.vars = "Level", measure.vars = method)
SummaryLII.model10.D$Level<-c(rep(4,10))
SummaryLII.model10.D<-melt(SummaryLII.model10.D, id.vars = "Level", measure.vars = method)

# merge datasets
model10Levels<-do.call("rbind", list(SummaryLII.model10.A,SummaryLII.model10.B,SummaryLII.model10.C,SummaryLII.model10.D))

# summarize for plotting
model10Levels.summary<-data_summary2(model10Levels, varname="value", groupnames=c("Level", "variable"))

# plot
ggplot(model10Levels.summary, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("LII Value (Median +/- min/max)")+
  theme_classic()
```
Here we can see clearly that the degree of structure in the dataset interacts with the normalization method to affect it's accuracy. In all cases the QSeq normaliation method out performs
And some statistics:

```{r}
summary(aov(model10Levels.summary$value~model10Levels.summary$Level*model10Levels.summary$variable))

TukeyHSD(aov(model10Levels.summary$value~model10Levels.summary$Level*model10Levels.summary$variable))
```
Conclusion: we can see clearly that the degree of structure in the dataset interacts with the normalization method to affect it's accuracy. In all cases the QSeq normaliation method out performs un-normalized data at low structure, but at high levels of structure only QSeq2 and QSeq3 parameters continue to consistently outperform the raw variables. Remeber that LII reports the degradation of R squared and so is a direct measure of lost information. Let's now look at how this results in a loss of environmental signal.

In this case interpretation is a little more challenging, because we are less interested in the mean value, and more interested in the variance of the values. This is because our metric is a ratio of the R-squared (variance explained) of each taxon by the environment to the same measure of the taxon in the reference dataset. This tells us the relative accuracy with which the normalized dataset is predicting the relationship of each taxon to the environment. Ideally the mean value should be 1 (a perfect fit). But it is possible for the mean to be 1, but the deviation to be substantial if it is balanced in the positive and negative direction. In otherwords, if the error in the positive and negative direction are equal in size, then even if there is large error in those directions, the mean value will still be 1 (a perfect fit). Thus we might have a perfect fit of the mean, yet have no taxa that is accurately modeled. To account for this, we put larger emphasis on the variance using a levene's test of equality of variance; but because variances may not be equally balanced in the positive and negative direction we are also still interested in changes in the average value. The two are necessary for accurate interpretation. We find especially for this analysis that outliers unduely affect the mean, so we prefer median for understanding the most common effect

```{r}
Summarize.lmRatiotab.Median<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method)) # make matrix
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-median(trt[[i]][[method[j]]]$lmRatiotab, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

Summarize.lmRatiotab.Var<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method)) # make matrix
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-var(trt[[i]][[method[j]]]$lmRatiotab, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

Summarize.lmRatiotab.Var(model10.A, method)



SM.lmRatio.model10.A<-as.data.frame(Summarize.lmRatiotab.Median(model10.A,method))
SV.lmRatio.model10.A<-as.data.frame(Summarize.lmRatiotab.Var(model10.A,method))
SM.lmRatio.model10.B<-as.data.frame(Summarize.lmRatiotab.Median(model10.B,method))
SV.lmRatio.model10.B<-as.data.frame(Summarize.lmRatiotab.Var(model10.B,method))
SM.lmRatio.model10.C<-as.data.frame(Summarize.lmRatiotab.Median(model10.C,method))
SV.lmRatio.model10.C<-as.data.frame(Summarize.lmRatiotab.Var(model10.C,method))
SM.lmRatio.model10.D<-as.data.frame(Summarize.lmRatiotab.Median(model10.D,method))
SV.lmRatio.model10.D<-as.data.frame(Summarize.lmRatiotab.Var(model10.D,method))

# prepare data for merging datasets

SM.lmRatio.model10.A$Level<-c(rep(1,10))
SV.lmRatio.model10.A$Level<-c(rep(1,10))
SM.lmRatio.model10.B$Level<-c(rep(2,10))
SV.lmRatio.model10.B$Level<-c(rep(2,10))
SM.lmRatio.model10.C$Level<-c(rep(3,10))
SV.lmRatio.model10.C$Level<-c(rep(3,10))
SM.lmRatio.model10.D$Level<-c(rep(4,10))
SV.lmRatio.model10.D$Level<-c(rep(4,10))

SM.lmRatio.model10.A<-melt(SM.lmRatio.model10.A, id.vars = "Level", measure.vars = method)
SV.lmRatio.model10.A<-melt(SV.lmRatio.model10.A, id.vars = "Level", measure.vars = method)
SM.lmRatio.model10.B<-melt(SM.lmRatio.model10.B, id.vars = "Level", measure.vars = method)
SV.lmRatio.model10.B<-melt(SV.lmRatio.model10.B, id.vars = "Level", measure.vars = method)
SM.lmRatio.model10.C<-melt(SM.lmRatio.model10.C, id.vars = "Level", measure.vars = method)
SV.lmRatio.model10.C<-melt(SV.lmRatio.model10.C, id.vars = "Level", measure.vars = method)
SM.lmRatio.model10.D<-melt(SM.lmRatio.model10.D, id.vars = "Level", measure.vars = method)
SV.lmRatio.model10.D<-melt(SV.lmRatio.model10.D, id.vars = "Level", measure.vars = method)

# merge datasets
SM.lmRatio.model10Levels<-do.call("rbind", list(SM.lmRatio.model10.A,SM.lmRatio.model10.B,SM.lmRatio.model10.C,SM.lmRatio.model10.D))
SV.lmRatio.model10Levels<-do.call("rbind", list(SV.lmRatio.model10.A,SV.lmRatio.model10.B,SV.lmRatio.model10.C,SV.lmRatio.model10.D))


# summarize for plotting
SM.lmRatio.model10.p<-data_summary2(SM.lmRatio.model10Levels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatio.model10.p<-data_summary2(SV.lmRatio.model10Levels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(SM.lmRatio.model10.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Median lmRatio (Median +/- min/max)")+
  theme_classic()

ggplot(SV.lmRatio.model10.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Variance lmRatio (Median +/- min/max)")+
  theme_classic()
```
... and run statistics to see the effect:
```{r}
summary(aov(SM.lmRatio.model10.p$value~SM.lmRatio.model10.p$Level*SM.lmRatio.model10.p$variable))

TukeyHSD(aov(SM.lmRatio.model10.p$value~SM.lmRatio.model10.p$Level*SM.lmRatio.model10.p$variable))

summary(aov(SV.lmRatio.model10.p$value~SV.lmRatio.model10.p$Level*SV.lmRatio.model10.p$variable))

TukeyHSD(aov(SV.lmRatio.model10.p$value~SV.lmRatio.model10.p$Level*SV.lmRatio.model10.p$variable))


```
Conclusion: there is an interaction between the normalization method and the internal structure of the community on our ability to accurately describe the relationship between taxa and the environmental gradients. According to this analysis we should prefer either QSeq2 or QSeq3 [check this later!!! (add in QSeq4)]

What about our ability to detect the experimental design? Following the same methodology:

```{r}
Summarize.lmRatiotabModel.Median<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method)) # make matrix
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-median(trt[[i]][[method[j]]]$lmRatiotab.model, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

Summarize.lmRatiotabModel.Var<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method)) # make matrix
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-var(trt[[i]][[method[j]]]$lmRatiotab.model, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}


SM.lmRatioM.model10.A<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.A,method))
SV.lmRatioM.model10.A<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.A,method))
SM.lmRatioM.model10.B<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.B,method))
SV.lmRatioM.model10.B<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.B,method))
SM.lmRatioM.model10.C<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.C,method))
SV.lmRatioM.model10.C<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.C,method))
SM.lmRatioM.model10.D<-as.data.frame(Summarize.lmRatiotabModel.Median(model10.D,method))
SV.lmRatioM.model10.D<-as.data.frame(Summarize.lmRatiotabModel.Var(model10.D,method))

# prepare data for merging datasets

SM.lmRatioM.model10.A$Level<-c(rep(1,10))
SV.lmRatioM.model10.A$Level<-c(rep(1,10))
SM.lmRatioM.model10.B$Level<-c(rep(2,10))
SV.lmRatioM.model10.B$Level<-c(rep(2,10))
SM.lmRatioM.model10.C$Level<-c(rep(3,10))
SV.lmRatioM.model10.C$Level<-c(rep(3,10))
SM.lmRatioM.model10.D$Level<-c(rep(4,10))
SV.lmRatioM.model10.D$Level<-c(rep(4,10))

SM.lmRatioM.model10.A<-melt(SM.lmRatioM.model10.A, id.vars = "Level", measure.vars = method)
SV.lmRatioM.model10.A<-melt(SV.lmRatioM.model10.A, id.vars = "Level", measure.vars = method)
SM.lmRatioM.model10.B<-melt(SM.lmRatioM.model10.B, id.vars = "Level", measure.vars = method)
SV.lmRatioM.model10.B<-melt(SV.lmRatioM.model10.B, id.vars = "Level", measure.vars = method)
SM.lmRatioM.model10.C<-melt(SM.lmRatioM.model10.C, id.vars = "Level", measure.vars = method)
SV.lmRatioM.model10.C<-melt(SV.lmRatioM.model10.C, id.vars = "Level", measure.vars = method)
SM.lmRatioM.model10.D<-melt(SM.lmRatioM.model10.D, id.vars = "Level", measure.vars = method)
SV.lmRatioM.model10.D<-melt(SV.lmRatioM.model10.D, id.vars = "Level", measure.vars = method)

# merge datasets
SM.lmRatioM.model10Levels<-do.call("rbind", list(SM.lmRatioM.model10.A,SM.lmRatioM.model10.B,SM.lmRatioM.model10.C,SM.lmRatioM.model10.D))
SV.lmRatioM.model10Levels<-do.call("rbind", list(SV.lmRatioM.model10.A,SV.lmRatioM.model10.B,SV.lmRatioM.model10.C,SV.lmRatioM.model10.D))


# summarize for plotting
SM.lmRatioM.model10.p<-data_summary2(SM.lmRatioM.model10Levels, varname="value", groupnames=c("Level", "variable"))
SV.lmRatioM.model10.p<-data_summary2(SV.lmRatioM.model10Levels, varname="value", groupnames=c("Level", "variable"))
# plot
ggplot(SM.lmRatioM.model10.p, aes(x=Level, y=value, group = variable, color=variable))+
  geom_errorbar(aes(ymin=low, ymax=high), width=.1, position=position_dodge(.3)) +
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Median lmRatio.model (Median +/- min/max)")+
  theme_classic()

ggplot(SV.lmRatioM.model10.p, aes(x=Level, y=log(value), group = variable, color=variable))+
  geom_errorbar(aes(ymin=log(low), ymax=log(high)), width=.1, position=position_dodge(.3)) +
  geom_line(position=position_dodge(.3))+
  geom_point(position=position_dodge(.3))+
  xlab("Degree of Structure")+
  ylab("Variance lmRatio.model (Median +/- min/max)")+
  theme_classic()

```

And again we do the stats:

```{r}
summary(aov(SM.lmRatioM.model10.p$value~SM.lmRatioM.model10.p$Level*SM.lmRatioM.model10.p$variable))

TukeyHSD(aov(SM.lmRatioM.model10.p$value~SM.lmRatioM.model10.p$Level*SM.lmRatioM.model10.p$variable))

summary(aov(SV.lmRatioM.model10.p$value~SV.lmRatioM.model10.p$Level*SV.lmRatioM.model10.p$variable))

TukeyHSD(aov(SV.lmRatioM.model10.p$value~SV.lmRatioM.model10.p$Level*SV.lmRatioM.model10.p$variable))

```
An emerging pattern that we see here is that raw data without any processing performs much worst when there is less structure and environmental variables can act more as a filter. Also, in our normalization method QSeq, higher sampling variables that don't omit low abundance taxa perform better. Higher abundance will continue to perform even better.







But wait, there's more! Let's say we want to know the effect of undersampling interacts with our normalization protocol to affect our ability to detect relationships between taxa and the environment. We can vary the sequencing depth:
```{r}
method<-c("QSeq0.5", "QSeq1", "QSeq2", "QSeq3", "QSeq4")
model10.C1<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=100, V=50, method)

model10.C2<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=200, V=100, method)

model10.C3<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=500, V=250, method)

model10.C4<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=1000, V=500, method)

model10.C5<-BENCHMARK.MM(reps=10, commonN=20, groupN=20, singleN=15, D=2000, V=1000, method)

```


Just like in the example about sparsity, we can look at the effect on sequencing depth on infering the relationship of species abundance to environmental gradients:
```{r}

```


Okay, at this point you may be wondering what the little network outputs have been about. Thus far we have looked at the effect of normalization on detecting important relationships between taxa and the modeled environment. Now let's look at how relationships among taxa are affected by normalization (or lack therof). We will be conducting a network analysis for this section. In the following section, each taxon is represented by a vertice, and relationships between taxa are represented as edges. To construct a network, we typically correlate taxa, then choose a threshold for netowrk construction. Taxa that have a high enough correlation coefficient are connected with an edge, and those falling below the threshold are not connected. We are generally interested in 3 network metrics:

In this analysis, a network is constructed using two methods: one has a preset correlation coefficient that determines which taxa are included (is called "static"); the other includes a dynamic threshold to reach a certain present number of edges (called "dynamic"). A common statistic that is used in network theory, but that also is interpretable in ecology is modularity. This is a measure of internal structure of the community. We can use the modules to interpret functional groups, identify covarying taxa, and potentially hypothesize functional relationships among organisms. However, measures of modularity are very sensitive to the number of edges and the number of vertices in a network. In our experiment, all of the networks have the same number of vertices because they come from the same community. But normalization may affect the strength of the relatioships among taxa, so it might affect the number of edges. The dynamic method is designed to ensure that the number of edges is the same across all networks constructed. To do this, the threshold for inclusion is varied. So for interpretation, we need to look at both the modularity, and the threshold level. Even if the modularity is the same, if the threshold from one normalization is much lower than another, it means that this method reduces the strength of the relatioships. The static method includes all relationships that meet a certain threshold. This can give a clear indication also of how well relationships are preserved through the normalization process.

Let's look at an example of this in practice. 
```{r}

```
Here we can see clearly that the degree of structure in the dataset interacts with the normalization method to affect it's accuracy. In all cases the QSeq normaliation method out performs
And some statistics:

```{r}

```


[FOR PAPER 2]/ development
```{r}
# new functions for more flexible use ####
#raw<-function(ps){ps}


RA<-function(ps){
  s<-transform_sample_counts(ps, function(x) x / sum(x))
  s
  }
eRare<-function(ps, level=NULL){
  if(is.null(level)){
  out<-make.rarefy2(ps, min(sample_sums(ps)))}
  else{
    out<-make.rarefy2(ps, level)}
  
  out
  }
pRare<-function(ps, level=NULL){
  if(is.null(level)){
  out<-make.rarefy2(ps, min(sample_sums(ps))*sample_sums(ps)/mean(sample_sums(ps)))} else{out<-make.rarefy2(ps, level)}
  
  out
}

QSeq<-function(ps){
  scale<-sample_data(ps)$DensityF
  out<-make.scaled2(ps, val=2*mean(sample_sums(ps)), scale)
  out
  }
deseqVST<-function(ps){
  out<-make.deseqVST(ps, "Factor", l=1)
  out
}
deseqVST.log<-function(ps){
  out<-make.deseqVST(ps, "Factor", l=1)
  out
}
limmaVST<-function(ps){
  out<-make.limmaVST(ps, "Factor")
  out
}
limmaVST.log<-function(ps){
  out<-make.limmaVST(ps, "Factor")
  out
}
lm.model<-function(ps){
  anova.otu<-t(as.data.frame(as.matrix(otu_table(ps))))
  anova.env<-data.frame(as.matrix(sample_data(ps)))
  anova.env$F1<-as.numeric(as.character(anova.env$F1))
  anova.env$F2<-as.numeric(as.character(anova.env$F2))
  anova.env$F3<-as.numeric(as.character(anova.env$F3))
  anova.env$F4<-as.numeric(as.character(anova.env$F4))
  anova.env$F5<-as.numeric(as.character(anova.env$F5))

  testlm<-adply(anova.otu, 2, function(x) {

    l1=summary(lm(x~anova.env$F1+anova.env$F2+anova.env$F3+anova.env$F4+anova.env$F5))
    return(l1$r.squared)
    })
  testlm[is.na(testlm)]<-0
  testlm2<-testlm$V1
  names(testlm2)<-testlm$X1
  #row.names(testlm)<-testlm$X1
  #testlm<-testlm[,-1]
  #testlm<-testlm[order(row.names(testlm)),]
  testlm2
  }

lm.model2<-function(ps){
  anova.otu<-t(as.data.frame(as.matrix(otu_table(ps))))
  anova.env<-data.frame(as.matrix(sample_data(ps)))
  

  testlm<-adply(anova.otu, 2, function(x) {

    l1=summary(lm(x~anova.env$Factor2))
    return(l1$r.squared)
    })
  testlm[is.na(testlm)]<-0
  testlm2<-testlm$V1
  names(testlm2)<-testlm$X1
  #row.names(testlm)<-testlm$X1
  #testlm<-testlm[,-1]
  #testlm<-testlm[order(row.names(testlm)),]
  testlm2
  }
  #' workhorse function for benchmark.MM version 2 with flexible inputs
  #' @param commonN number of common species
  #' @param groupN number of unique taxa to groups
  #' @param singleN number of unique taxa to samples
  #' @param D average sampling depth
  #' @param V variation in sampling depth
  #' @param method new normalization function to implement
  #' @keywords benchmark
  #' @export
  #' @examples
  #' run.analysis()

  
  
  # test funcitons ####

  method<-c("RA", "eRare", "pRare", "QSeq", "deseqVST", "limmaVST")
  
 #test.analysis<-run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method)
test.analysisB<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))
 test.analysis2<-run.analysis(commonN=30, groupN=20, singleN=5, D=500, V=250)
```



```{r}




run.analysis2<-function(commonN, groupN, singleN, D, V, method){
      AllSpp<-c(paste0("spp", c(1:700), sep="")) # make a quick list of all species functions
      AllSpp<-lapply(AllSpp, get) # connect function to name
      AllSpp<-unlist(AllSpp)  # format to be read by downstream functions
      names(AllSpp)<-c(paste0("spp", c(1:700)))

      # Define list of 5 species w/ global distribution
      global.spp<-names(sample(AllSpp, commonN, replace=F))

  # define list of species w/ regional distribution
      group.spp<-NULL
      group.spp$group1<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group2<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group3<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group4<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group5<-names(sample(AllSpp, groupN, replace=F))
      group.spp$group6<-names(sample(AllSpp, groupN, replace=F))

  # define list of species found at each site
      rando.spp<-NULL
      rando.spp$Site1<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site2<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site3<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site4<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site5<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
      rando.spp$Site6<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site7<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site8<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site9<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site10<-unique(c(names(sample(AllSpp,singleN, replace=F)), c(group.spp$group2), global.spp))
      rando.spp$Site11<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site12<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site13<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site14<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site15<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
      rando.spp$Site16<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site17<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site18<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site19<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site20<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
      rando.spp$Site21<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site22<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site23<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site24<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site25<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
      rando.spp$Site26<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
      rando.spp$Site27<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
      rando.spp$Site28<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
      rando.spp$Site29<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
      rando.spp$Site30<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))

  # make list of unique species arrays

      library(reshape2)
      f1c1<-c(5,5,5,5,5,5) # number of selections
      f1c2<-c(1,3,10,30,60,15) # mean value of selections
      f1c3<-c(0.5,1,4,10,20,5) # SD of selections
      F1.frame<-mapply(rnorm, f1c1,f1c2,f1c3) # pick Factor 1 value for each site
      F1<-reshape2::melt(F1.frame)

  #F2
      f2c1<-c(5,5,5,5,5,5) # number of selections
      f2c2<-c(34,30,10,55,35,60) # mean value of selections
      f2c3<-c(10,10,3,10,1,20) # SD of selections
      F2.frame<-mapply(rnorm, f2c1,f2c2,f2c3) # pick Factor 2 value for each site
      F2<-reshape2::melt(F2.frame)

  #F3
      f3c1<-c(5,5,5,5,5,5) # number of selections
      f3c2<-c(1,3,10,15,3,15) # mean value of selections
      f3c3<-c(0.5,1,3,3,1,5) # SD of selections
      F3.frame<-mapply(rnorm, f3c1,f3c2,f3c3) # pick Factor 3 value for each site
      F3<-reshape2::melt(F3.frame)

  #F4
      f4c1<-c(5,5,5,5,5,5) # number of selections
      f4c2<-c(5,5,5,5,5,5) # mean value of selections
      f4c3<-c(1,1,1,1,1,1) # SD of selections
      F4.frame<-mapply(rnorm, f4c1,f4c2,f4c3) # pick Factor 4 value for each site
      F4<-reshape2::melt(F4.frame)

  #F5
      f5c1<-c(5,5,5,5,5,5) # number of selections
      f5c2<-c(50,50,50,50,50,50) # mean value of selections
      f5c3<-c(20,20,20,20,20,20) # SD of selections
      F5.frame<-mapply(rnorm, f5c1,f5c2,f5c3) # pick Factor 5 value for each site
      F5<-reshape2::melt(F5.frame)
      Factors<-data.frame(F1$value,F2$value,F3$value,F4$value,F5$value) # combine factors into data table
      Sites<-c(paste0("Site", 1:30))
      rownames(Factors)<-Sites
      colnames(Factors)<-c("F1","F2","F3","F4","F5")

      output<-list("model"=NULL, "spplist"=NULL, "raw"=NULL)

      output$spplist<-rando.spp

      output$model$comm<-make.refcomm(rando.spp, Factors) # output a phyloseq object... will make a list of phyloseq objects
      output$model$comm<-filter_taxa(output$model$comm, function(x) sum(x)>0, TRUE)
      output$model$EV<-transform_sample_counts(output$model$comm, function(x) x / sum(x) )
      output$metrics<-NULL
      output$metrics$stats<-NULL
      output$metrics$Richness<-NULL
      Rich<-estimate_richness(output$model$comm, measures="Observed")
      output$metrics$Richness<-Rich
      output$metrics$skewness<-median(apply(X = otu_table(output$model$comm), MARGIN=2,FUN = function(x){skewness(x)}))
     
      sample<-set.seqDepth(D,V)
      output$raw$comm<-model.rarefy(output$model$comm, sample, D, V)

      print("spp selection complete")
      sample_data(output$model$comm)$Density<-sample_sums(output$model$comm)# add sample sums
      sample_data(output$model$comm)$DensityF<-sample_sums(output$model$comm)/mean(sample_sums(output$model$comm))
      sample_data(output$model$comm)$Factor<-as.factor(c(rep("one",5),rep("two",5),rep("three",5),rep("four",5),rep("five",5),rep("six",5)))
      sample_data(output$model$comm)$Factor2<-as.factor(c(rep(1,5),rep(2,5),rep(3,5),rep(4,5),rep(5,5),rep(6,5)))


      print("start subsampling")

      print("subsampling finished")
      # remove taxa that have zero abundance in "raw" sequencing run
      tax.filt<-filter_taxa(output$raw$comm, function(x)sum(x)>0)
      output$metrics$tax.lost<-tax.filt
        output$raw$comm<-filter_taxa(output$raw$comm, function(x)sum(x)>0, TRUE)
        # remove taxa that are not kept from sequencing so that they don't penalize downstream methods
        output$model$comm<-prune_taxa(tax.filt, output$model$comm)
        output$model$EV<-prune_taxa(tax.filt, output$model$EV)


      # for each species: measure prevalence
          prevalence=apply(X = otu_table(output$model$comm), MARGIN=1,FUN = function(x){sum(x > 0)})
      # for each species: measure relative abundance (proportion of total counts?
          p.abund<-transform_sample_counts(output$model$comm, function(x) x/sum(x) )

          mean_abundance<-apply(X = otu_table(p.abund), MARGIN=1,FUN = function(x){mean(x)})

          sd_abundance<-apply(X = otu_table(p.abund), MARGIN=1,FUN = function(x){sd(x)})
      # create a tax table for whole dataset ...
          tab<-data.frame(prevalence, mean_abundance, sd_abundance)
          tab$names<-rownames(tab)
          output$model$SpeciesMeta<-tab

          output$model$R<-lm.test(output$model$comm)
         

      # make expected value
          s<-sample_sums(output$raw$comm)
          s2<-as.data.frame(as.matrix(otu_table(output$model$EV)))
          s2<-for (i in 1:ncol(s2)) {s2[,i]<-s2[,i]*s[i]}
          otu_table(output$model$EV)<-otu_table(output$model$EV, taxa_are_rows=TRUE)
          M.Eval<-apply(X = otu_table(output$model$EV), MARGIN=1,FUN = function(x){mean(x[x>0])})


      #output$model$SpeciesMeta$USI<-output$model$SpeciesMeta$
      # create a tax table for whole dataset ...
          tab<-data.frame(prevalence, mean_abundance, sd_abundance, M.Eval)
          tab$names<-rownames(tab)
          output$model$SpeciesMeta<-tab
          output$model$R<-lm.test(output$model$comm)
          sample_data(output$raw$comm)<-sample_data(output$model$comm)
          output$model$PERMANOVA<-make.PERMANOVA(output$model$comm)
          output$model$rarecurve<-ggrare(output$model$comm, 50, color="Factor")
  print("metadata complete")
  
  # implement each normalization function
      for (i in 1:length(method)){
        a<-get(method[i])
        #name(a)<-i
        b<-output$raw$comm
        c<-a(b)
        output[[method[i]]]$comm<-c
        output[[method[i]]]$PERMANOVA<-make.PERMANOVA(c)
        output[[method[i]]]$PERMANOVA$Rratio<-output[[method[i]]]$PERMANOVA$aov.tab$R2/output$model$PERMANOVA$aov.tab$R2
        output[[method[i]]]$LII<-LII(output$model$comm, output[[method[i]]]$comm)
        #output[[method[i]]]$Dtab.plot<-plot(output$model$SpeciesMeta$prevalence,output[[method[i]]]$Dtab, col=output$model$SpeciesMeta$M.Eval, main="Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")
        #output[[method[i]]]$Dtab.model.plot<-plot(output$model$SpeciesMeta$prevalence,output[[method[i]]]$Dtab.model, col=output$model$SpeciesMeta$M.Eval, main="Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")
        
        print(paste(method[i], " complete"))
      }
# prepare raw metadata for lm analysis and model for lm analysis
      output$raw$PERMANOVA<-make.PERMANOVA(output$raw$comm)
      print("raw permanova complete")
      output$raw$LII<-LII(output$model$comm, output$raw$comm)
      print("raw LII complete")
      output$raw$lmtab<-lm.model(output$raw$comm)
      output$raw$lmtab.model<-lm.model2(output$raw$comm)
      print("lmtab raw complete")
      output$model$lmtab<-lm.model(output$model$comm)
      output$model$lmtab.model<-lm.model2(output$model$comm)# linear model of env. parameters as explanatory variables for abundance of each taxon
      print("lmtab model complete")
      output$raw$lmRatiotab<-output$raw$lmtab/output$model$lmtab #ratio of lm of env from normalized data to reference
      output$raw$lmRatiotab.model<-output$raw$lmtab.model/output$model$lmtab.model #ratio of lm of env from normalized data to reference using only categorical model variables (no explicit env. model)
      print("dtab complete")

  # do 1- : calculates the information lost per taxon
    for (i in 1:length(method)){
    # make the species-wise LII value:
      output$model$SpeciesMeta[[method[i]]]<-1-output[[method[i]]]$LII$R
      # make the lm output for each normalization: (this is r-squared, could be r value ...)
      output[[method[i]]]$lmtab<-lm.model(output[[method[i]]]$comm)# linear model of env. parameters as explanatory variables for abundance of each taxon
      output[[method[i]]]$lmtab.model<-lm.model2(output[[method[i]]]$comm)
      # conditional statement to make sure that the taxon names match, then:
      # difference in rsquared values (or r values?) from reference:
      if(names(output$model$lmtab)==names(output[[method[i]]]$lmtab)){
      output[[method[i]]]$lmRatiotab<-output[[method[i]]]$lmtab/output$model$lmtab
      output[[method[i]]]$lmRatiotab.model<-output[[method[i]]]$lmtab.model/output$model$lmtab.model} #ratio of lm of env from normalized data to reference
      else{print("Error in Dtab: names do not match")}
    }
    
    output
  }
```

test space:
```{r}
method<-c("RA", "QSeq")
tget<-get(method[1])

trt$rep1<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))
trt$rep2<-suppressWarnings(run.analysis2(commonN=30, groupN=20, singleN=5, D=500, V=250, method))

Summarize.Ftable<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method))
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-sum(trt[[i]][[method[j]]]$PERMANOVA$aov.tab$F.Model, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

Summarize.PERMANOVA.Rratio<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method))
  print("reference")
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      print(method[j])
     print(trt[[i]][[method[j]]]$PERMANOVA$Rratio)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
}

Summarize.LII<-function(trt, method){
  Ftab<-matrix(NA, nrow = length(trt), ncol = length(method))
  for(i in 1:length(trt)){
    for(j in 1:length(method)){
      Ftab[i,j]<-sum(trt[[i]][[method[j]]]$LII$Index, na.rm=T)
    #print(sum(trt[[i]][j]$PERMANOVA$aov.tab$F.Model))
  }
}
  rownames(Ftab)<-names(trt)
  colnames(Ftab)<-method
  Ftab
}

Plot.dtab<-function(trt, method){
  for(i in 1:length(trt)){
    for(j in 1:length(method)){plot(trt[[i]]$model$SpeciesMeta$prevalence,trt[[i]][[method[j]]]$Dtab, col=trt[[i]]$model$SpeciesMeta$M.Eval, main="Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")
  }
}}

Summarize.PERMANOVA.Rratio(trt, method)

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$QSeq$Dtab, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="QSeq; Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$QSeq$Dtab.model, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="QSeq; Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")


plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$deseqVST$Dtab, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="deseq; Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$deseqVST$Dtab.model, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="deseq; Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$QSeq$lmtab, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="QSeq; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$QSeq$lmtab.model, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="QSeq; Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$deseqVST$lmtab, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="deseq; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
plot(trt$rep1$model$SpeciesMeta$prevalence,trt$rep1$deseqVST$lmtab.model, col=gray(2*trt$rep1$model$SpeciesMeta$M.Eval), 
  main="deseq; Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

#collect variance data for stats!!
var(trt$rep1$QSeq$Dtab)
median(trt$rep1$QSeq$Dtab)
var(trt$rep1$QSeq$Dtab.model)
median(trt$rep1$QSeq$Dtab.model)
var(trt$rep1$deseqVST$Dtab)
median(trt$rep1$deseqVST$Dtab)
var(trt$rep1$deseqVST$Dtab.model)
median(trt$rep1$deseqVST$Dtab.model)
# make output with difference with env. and without env. variables in lm for lm.model()
# 
```
```{r}
plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$QSeq$Dtab, col=gray(trt$rep2$model$SpeciesMeta$M.Eval), 
  main="QSeq; Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$QSeq$Dtab.model, col=gray(trt$rep2$model$SpeciesMeta$M.Eval), 
  main="QSeq; Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")


plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$deseqVST$Dtab, col=gray(trt$rep2$model$SpeciesMeta$M.Eval), 
  main="deseq; Ratio of Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$deseqVST$Dtab.model, col=gray(trt$rep2$model$SpeciesMeta$M.Eval), 
  main="deseq; Ratio of Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared ratio")

plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$QSeq$lmtab, col=gray(2*trt$rep2$model$SpeciesMeta$M.Eval), 
  main="QSeq; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$QSeq$lmtab.model, col=gray(2*trt$rep2$model$SpeciesMeta$M.Eval), 
  main="QSeq; Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$deseqVST$lmtab, col=gray(2*trt$rep2$model$SpeciesMeta$M.Eval), 
  main="deseq; Rsquared for each taxon; using only environmental variables", xlab="Taxon Prevalence", ylab="Rsquared")
plot(trt$rep2$model$SpeciesMeta$prevalence,trt$rep2$deseqVST$lmtab.model, col=gray(2*trt$rep2$model$SpeciesMeta$M.Eval), 
  main="deseq; Rsquared for each taxon; using only categorical variables", xlab="Taxon Prevalence", ylab="Rsquared")

#collect variance data for stats!!
var(trt$rep2$QSeq$Dtab)
median(trt$rep2$QSeq$Dtab)
var(trt$rep2$QSeq$Dtab.model)
median(trt$rep2$QSeq$Dtab.model)
var(trt$rep1$deseqVST$Dtab)
median(trt$rep2$deseqVST$Dtab)
var(trt$rep2$deseqVST$Dtab.model)
median(trt$rep2$deseqVST$Dtab.model)

ConnStat<-function(ps, num=200){
  require(phyloseq)
  require(igraph)
  o<-otu_table(ps)
  c<-cor(o)
  i=1
  c[c<1]<-0
  n<-graph_from_incidence_matrix(c)
  while(ecount(n)<num){
    t<-otu_table(ps)
    t<-cor(t)
    t[t<i]<-0
    t[t>i]<-1
    n<-graph_from_incidence_matrix(t)
    i=i-0.001
  }
   cfg<-cluster_fast_greedy(as.undirected(n))
  plot(cfg, as.undirected(n), layout=layout_nicely(n), vertex.label=NA, main="Dynamic", vertex.size=10)
  table<-matrix(nrow=2,ncol=4)
  colnames(table)<-c("Mean_Closeness", "Mean_Degree", "Modularity", "Threshold")
  rownames(table)<-c("Dynamic", "Static")
  table[1,1]<-mean(closeness(n))
  table[1,2]<-mean(degree(n))
  table[1,3]<-modularity(n, membership(cfg))
  table[1,4]<-i

  t<-otu_table(ps)
  t<-cor(t)
  t[t<0.8]<-0
  t[t>0.8]<-1
  n<-graph_from_incidence_matrix(t)
  cfg<-cluster_fast_greedy(as.undirected(n))
  table[2,1]<-mean(closeness(n))
  table[2,2]<-mean(degree(n))
  table[2,3]<-modularity(n, membership(cfg))
  table[2,4]<-0.8
  plot(cfg, as.undirected(n), layout=layout_nicely(n), vertex.label=NA, main="Static", vertex.size=10)
  table
}


ConnStat(trt$rep1$model$comm, num=50)
ConnStat(trt$rep1$model$comm, num=100)
ConnStat(trt$rep1$model$comm, num=150)
ConnStat(trt$rep1$model$comm, num=200)
ConnStat(trt$rep1$model$comm, num=250)
ConnStat(trt$rep1$model$comm, num=350)
ConnStat(trt$rep1$model$comm, num=450)


Mean<-c(1,3,10,30,60,15,34,30,10,55,35,60,1,3,10,15,3,15,5,5,5,5,5,5,50,50,50,50,50,50)
Var<-c(0.5,1,4,10,20,5,10,10,3,10,1,20,0.5,1,3,3,1,5,1,1,1,1,1,1,20,20,20,20,20,20)
ymin<-Mean-Var
ymax<-Mean+Var
Factor.Label<-c(rep("Factor 1", 6), rep("Factor 2", 6), rep("Factor 3", 6), rep("Factor 4", 6), rep("Factor 5", 6))
Group.label<-c(rep(c("Group 1", "Group 2", "Group 3", "Group 4", "Group 5", "Group 6"), 5))

df.envplot<-data.frame(Group.label, Factor.Label, Mean, Var, ymin, ymax)

ggplot(df.envplot, aes(x=Group.label, y=Mean,group=Factor.Label, colour=Factor.Label))+ geom_point() + geom_line()+geom_errorbar(aes(ymin=ymin, ymax=ymax), linetype=2, alpha=0.9, position=position_dodge(0.1))+theme_classic()



```
