output$reference$SpeciesMeta<-tab
output$reference$R<-lm.test(output$reference$comm)
sample_data(output$raw$comm)<-sample_data(output$reference$comm)
print("metadata complete")
# implement each normalization function
for (i in 1:length(method)){
a<-get(method[i])
#name(a)<-i
b<-output$raw$comm
c<-a(b)
output[[method[i]]]$comm<-c
}
output
}
simulate.MM<-function(reps, commonN, groupN, singleN, D, V, method, Spike=T){
require(phyloseq)
array<-c(rep(commonN, reps))
names(array)<-c(paste0("rep", 1:reps, sep=""))
array<-sapply(array, run.analysis3, simplify=F, USE.NAMES = T, groupN, singleN, D, V, method)
array
}
# make a community using Model.Microbiome
com1.T<-simulate.MM(1, 30,10,1,100,20,Spike=T)
library(Model.Microbiome)
# make a community using Model.Microbiome
com1.T<-simulate.MM(1, 30,10,1,100,20,Spike=T)
run.analysis3<-function(commonN, groupN, singleN, D, V, method, Spike=T){
AllSpp<-c(paste0("spp", c(1:700), sep="")) # make a quick list of all species functions
AllSpp<-lapply(AllSpp, get) # connect function to name
AllSpp<-unlist(AllSpp)  # format to be read by downstream functions
names(AllSpp)<-c(paste0("spp", c(1:700)))
if(Spike==TRUE){
spike<-c("spike1", "spike2", "spike3")
spike<-lapply(spike, get)
spike<-unlist(spike)
names(spike)<-c("spike1", "spike2", "spike3")
# Define list of 5 species w/ global distribution
global.spp<-c(names(spike), names(sample(AllSpp, commonN, replace=F)))
} else {
global.spp<-names(sample(AllSpp, commonN, replace=F))
}
# define list of species w/ regional distribution
group.spp<-NULL
group.spp$group1<-names(sample(AllSpp, groupN, replace=F))
group.spp$group2<-names(sample(AllSpp, groupN, replace=F))
group.spp$group3<-names(sample(AllSpp, groupN, replace=F))
group.spp$group4<-names(sample(AllSpp, groupN, replace=F))
group.spp$group5<-names(sample(AllSpp, groupN, replace=F))
group.spp$group6<-names(sample(AllSpp, groupN, replace=F))
# define list of species found at each site
rando.spp<-NULL
rando.spp$Site1<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site2<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site3<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site4<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site5<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site6<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site7<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site8<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site9<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site10<-unique(c(names(sample(AllSpp,singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site11<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site12<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site13<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site14<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site15<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site16<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site17<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site18<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site19<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site20<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site21<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site22<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site23<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site24<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site25<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site26<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site27<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site28<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site29<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site30<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
# make list of unique species arrays
library(reshape2)
f1c1<-c(5,5,5,5,5,5) # number of selections
f1c2<-c(1,3,10,30,60,15) # mean value of selections
f1c3<-c(0.5,1,4,10,20,5) # SD of selections
F1.frame<-mapply(rnorm, f1c1,f1c2,f1c3) # pick Factor 1 value for each site
F1<-reshape2::melt(F1.frame)
#F2
f2c1<-c(5,5,5,5,5,5) # number of selections
f2c2<-c(34,30,10,55,35,60) # mean value of selections
f2c3<-c(10,10,3,10,1,20) # SD of selections
F2.frame<-mapply(rnorm, f2c1,f2c2,f2c3) # pick Factor 2 value for each site
F2<-reshape2::melt(F2.frame)
#F3
f3c1<-c(5,5,5,5,5,5) # number of selections
f3c2<-c(1,3,10,15,3,15) # mean value of selections
f3c3<-c(0.5,1,3,3,1,5) # SD of selections
F3.frame<-mapply(rnorm, f3c1,f3c2,f3c3) # pick Factor 3 value for each site
F3<-reshape2::melt(F3.frame)
#F4
f4c1<-c(5,5,5,5,5,5) # number of selections
f4c2<-c(5,5,5,5,5,5) # mean value of selections
f4c3<-c(1,1,1,1,1,1) # SD of selections
F4.frame<-mapply(rnorm, f4c1,f4c2,f4c3) # pick Factor 4 value for each site
F4<-reshape2::melt(F4.frame)
#F5
f5c1<-c(5,5,5,5,5,5) # number of selections
f5c2<-c(50,50,50,50,50,50) # mean value of selections
f5c3<-c(20,20,20,20,20,20) # SD of selections
F5.frame<-mapply(rnorm, f5c1,f5c2,f5c3) # pick Factor 5 value for each site
F5<-reshape2::melt(F5.frame)
Factors<-data.frame(F1$value,F2$value,F3$value,F4$value,F5$value) # combine factors into data table
Sites<-c(paste0("Site", 1:30))
rownames(Factors)<-Sites
colnames(Factors)<-c("F1","F2","F3","F4","F5")
output<-list("reference"=NULL, "spplist"=NULL, "raw"=NULL)
output$spplist<-rando.spp
output$reference$comm<-make.refcomm(rando.spp, Factors) # output a phyloseq object... will make a list of phyloseq objects
output$reference$comm<-filter_taxa(output$reference$comm, function(x) sum(x)>0, TRUE)
output$reference$EV<-transform_sample_counts(output$reference$comm, function(x) x / sum(x) )
output$metrics<-NULL
output$metrics$stats<-NULL
output$metrics$Richness<-NULL
Rich<-estimate_richness(output$reference$comm, measures="Observed")
output$metrics$Richness<-Rich
output$metrics$skewness<-median(apply(X = otu_table(output$reference$comm), MARGIN=2,FUN = function(x){skewness(x)}))
sample<-set.seqDepth(D,V)
output$raw$comm<-model.rarefy(output$reference$comm, sample, D, V)
print("spp selection complete")
sample_data(output$reference$comm)$Density<-sample_sums(output$reference$comm)# add sample sums
sample_data(output$reference$comm)$DensityF<-sample_sums(output$reference$comm)/mean(sample_sums(output$reference$comm))
sample_data(output$reference$comm)$Factor<-as.factor(c(rep("one",5),rep("two",5),rep("three",5),rep("four",5),rep("five",5),rep("six",5)))
sample_data(output$reference$comm)$Factor2<-as.factor(c(rep(1,5),rep(2,5),rep(3,5),rep(4,5),rep(5,5),rep(6,5)))
# remove taxa that have zero abundance in "raw" sequencing run
tax.filt<-filter_taxa(output$raw$comm, function(x)sum(x)>0)
output$metrics$tax.lost<-tax.filt
output$raw$comm<-filter_taxa(output$raw$comm, function(x)sum(x)>0, TRUE)
# remove taxa that are not kept from sequencing so that they don't penalize downstream methods
output$reference$comm<-prune_taxa(tax.filt, output$reference$comm)
output$reference$EV<-prune_taxa(tax.filt, output$reference$EV)
# for each species: measure prevalence
prevalence=apply(X = otu_table(output$reference$comm), MARGIN=1,FUN = function(x){sum(x > 0)})
# for each species: measure relative abundance (proportion of total counts?
p.abund<-transform_sample_counts(output$reference$comm, function(x) x/sum(x) )
mean_abundance<-apply(X = otu_table(p.abund), MARGIN=1,FUN = function(x){mean(x)})
sd_abundance<-apply(X = otu_table(p.abund), MARGIN=1,FUN = function(x){sd(x)})
# create a tax table for whole dataset ...
tab<-data.frame(prevalence, mean_abundance, sd_abundance)
tab$names<-rownames(tab)
output$reference$SpeciesMeta<-tab
# make expected value
s<-sample_sums(output$raw$comm)
s2<-as.data.frame(as.matrix(otu_table(output$reference$EV)))
s2<-for (i in 1:ncol(s2)) {s2[,i]<-s2[,i]*s[i]}
otu_table(output$reference$EV)<-otu_table(output$reference$EV, taxa_are_rows=TRUE)
M.Eval<-apply(X = otu_table(output$reference$EV), MARGIN=1,FUN = function(x){mean(x[x>0])})
# create a tax table for whole dataset ...
tab<-data.frame(prevalence, mean_abundance, sd_abundance, M.Eval)
tab$names<-rownames(tab)
output$reference$SpeciesMeta<-tab
output$reference$R<-lm.test(output$reference$comm)
sample_data(output$raw$comm)<-sample_data(output$reference$comm)
print("metadata complete")
# implement each normalization function
for (i in 1:length(method)){
a<-get(method[i])
#name(a)<-i
b<-output$raw$comm
c<-a(b)
output[[method[i]]]$comm<-c
}
output
}
# make a community using Model.Microbiome
com1.T<-simulate.MM(1, 30,10,1,100,20,Spike=T)
method<-c("eRare", "QSeq")
# make a community using Model.Microbiome
com1.T<-simulate.MM(1, 30,10,1,100,20,method,Spike=T)
com1.F<-simulate.MM(1, 30,10,1,100,20,method,Spike=F)
View(com1.T$rep1$reference$comm)
View(as.data.frame(as.matrix(otu_table(com1.T$rep1$reference$comm))))
View(F2)
View(spike1)
View(as.data.frame(as.matrix(otu_table(com1.F$rep1$reference$comm))))
run.analysis3<-function(commonN, groupN, singleN, D, V, method, Spike){
AllSpp<-c(paste0("spp", c(1:700), sep="")) # make a quick list of all species functions
AllSpp<-lapply(AllSpp, get) # connect function to name
AllSpp<-unlist(AllSpp)  # format to be read by downstream functions
names(AllSpp)<-c(paste0("spp", c(1:700)))
if(Spike==TRUE){
spike<-c("spike1", "spike2", "spike3")
spike<-lapply(spike, get)
spike<-unlist(spike)
names(spike)<-c("spike1", "spike2", "spike3")
# Define list of 5 species w/ global distribution
global.spp<-c(names(spike), names(sample(AllSpp, commonN, replace=F)))
} else {
global.spp<-names(sample(AllSpp, commonN, replace=F))
}
# define list of species w/ regional distribution
group.spp<-NULL
group.spp$group1<-names(sample(AllSpp, groupN, replace=F))
group.spp$group2<-names(sample(AllSpp, groupN, replace=F))
group.spp$group3<-names(sample(AllSpp, groupN, replace=F))
group.spp$group4<-names(sample(AllSpp, groupN, replace=F))
group.spp$group5<-names(sample(AllSpp, groupN, replace=F))
group.spp$group6<-names(sample(AllSpp, groupN, replace=F))
# define list of species found at each site
rando.spp<-NULL
rando.spp$Site1<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site2<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site3<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site4<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site5<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site6<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site7<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site8<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site9<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site10<-unique(c(names(sample(AllSpp,singleN, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site11<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site12<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site13<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site14<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site15<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site16<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site17<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site18<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site19<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site20<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site21<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site22<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site23<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site24<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site25<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site26<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site27<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site28<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site29<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site30<-unique(c(names(sample(AllSpp, singleN, replace=F)), c(group.spp$group6), global.spp))
# make list of unique species arrays
library(reshape2)
f1c1<-c(5,5,5,5,5,5) # number of selections
f1c2<-c(1,3,10,30,60,15) # mean value of selections
f1c3<-c(0.5,1,4,10,20,5) # SD of selections
F1.frame<-mapply(rnorm, f1c1,f1c2,f1c3) # pick Factor 1 value for each site
F1<-reshape2::melt(F1.frame)
#F2
f2c1<-c(5,5,5,5,5,5) # number of selections
f2c2<-c(34,30,10,55,35,60) # mean value of selections
f2c3<-c(10,10,3,10,1,20) # SD of selections
F2.frame<-mapply(rnorm, f2c1,f2c2,f2c3) # pick Factor 2 value for each site
F2<-reshape2::melt(F2.frame)
#F3
f3c1<-c(5,5,5,5,5,5) # number of selections
f3c2<-c(1,3,10,15,3,15) # mean value of selections
f3c3<-c(0.5,1,3,3,1,5) # SD of selections
F3.frame<-mapply(rnorm, f3c1,f3c2,f3c3) # pick Factor 3 value for each site
F3<-reshape2::melt(F3.frame)
#F4
f4c1<-c(5,5,5,5,5,5) # number of selections
f4c2<-c(5,5,5,5,5,5) # mean value of selections
f4c3<-c(1,1,1,1,1,1) # SD of selections
F4.frame<-mapply(rnorm, f4c1,f4c2,f4c3) # pick Factor 4 value for each site
F4<-reshape2::melt(F4.frame)
#F5
f5c1<-c(5,5,5,5,5,5) # number of selections
f5c2<-c(50,50,50,50,50,50) # mean value of selections
f5c3<-c(20,20,20,20,20,20) # SD of selections
F5.frame<-mapply(rnorm, f5c1,f5c2,f5c3) # pick Factor 5 value for each site
F5<-reshape2::melt(F5.frame)
Factors<-data.frame(F1$value,F2$value,F3$value,F4$value,F5$value) # combine factors into data table
Sites<-c(paste0("Site", 1:30))
rownames(Factors)<-Sites
colnames(Factors)<-c("F1","F2","F3","F4","F5")
output<-list("reference"=NULL, "spplist"=NULL, "raw"=NULL)
output$spplist<-rando.spp
output$reference$comm<-make.refcomm(rando.spp, Factors) # output a phyloseq object... will make a list of phyloseq objects
output$reference$comm<-filter_taxa(output$reference$comm, function(x) sum(x)>0, TRUE)
output$reference$EV<-transform_sample_counts(output$reference$comm, function(x) x / sum(x) )
output$metrics<-NULL
output$metrics$stats<-NULL
output$metrics$Richness<-NULL
Rich<-estimate_richness(output$reference$comm, measures="Observed")
output$metrics$Richness<-Rich
output$metrics$skewness<-median(apply(X = otu_table(output$reference$comm), MARGIN=2,FUN = function(x){skewness(x)}))
sample<-set.seqDepth(D,V)
output$raw$comm<-model.rarefy(output$reference$comm, sample, D, V)
print("spp selection complete")
sample_data(output$reference$comm)$Density<-sample_sums(output$reference$comm)# add sample sums
sample_data(output$reference$comm)$DensityF<-sample_sums(output$reference$comm)/mean(sample_sums(output$reference$comm))
sample_data(output$reference$comm)$Factor<-as.factor(c(rep("one",5),rep("two",5),rep("three",5),rep("four",5),rep("five",5),rep("six",5)))
sample_data(output$reference$comm)$Factor2<-as.factor(c(rep(1,5),rep(2,5),rep(3,5),rep(4,5),rep(5,5),rep(6,5)))
# remove taxa that have zero abundance in "raw" sequencing run
tax.filt<-filter_taxa(output$raw$comm, function(x)sum(x)>0)
output$metrics$tax.lost<-tax.filt
output$raw$comm<-filter_taxa(output$raw$comm, function(x)sum(x)>0, TRUE)
# remove taxa that are not kept from sequencing so that they don't penalize downstream methods
output$reference$comm<-prune_taxa(tax.filt, output$reference$comm)
output$reference$EV<-prune_taxa(tax.filt, output$reference$EV)
# for each species: measure prevalence
prevalence=apply(X = otu_table(output$reference$comm), MARGIN=1,FUN = function(x){sum(x > 0)})
# for each species: measure relative abundance (proportion of total counts?
p.abund<-transform_sample_counts(output$reference$comm, function(x) x/sum(x) )
mean_abundance<-apply(X = otu_table(p.abund), MARGIN=1,FUN = function(x){mean(x)})
sd_abundance<-apply(X = otu_table(p.abund), MARGIN=1,FUN = function(x){sd(x)})
# create a tax table for whole dataset ...
tab<-data.frame(prevalence, mean_abundance, sd_abundance)
tab$names<-rownames(tab)
output$reference$SpeciesMeta<-tab
# make expected value
s<-sample_sums(output$raw$comm)
s2<-as.data.frame(as.matrix(otu_table(output$reference$EV)))
s2<-for (i in 1:ncol(s2)) {s2[,i]<-s2[,i]*s[i]}
otu_table(output$reference$EV)<-otu_table(output$reference$EV, taxa_are_rows=TRUE)
M.Eval<-apply(X = otu_table(output$reference$EV), MARGIN=1,FUN = function(x){mean(x[x>0])})
# create a tax table for whole dataset ...
tab<-data.frame(prevalence, mean_abundance, sd_abundance, M.Eval)
tab$names<-rownames(tab)
output$reference$SpeciesMeta<-tab
output$reference$R<-lm.test(output$reference$comm)
sample_data(output$raw$comm)<-sample_data(output$reference$comm)
print("metadata complete")
# implement each normalization function
for (i in 1:length(method)){
a<-get(method[i])
#name(a)<-i
b<-output$raw$comm
c<-a(b)
output[[method[i]]]$comm<-c
}
output
}
simulate.MM<-function(reps, commonN, groupN, singleN, D, V, method, Spike){
require(phyloseq)
array<-c(rep(commonN, reps))
names(array)<-c(paste0("rep", 1:reps, sep=""))
array<-sapply(array, run.analysis3, simplify=F, USE.NAMES = T, groupN, singleN, D, V, method)
array
}
# make a community using Model.Microbiome
com1.T<-simulate.MM(1, 30,10,1,100,20,method,Spike=TRUE)
Spike<-TRUE
Spike<-TRUE
# make a community using Model.Microbiome
com1.T<-simulate.MM(1, 30,10,1,100,20,method,Spike)
Spike
com1.T<-simulate.MM(1, 30,10,1,100,20,method,Spike=T)
simulate.MM<-function(reps, commonN, groupN, singleN, D, V, method, Spike){
require(phyloseq)
array<-c(rep(commonN, reps))
names(array)<-c(paste0("rep", 1:reps, sep=""))
array<-sapply(array, run.analysis3, simplify=F, USE.NAMES = T, groupN, singleN, D, V, method, Spike)
array
}
Spike<-TRUE
# make a community using Model.Microbiome
com1.T<-simulate.MM(1, 30,10,1,100,20,method,Spike)
Spike<-FALSE
com1.F<-simulate.MM(1, 30,10,1,100,20,method,Spike)
View(as.data.frame(as.matrix(otu_table(com1.T$rep1$reference$comm))))
View(as.data.frame(as.matrix(otu_table(com1.F$rep1$reference$comm))))
View(as.data.frame(as.matrix(otu_table(com1.F$rep1$eRare$comm))))
View(as.data.frame(as.matrix(otu_table(com1.T$rep1$eRare$comm))))
View(as.data.frame(as.matrix(otu_table(com1.T$rep1$QSeq$comm))))
Y<-t(as.data.frame(as.matrix(otu_table(com1.F$rep1$reference$comm))))
XFormula= ~ Factor2 + poly(F1, degree=2, raw=TRUE)+ poly(F2, degree=2, raw=TRUE)+ poly(F3, degree=2, raw=TRUE)+ poly(F4, degree=2, raw=TRUE)+ poly(F5, degree=2, raw=TRUE)
XData$sample<-rownames(XData)
XData<-as.data.frame(as.matrix(sample_data(com1.T$rep1$reference$comm))) # environmental data table with factors as columns
XData$sample<-rownames(XData)
XData$F1<-as.numeric(XData$F1)
XData$F2<-as.numeric(XData$F2)
XData$F3<-as.numeric(XData$F3)
XData$F4<-as.numeric(XData$F4)
XData$F5<-as.numeric(XData$F5) # make sure numeric covariates are treated as continuous not discrete
XFormula= ~ Factor2 + poly(F1, degree=2, raw=TRUE)+ poly(F2, degree=2, raw=TRUE)+ poly(F3, degree=2, raw=TRUE)+ poly(F4, degree=2, raw=TRUE)+ poly(F5, degree=2, raw=TRUE) # formula for environment with a categorical factor
studyDesign = XData[,c(9,10)] # study design
rL <- HmscRandomLevel(units=studyDesign$sample)
library(Hmsc)
install.packages("fields")
library(Hmsc)
library(fields)
install.packages("fields")
install.packages("fields")
install.packages("fields", depends=TRUE)
install.packages("fields", depends=TRUE)
library(fields)
library(Hmsc)
knitr::opts_chunk$set(echo = TRUE)
getwd()
setwd("/Users/dietrich/Documents/GitHub/Model.Microbiome")
library(devtools)
library(roxygen2)
document()
XFormula= ~ Factor2 + poly(F1, degree=2, raw=TRUE)+ poly(F2, degree=2, raw=TRUE)+ poly(F3, degree=2, raw=TRUE)+ poly(F4, degree=2, raw=TRUE)+ poly(F5, degree=2, raw=TRUE) # formula for environment with a categorical factor
studyDesign = XData[,c(9,10)] # study design
rL <- HmscRandomLevel(units=studyDesign$sample)
rL$nfMax=15
m<-Hmsc(Y=Y, XData=XData, XFormula=XFormula, studyDesign=studyDesign, ranLevels=list(sample=rL))
nChains=2
test.run=TRUE
m<-sampleMcmc(m, thin=1, samples=100, transient=50, nChains=nChains, nParallel=nChains, verbose=T)
mpost=convertToCodaObject(m)
ns=50
ess.beta=effectiveSize(mpost$Beta)
ess.beta=effectiveSize(mpost$Beta)
psrf.beta=gelman.diag(mpost$Beta, multivariate=FALSE)$psrf
hist(ess.beta)
hist(ess.beta)
hist(bsrf.beta)
hist(psrf.beta)
ess.gamma=effectiveSize(mpost$Gamma)
hist(ess.beta)
hist(psrf.beta)
ess.gamma=effectiveSize(mpost$Gamma)
psrf.gamma=gelman.diag(mpost$Gamma, multivariate=FALSE)$psrf
hist(ess.gamma)
hist(psrf.gamma)
sppairs = matrix(sample(x=1:ns^2, size=100))
tmp=mpost$Omega=gelman.diag(tmp, multivariate=FALSE)$sprf
for(chain in 1:length(tmp)){
tmp[[chain]]=tmp[[chain]][,sppairs]
}
tmp=mpost$Omega[[1]]
for(chain in 1:length(tmp)){
tmp[[chain]]=tmp[[chain]][,sppairs]
}
ess.omega=effectiveSize(tmp)
psrf.omega=gelman.diag(tmp, multivariate=FALSE)$psrf
hist(ess.omega)
hist(psrf.omega)
preds=computePredictedValues(m)
m<-Hmsc(Y=Y, XData=XData, XFormula=XFormula, studyDesign=studyDesign, ranLevels=list(sample=rL))
nChains=2
test.run=TRUE
verbose=0
m<-sampleMcmc(m, thin=1, samples=100, transient=50, nChains=nChains, nParallel=nChains, verbose=T)
mpost=convertToCodaObject(m)
ns=50
ess.beta=effectiveSize(mpost$Beta)
ess.beta=effectiveSize(mpost$Beta)
psrf.beta=gelman.diag(mpost$Beta, multivariate=FALSE)$psrf
hist(ess.beta)
hist(psrf.beta)
ess.gamma=effectiveSize(mpost$Gamma)
psrf.gamma=gelman.diag(mpost$Gamma, multivariate=FALSE)$psrf
psrf.gamma=gelman.diag(mpost$Gamma, multivariate=FALSE)$psrf
hist(ess.gamma)
hist(psrf.gamma)
sppairs = matrix(sample(x=1:ns^2, size=100))
tmp=mpost$Omega[[1]]
for(chain in 1:length(tmp)){
tmp[[chain]]=tmp[[chain]][,sppairs]
}
ess.omega=effectiveSize(tmp)
ess.omega=effectiveSize(tmp)
psrf.omega=gelman.diag(tmp, multivariate=FALSE)$psrf
hist(ess.omega)
hist(psrf.omega)
preds=computePredictedValues(m)
?computePredictedValues
hist(MF$R2, xlim=c(0,1), main=paste0("Mean =", round(mean(MF$R2), 2)))
VP=computeVariancePartitioning(m, group=c(1,2,3,4,5), groupnames=c("F1","F2","F3","F4","F5"))
plotVarianceParitioning(m, VP=VP)
plotVariancePartitioning(m, VP=VP)
plotVariancePartitioning(m, VP=VP)
VP=computeVariancePartitioning(m, group=c(1,2,3,4,5,6), groupnames=c("Factor","1","F2","F3","F4","F5"))
plotVariancePartitioning(m, VP=VP)
VP=computeVariancePartitioning(m, group=c(1,2,3,4,5,6), groupnames=c("Factor","F1","F2","F3","F4","F5"))
plotVariancePartitioning(m, VP=VP)
postBeta<- getPostEstimate(m, parName="Beta")
plotBeta(m, post=postBeta, param="Support", plotTree=FALSE, supportLevel=0.95, split=0.4, spNamesNumbers=c(T,F))
OmegaCor=computeAssociations(m)
supportLevel=0.95
toPlot=((OmegaCor[[1]]$support>supportLevel)+(OmegaCor[[1]]$support<(1-supportLevel))>0)*OmegaCor[[1]]$mean
corrplot(toPlot, method="color", col=colorRampPalette(c("blue", "white", "red"))(200),tl.cex=.6, tl.col="black", title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))
?corrplot
install.packages("corrplot")
library(corrplot)
corrplot(toPlot, method="color", col=colorRampPalette(c("blue", "white", "red"))(200),tl.cex=.6, tl.col="black", title=paste("random effect level:", m$rLNames[1]), mar=c(0,0,1,0))
summary(mpost$Rho)
summary(mpost$Omega)
Gradient= constructGradient(m, focalVariable="F1", non.focalVariables = list("F2", "F3", "F4", "F5"))
Gradient= constructGradient(m, focalVariable="1", non.focalVariables = list("F2", "F3", "F4", "F5"))
?list
Gradient= constructGradient(m, focalVariable="1", non.focalVariables = list("Other" = list("F2", "F3", "F4", "F5"))
)
Gradient= constructGradient(m, focalVariable="F1", non.focalVariables = list("Other" = list("F2", "F3", "F4", "F5")))
Gradient$XDataNew
predY=predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew, ranLevels = Gradient$rLNew, expected=TRUE)
predY=predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew, ranLevels = Gradient$rLNew, expected=F)
predY=predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew, ranLevels = Gradient$rLNew, expected=T)
Gradient$XDataNew
Gradient= constructGradient(m, focalVariable="F1", non.focalVariables = list("Factor2" = list("Factor2")))
Gradient= constructGradient(m, focalVariable="F1", non.focalVariables = list("Others" = list("F2","F3","F4", "F5" ,"Factor2")))
Gradient$XDataNew
predY=predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew, ranLevels = Gradient$rLNew, expected=T)
?constructGradient
Gradient= constructGradient(m, focalVariable="F1", non.focalVariables = list("Others" = list("F2","F3","F4", "F5" ,"Factor2")), ngrid=10)
Gradient$XDataNew
predY=predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew, ranLevels = Gradient$rLNew, expected=T)
Gradient= constructGradient(m, focalVariable="F1", non.focalVariables = list("F2" = list(1),"F3"=list(1),"F4"=list(1), "F5"=list(1) ,"Factor2"=list(1))))
Gradient= constructGradient(m, focalVariable="F1", non.focalVariables = list("F2" = list(1),"F3"=list(1),"F4"=list(1), "F5"=list(1) ,"Factor2"=list(1)))
Gradient= constructGradient(m, focalVariable="F1")
Gradient$XDataNew
predY=predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew, ranLevels = Gradient$rLNew, expected=T)
Gradient= constructGradient(m, focalVariable="F1", ngrid=5)
predY=predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew, ranLevels = Gradient$rLNew, expected=T)
Gradient$XDataNew
Gradient= constructGradient(m, focalVariable="F1")
Gradient$XDataNew
predY=predict(m, XData=Gradient$XDataNew, studyDesign=Gradient$studyDesignNew, ranLevels = Gradient$rLNew, expected=T)
